"""
Incremental update helpers used by the watcher and backfill pipelines.

This module centralises the bookkeeping needed to skip unchanged files,
track the chunk identifiers generated for each source, and clean up stale
artifacts before reprocessing.
"""

from __future__ import annotations

import hashlib
import json
import logging
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from pathlib import Path
from threading import RLock
from typing import Any, Dict, Iterable, List, Optional, Sequence, Union

DEFAULT_HASH_ALGORITHM = "sha256"


def normalize_timestamp(timestamp: str, *, fallback: Optional[datetime] = None) -> str:
    """
    Convert timestamps generated by the watcher (underscored) into ISO-8601 format.
    """
    if not timestamp:
        fallback_dt = fallback or datetime.now(timezone.utc)
        return fallback_dt.replace(microsecond=0).isoformat()

    patterns = (
        "%Y_%m_%d_%H_%M_%S",
        "%Y-%m-%d_%H-%M-%S",
        "%Y-%m-%dT%H:%M:%S",
        "%Y-%m-%d %H:%M:%S",
    )
    for pattern in patterns:
        try:
            dt = datetime.strptime(timestamp, pattern)
            dt = dt.replace(microsecond=0, tzinfo=timezone.utc)
            return dt.isoformat()
        except ValueError:
            continue

    try:
        dt = datetime.fromisoformat(timestamp)
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt.replace(microsecond=0).isoformat()
    except ValueError:
        fallback_dt = fallback or datetime.now(timezone.utc)
        return fallback_dt.replace(microsecond=0).isoformat()


def build_chunk_id(timestamp: str, base_name: str, chunk_index: int) -> str:
    """
    Generate the canonical chunk identifier shared by watcher and backfill.
    """
    iso_ts = normalize_timestamp(timestamp)
    safe_base = base_name.replace(" ", "_")
    return f"{iso_ts}_{safe_base}_chunk{chunk_index}"


def _unique(items: Iterable[str]) -> List[str]:
    seen: List[str] = []
    for item in items:
        if item and item not in seen:
            seen.append(item)
    return seen


def _ensure_aware(dt: datetime) -> datetime:
    """
    Ensure datetime has timezone information (defaults to UTC).
    """
    if dt.tzinfo is None:
        return dt.replace(tzinfo=timezone.utc)
    return dt


def remove_previous_chunk_ids(
    chunk_ids: Iterable[str],
    *,
    dedup_manager: Optional[Any] = None,
    chroma_directory: Optional[Union[str, Path]] = None,
    logger: Optional[logging.Logger] = None,
) -> int:
    """
    Attempt best-effort removal of stale chunk identifiers from ChromaDB.
    """
    ids = _unique(chunk_ids)
    if not ids:
        return 0

    active_logger = logger or logging.getLogger(__name__)
    removed = 0

    if dedup_manager is not None:
        try:
            dedup_manager.collection.delete(ids=ids)
            removed = len(ids)
            hash_index = getattr(dedup_manager, "hash_index", {})
            chunk_hash_map = getattr(dedup_manager, "chunk_hash_map", {})
            for chunk_id in ids:
                hash_value = chunk_hash_map.pop(chunk_id, None)
                if not hash_value:
                    continue
                bucket = hash_index.get(hash_value)
                if bucket:
                    bucket.discard(chunk_id)
                    if not bucket:
                        hash_index.pop(hash_value, None)
            return removed
        except Exception as exc:  # noqa: BLE001
            active_logger.warning(
                "Incremental updates: failed to delete chunks via dedup manager: %s", exc
            )

    if chroma_directory:
        try:
            from chromadb_crud import ChromaDBManager
        except ImportError:
            active_logger.debug(
                "Incremental updates: chromadb_crud not available; cannot delete chunks."
            )
        else:
            try:
                manager = ChromaDBManager(persist_directory=str(chroma_directory))
                for chunk_id in ids:
                    try:
                        manager.delete_chunk(chunk_id)
                        removed += 1
                    except Exception:  # noqa: BLE001 - ignore individual failures
                        active_logger.debug(
                            "Incremental updates: failed to delete chunk %s via ChromaDB manager",
                            chunk_id,
                        )
            except Exception as exc:  # noqa: BLE001
                active_logger.warning(
                    "Incremental updates: error deleting chunks via ChromaDB manager: %s",
                    exc,
                )

    return removed


@dataclass
class VersionRecord:
    hash: str
    hash_algorithm: str
    chunk_ids: List[str]
    last_processed_at: str
    last_indexed_at: Optional[str]
    metadata: Dict[str, Any]
    version: int = 1

    def needs_indexing(self, expected_chunk_ids: Optional[Sequence[str]] = None) -> bool:
        if expected_chunk_ids is not None and sorted(expected_chunk_ids) != sorted(
            self.chunk_ids
        ):
            return True
        return self.last_indexed_at is None


class VersionTracker:
    """
    Persist per-file hashes and chunk identifiers for incremental processing.
    """

    def __init__(
        self,
        config_or_path: Union[Dict[str, Any], str, Path],
        *,
        logger: Optional[logging.Logger] = None,
    ) -> None:
        if isinstance(config_or_path, (str, Path)):
            config: Dict[str, Any] = {
                "version_file": str(config_or_path),
                "hash_algorithm": DEFAULT_HASH_ALGORITHM,
            }
        else:
            config = dict(config_or_path)

        base_dir = Path(config.get("base_dir") or Path.cwd())
        version_file = config.get("version_file", "./06_config/file_versions.json")
        version_path = Path(version_file)
        if not version_path.is_absolute():
            version_path = base_dir / version_path

        self.version_file = version_path
        self.hash_algorithm = str(config.get("hash_algorithm", DEFAULT_HASH_ALGORITHM))
        self.logger = logger or logging.getLogger(__name__)

        self._lock = RLock()
        self._data: Dict[str, Any] = {
            "files": {},
            "meta": {"hash_algorithm": self.hash_algorithm},
        }

        self.version_file.parent.mkdir(parents=True, exist_ok=True)
        self._load()

    # ------------------------------------------------------------------ #
    # Internal helpers
    # ------------------------------------------------------------------ #
    def _canonical_path(self, file_path: Union[str, Path]) -> str:
        return str(Path(file_path).resolve())

    def _load(self) -> None:
        if not self.version_file.exists():
            return
        try:
            with self.version_file.open("r", encoding="utf-8") as handle:
                data = json.load(handle)
            if isinstance(data, dict):
                self._data.update(data)
        except Exception as exc:  # noqa: BLE001
            self.logger.warning(
                "Incremental updates: failed to load tracker file %s: %s",
                self.version_file,
                exc,
            )

    def _save_unlocked(self) -> None:
        tmp_path = self.version_file.with_suffix(".tmp")
        with tmp_path.open("w", encoding="utf-8") as handle:
            json.dump(self._data, handle, indent=2, ensure_ascii=False)
        tmp_path.replace(self.version_file)

    # ------------------------------------------------------------------ #
    # Hash helpers
    # ------------------------------------------------------------------ #
    def hash_content(self, content: Union[str, bytes]) -> str:
        if isinstance(content, str):
            content = content.encode("utf-8")
        digest = hashlib.new(self.hash_algorithm)
        digest.update(content)
        return digest.hexdigest()

    def file_hash(self, file_path: Union[str, Path]) -> str:
        digest = hashlib.new(self.hash_algorithm)
        with Path(file_path).open("rb") as handle:
            for chunk in iter(lambda: handle.read(65536), b""):
                digest.update(chunk)
        return digest.hexdigest()

    # ------------------------------------------------------------------ #
    # Public API
    # ------------------------------------------------------------------ #
    def has_changed(
        self,
        file_path: Union[str, Path],
        content_hash: Optional[str] = None,
    ) -> bool:
        key = self._canonical_path(file_path)
        with self._lock:
            record = self._data["files"].get(key)
            if content_hash is None:
                content_hash = self.file_hash(file_path)
            if not record:
                return True
            return record.get("hash") != content_hash

    def mark_processed(
        self,
        file_path: Union[str, Path],
        content_hash: str,
        *,
        chunk_ids: Optional[Sequence[str]] = None,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> VersionRecord:
        key = self._canonical_path(file_path)
        now = datetime.now(timezone.utc).replace(microsecond=0).isoformat()
        chunks = _unique(chunk_ids or [])
        with self._lock:
            record = self._data["files"].get(key) or {}
            version = int(record.get("version", 0)) + 1
            record.update(
                {
                    "hash": content_hash,
                    "hash_algorithm": self.hash_algorithm,
                    "chunk_ids": chunks,
                    "metadata": dict(metadata or {}),
                    "last_processed_at": now,
                    "last_indexed_at": None,
                    "version": version,
                }
            )
            self._data["files"][key] = record
            self._save_unlocked()
        return VersionRecord(
            hash=content_hash,
            hash_algorithm=self.hash_algorithm,
            chunk_ids=chunks,
            last_processed_at=now,
            last_indexed_at=None,
            metadata=dict(metadata or {}),
            version=version,
        )

    def mark_indexed(
        self,
        file_path: Union[str, Path],
        chunk_ids: Optional[Sequence[str]] = None,
    ) -> Optional[VersionRecord]:
        key = self._canonical_path(file_path)
        now = datetime.now(timezone.utc).replace(microsecond=0).isoformat()
        with self._lock:
            record = self._data["files"].get(key)
            if not record:
                return None
            chunks = _unique(chunk_ids or record.get("chunk_ids", []))
            record["chunk_ids"] = chunks
            record["last_indexed_at"] = now
            self._save_unlocked()
        return VersionRecord(
            hash=record.get("hash", ""),
            hash_algorithm=record.get("hash_algorithm", self.hash_algorithm),
            chunk_ids=chunks,
            last_processed_at=record.get("last_processed_at", now),
            last_indexed_at=now,
            metadata=dict(record.get("metadata", {})),
            version=int(record.get("version", 1)),
        )

    def get_chunk_ids(self, file_path: Union[str, Path]) -> List[str]:
        key = self._canonical_path(file_path)
        with self._lock:
            record = self._data["files"].get(key)
            if not record:
                return []
            return list(record.get("chunk_ids") or [])

    def get_record(self, file_path: Union[str, Path]) -> Optional[VersionRecord]:
        key = self._canonical_path(file_path)
        with self._lock:
            record = self._data["files"].get(key)
            if not record:
                return None
            return VersionRecord(
                hash=record.get("hash", ""),
                hash_algorithm=record.get("hash_algorithm", self.hash_algorithm),
                chunk_ids=list(record.get("chunk_ids") or []),
                last_processed_at=record.get("last_processed_at", ""),
                last_indexed_at=record.get("last_indexed_at"),
                metadata=dict(record.get("metadata", {})),
                version=int(record.get("version", 1)),
            )

    def needs_indexing(
        self,
        file_path: Union[str, Path],
        expected_chunk_ids: Optional[Sequence[str]] = None,
    ) -> bool:
        record = self.get_record(file_path)
        if record is None:
            return True
        return record.needs_indexing(expected_chunk_ids)

    def prune_old_entries(
        self,
        *,
        max_entries: Optional[int] = None,
        max_age_days: Optional[int] = None,
    ) -> int:
        removed = 0
        with self._lock:
            files: Dict[str, Dict[str, Any]] = self._data.get("files", {})
            if max_age_days is not None:
                cutoff = datetime.now(timezone.utc) - timedelta(days=max_age_days)
                keys_to_remove = [
                    key
                    for key, record in files.items()
                    if record.get("last_processed_at")
                    and _ensure_aware(datetime.fromisoformat(record["last_processed_at"])) < cutoff
                ]
                for key in keys_to_remove:
                    files.pop(key, None)
                    removed += 1

            if max_entries is not None and len(files) > max_entries:
                sorted_items = sorted(
                    files.items(),
                    key=lambda item: item[1].get("last_processed_at", ""),
                    reverse=True,
                )
                for key, _ in sorted_items[max_entries:]:
                    files.pop(key, None)
                    removed += 1

            if removed:
                self._save_unlocked()
        return removed


__all__ = [
    "VersionTracker",
    "VersionRecord",
    "normalize_timestamp",
    "build_chunk_id",
    "remove_previous_chunk_ids",
]

