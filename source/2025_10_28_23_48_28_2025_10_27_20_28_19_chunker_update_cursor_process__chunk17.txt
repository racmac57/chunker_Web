Create evaluation datasets with common queries. 3. Implement core metrics (precision@k, recall@k, MRR). 4. Add LangSmith tracing to RAG pipeline. 5. Set up monitoring and alerting. Apply these code patches/enhancements:

### requirements_rag.txt
ollama==0.3.3
faiss-cpu==1.8.0
pandas==2.2.3
langchain==0.3.1
langchain-community==0.3.1
openpyxl==3.1.5  # For .xlsx
pypdf2==3.0.1  # For .pdf
langsmith==0.1.129  # For LangSmith
rouge-score==0.1.2  # For ROUGE
nltk==3.9.1  # For BLEU
bert-score==0.3.13  # For BERTScore
sentence-transformers==3.1.1  # For faithfulness
chromadb==0.5.11  # For ChromaDB
# Add others: pdfminer.six, python-docx, etc. ### ollama_integration.py
"""RAG search using Ollama embeddings and FAISS vector store.""" from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.retrievers import BM25Retriever

def initialize_ollama(model_name="nomic-embed-text"):
    """Initialize Ollama embeddings.""" return OllamaEmbeddings(model=model_name)

def initialize_vectorstore(embeddings, chunks=None, metadatas=None):
    """Initialize FAISS vector store with optional pre-loaded chunks.""" if chunks and metadatas:
        return FAISS.from_texts(texts=chunks, embedding=embeddings, metadatas=metadatas)
    return FAISS(embeddings.embed_query, index=None)  # Placeholder for empty

def hybrid_search(query, vectorstore, keyword_retriever, top_k=5):
    """Combine semantic and keyword search.""" vector_results = vectorstore.similarity_search_with_score(query, k=top_k)
    keyword_results = keyword_retriever.get_relevant_documents(query)[:top_k]
    combined = list(set([(r[0].page_content, r[0].metadata, r[1]) for r in vector_results] +
                        [(r.page_content, r.metadata, 0.0) for r in keyword_results]))
    return sorted(combined, key=lambda x: x[2], reverse=True)[:top_k]

def search_knowledge_base(query, vectorstore, keyword_retriever):
    """Search knowledge base and return formatted results.""" results = hybrid_search(query, vectorstore, keyword_retriever)
    return [{"content": r[0], "metadata": r[1], "score": r[2]} for r in results]

### embedding_helpers.py
"""Helpers for embedding generation and storage.""" import faiss
import numpy as np
from langchain_community.vectorstores import FAISS

def save_embeddings(vectorstore, path="faiss_index"):
    """Save FAISS index to disk.""" vectorstore.save_local(path)

def load_embeddings(embeddings, path="faiss_index"):
    """Load FAISS index from disk.""" return FAISS.load_local(path, embeddings, allow_dangerous_deserialization=True)

def generate_chunk_metadata(chunk, filename, file_type, timestamp, index, extract_keywords):
    """Generate metadata for a chunk.""" return {
        "source_file": filename,
        "file_type": file_type,
        "keywords": extract_keywords(chunk),
        "timestamp": timestamp,
        "chunk_index": index
    }

### Patch watcher_splitter.py
# Add imports
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from embedding_helpers import generate_chunk_metadata, save_embeddings
from langchain.retrievers import BM25Retriever
# For new file types: import openpyxl, pypdf2, ast, etc. # Globals
embeddings = OllamaEmbeddings(model="nomic-embed-text")
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
vectorstore = FAISS(embeddings.embed_query, index=None)  # Init empty
keyword_retriever = None

def update_embeddings(filename, file_content, file_type, timestamp, extract_keywords):
    """Update embeddings for a file.""" global vectorstore, keyword_retriever
    chunks = splitter.split_text(file_content)
    metadatas = [generate_chunk_metadata(chunk, filename, file_type, timestamp, i, extract_keywords)
                 for i, chunk in enumerate(chunks)]
    
    # Remove old
    if vectorstore.index.ntotal > 0:
        ids_to_delete = [id for id, meta in vectorstore.docstore._dict.items() if meta["source_file"] == filename]
        vectorstore.delete(ids_to_delete)
    
    # Add new
    vectorstore.add_texts(texts=chunks, metadatas=metadatas)
    keyword_retriever = BM25Retriever.from_texts(chunks, metadatas=metadatas)
    
    # Save
    save_embeddings(vectorstore, "faiss_index")

# In process_file:
def process_file(filename, file_content, file_type, timestamp):
    # Existing logic + extract_metadata(file_type, file_content) -> add to chunks
    update_embeddings(filename, file_content, file_type, timestamp, extract_keywords)  # Assume extract_keywords exists

# Add file-type handlers (e.g., def extract_from_xlsx(file_path): load_workbook, get sheets/formulas/etc.) ### New rag_evaluation.py
"""RAG evaluation metrics.""" from rouge_score import rouge_scorer
from nltk.translate.bleu_score import sentence_bleu
from bert_score import score as bertscore
import numpy as np

def precision_at_k(relevant, k):
    """Precision@K."""
    return sum(relevant[:k]) / k

def recall_at_k(relevant, total_relevant):
    """Recall@K."""
    return sum(relevant) / total_relevant

def mrr(relevant):
    """Mean Reciprocal Rank.""" for idx, rel in enumerate(relevant):
        if rel == 1:
            return 1 / (idx + 1)
    return 0

def evaluate_retrieval(ground_truth, retrieved, k=5):
    """Compute precision, recall, MRR.""" relevant = [1 if doc in ground_truth else 0 for doc in retrieved[:k]]
    p_at_k = precision_at_k(relevant, k)
    r_at_k = recall_at_k(relevant, len(ground_truth))
    mrr_score = mrr(relevant)
    return {"precision_at_k": p_at_k, "recall_at_k": r_at_k, "mrr": mrr_score}

def evaluate_generation(reference, generated):
    """ROUGE, BLEU, BERTScore.""" scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    rouge_scores = scorer.score(reference, generated)
    bleu = sentence_bleu([reference.split()], generated.split())
    P, R, F1 = bertscore([generated], [reference], lang="en")
    return {"rouge": rouge_scores, "bleu": bleu, "bertscore": F1.mean().item()}

def evaluate_end_to_end(query, response, context):
    """Faithfulness, relevancy - use heuristics or LLM judge.""" # Placeholder: Implement with LangSmith evaluators or custom logic
    pass

# Integrate: In rag_search.py, log metrics for test queries; export to CSV/JSON. ### New langsmith_integration.py
"""LangSmith integration for tracing and evaluation.""" import os
from langsmith import Client, traceable
from langsmith.evaluation import evaluate
from langsmith.schemas import Example, Run

os.environ["LANGCHAIN_API_KEY"] = "your_langsmith_api_key"
os.environ["LANGCHAIN_PROJECT"] = "chunker-rag-eval"

client = Client()

@traceable(run_type="chain")
def traced_rag_chain(query):
    # Wrap existing rag_chain or search_knowledge_base
    results = search_knowledge_base(query, vectorstore, keyword_retriever)
    # Generation step here if needed
    return results

def create_feedback(run_id, score, comment=""):
    client.create_feedback(run_id=run_id, key="user_feedback", score=score, comment=comment)

def run_evaluation(examples, rag_func):
    results = evaluate(
        rag_func,
        data=examples,
        evaluators=["answer_relevance", "context_relevance", "faithfulness", "answer_correctness"],
        experiment_prefix="rag-eval-v1"
    )
    return results

# Example evaluation dataset
def load_eval_dataset(path="test_queries.json"):
    import json
    with open(path, "r") as f:
        data = json.load(f)
    return [Example(inputs={"question": d["query"]}, outputs={"answer": d["expected_answer"]}) for d in data]

# Usage: eval_results = run_evaluation(load_eval_dataset(), traced_rag_chain)

### New automated_eval.py
"""Automated RAG evaluation.""" import schedule
import time

def run_auto_eval():
    examples = load_eval_dataset()
    results = run_evaluation(examples, traced_rag_chain)
    # Compare to baseline, alert if regression (e.g., via email/slack)
    # Generate report (CSV/JSON)
    pass

schedule.every().day.at("02:00").do(run_auto_eval)

while True:
    schedule.run_pending()
    time.sleep(60)

Update README: Add setup (Ollama pull/run nomic-embed-text, deps, LangSmith API key), usage (process files, query KB, run evaluations), metrics eval, LangSmith dashboards. Ensure: Local-first, no costs (except LangSmith if scaled), handle updates/security/performance. Make all changes, add comments/docstrings for clarity/maintainability. ---

**Cursor**

Starting the RAG integration and evaluation support. Creating the implementation files. Adding RAG requirements and creating the initial implementation files. ```pip-requirements
# Enterprise Chunker System Requirements
# Core dependencies
Flask==2.3.3
Flask-SocketIO==5.3.6
python-socketio==5.8.0
python-engineio==4.7.1

# File processing and utilities
nltk==3.8.1
psutil==5.9.5
Werkzeug==2.3.7

# Database and data handling
sqlite3  # Built-in with Python

# System monitoring and notifications
# (These are included in the existing notification_system.py)

# Development and testing (optional)
pytest==7.4.2
pytest-flask==1.2.0

# Production deployment (optional)
gunicorn==21.2.0
eventlet==0.33.3

# Additional enhancements (optional)
redis==4.6.0  # For advanced caching and job queues
celery==5.3.1  # For background task processing

# RAG and Vector Database Dependencies
chromadb==0.5.11  # Vector database for embeddings
faiss-cpu==1.8.0  # FAISS for vector similarity search
sentence-transformers==3.1.1  # For embeddings and faithfulness scoring
ollama==0.3.3  # Local LLM and embeddings

# File type processing
openpyxl==3.1.5  # For .xlsx files
pypdf2==3.0.1  # For .pdf files
python-docx==1.1.0  # For .docx files
pyyaml==6.0.1  # For .yaml files

# LangChain and RAG framework
langchain==0.3.1
langchain-community==0.3.1
langsmith==0.1.129  # For tracing and evaluation

# Evaluation metrics
rouge-score==0.1.2  # For ROUGE scores
bert-score==0.3.13  # For BERTScore
numpy==1.26.0  # For numerical operations
scikit-learn==1.4.0  # For machine learning utilities

```

Creating the core RAG integration files:


```python
"""
RAG Integration Module for Chunker_v2
Handles retrieval-augmented generation with ChromaDB and Ollama embeddings
"""

import chromadb
from chromadb.config import Settings
from datetime import datetime
import json
import logging
from typing import List, Dict, Any, Optional
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

logger = logging.getLogger(__name__)


class ChromaRAG:
    """
    ChromaDB-based RAG implementation for knowledge base management
    Provides CRUD operations for vector database with hybrid search
    """
    
    def __init__(self, persist_directory="./chroma_db"):
        """
        Initialize ChromaDB client and collection
        
        Args:
            persist_directory: Directory to persist ChromaDB database
        """
        self.client = chromadb.PersistentClient(
            path=persist_directory,
            settings=Settings(anonymized_telemetry=False)
        )
        
        # Create or get collection for chunker knowledge base
        self.collection = self.client.get_or_create_collection(
            name="chunker_knowledge_base",
            metadata={"description": "Enterprise chunker knowledge base with RAG capabilities"}
        )
        
        logger.info(f"Initialized ChromaDB collection with {self.collection.count()} existing chunks")
    
    def add_chunk(self, chunk_text: str, metadata: Dict[str, Any]) -> str:
        """
        Add a chunk to the vector database
        
        Args:
            chunk_text: The text content of the chunk
            metadata: Dictionary containing chunk metadata
        
        Returns:
            chunk_id: Unique identifier for the added chunk
        """
        # Generate unique chunk ID
        chunk_id = f"{metadata['timestamp']}_{metadata['file_name']}_chunk{metadata['chunk_index']}"
        
        # Prepare metadata for ChromaDB (ensure values are serializable)
        chroma_metadata = {
            "file_name": metadata["file_name"],
            "file_type": metadata["file_type"],
            "chunk_index": str(metadata["chunk_index"]),
            "timestamp": metadata["timestamp"],
            "department": metadata.get("department", "admin"),
            "keywords": json.dumps(metadata.get("keywords", [])),
            "file_size": str(metadata.get("file_size", 0)),
            "processing_time": str(metadata.get("processing_time", 0))
        }
        
        try:
            # Add to collection
            self.collection.add(
                documents=[chunk_text],
                metadatas=[chroma_metadata],
                ids=[chunk_id]
            )
            
            logger.info(f"Added chunk to ChromaDB: {chunk_id}")
            return chunk_id
            
        except Exception as e:
            logger.error(f"Failed to add chunk to ChromaDB: {e}")
            raise
    
    def search_similar(self, query: str, n_results: int = 5, 
                       file_type: Optional[str] = None, 
                       department: Optional[str] = None) -> List[Dict]:
        """
        Search for similar chunks using semantic similarity
        
        Args:
            query: Search query text
            n_results: Number of results to return
            file_type: Filter by file type (optional)
            department: Filter by department (optional)
        
        Returns:
            List of formatted search results
        """
        where_clause = {}
        
        if file_type:
            where_clause["file_type"] = file_type
        if department:
            where_clause["department"] = department
        
        try:
            # Query the collection
            results = self.collection.query(
                query_texts=[query],
                n_results=n_results,
                where=where_clause if where_clause else None
            )
            
            return self._format_results(results)
            
        except Exception as e:
            logger.error(f"Failed to search ChromaDB: {e}")
            return []
    
    def search_by_keywords(self, keywords: List[str], n_results: int = 5) -> List[Dict]:
        """
        Search by specific keywords
        
        Args:
            keywords: List of keywords to search for
            n_results: Number of results to return
        
        Returns:
            List of formatted search results
        """
        # Convert keywords to query string
        query = " ".join(keywords)
        return self.search_similar(query, n_results)
    
    def get_chunk_by_id(self, chunk_id: str) -> Dict:
        """
        Retrieve specific chunk by ID
        
        Args:
            chunk_id: Unique chunk identifier
        
        Returns:
            Chunk data as dictionary
        """
        try:
            results = self.collection.get(ids=[chunk_id])
            formatted = self._format_results(results)
            return formatted[0] if formatted else None
            
        except Exception as e:
            logger.error(f"Failed to get chunk by ID: {e}")
            return {}
    
    def update_chunk(self, chunk_id: str, new_text: str, new_metadata: Dict):
        """Update existing chunk in database"""
        try:
            self.collection.update(
                ids=[chunk_id],
                documents=[new_text],
                metadatas=[new_metadata]
            )
            logger.info(f"Updated chunk: {chunk_id}")
            
        except Exception as e:
            logger.error(f"Failed to update chunk: {e}")
            raise
    
    def delete_chunk(self, chunk_id: str):
        """Delete chunk from database"""
        try:
            self.collection.delete(ids=[chunk_id])
            logger.info(f"Deleted chunk: {chunk_id}")
            
        except Exception as e:
            logger.error(f"Failed to delete chunk: {e}")
            raise
    
    def get_collection_stats(self) -> Dict:
        """
        Get statistics about the collection
        
        Returns:
            Dictionary with collection statistics
        """
        count = self.collection.count()
        return {
            "total_chunks": count,
            "collection_name": self.collection.name,
            "last_updated": datetime.now().isoformat()
        }
    
    def _format_results(self, results: Dict) -> List[Dict]:
        """
        Format ChromaDB results for easier use
        
        Args:
            results: Raw ChromaDB query results
        
        Returns:
            List of formatted result dictionaries
        """
        formatted_results = []
        
        if results.get("documents") and results["documents"][0]:
            for i, doc in enumerate(results["documents"][0]):
                formatted_results.append({
                    "id": results["ids"][0][i],
                    "document": doc,
                    "metadata": results["metadatas"][0][i],
                    "distance": results["distances"][0][i] if results.get("distances") else None
                })
        
        return formatted_results


class FaithfulnessScorer:
    """
    Advanced faithfulness scoring for RAG evaluation
    Evaluates how well generated answers are grounded in source context
    """
    
    def __init__(self):
        """Initialize faithfulness scorer with sentence transformer model"""
        self.model = SentenceTransformer('all-MiniLM-L6-v2')
    
    def extract_claims(self, answer: str) -> List[str]:
        """
        Extract factual claims from the answer
        
        Args:
            answer: Generated answer text
        
        Returns:
            List of extracted claim sentences
        """
        import nltk
        import re
        
        sentences = nltk.sent_tokenize(answer)
        claims = []
        
        for sentence in sentences:
            # Remove questions and opinion indicators
            if not re.search(r'\?|I think|I believe|in my opinion', sentence, re.IGNORECASE):
                # Extract factual statements
                if re.search(r'\b(is|are|was|were|has|have|will|can|should)\b', sentence, re.IGNORECASE):
                    claims.append(sentence.strip())
        
        return claims
    
    def calculate_faithfulness(self, answer: str, context: str, threshold: float = 0.7) -> float:
        """
        Calculate faithfulness score (0-1) based on how well answer claims
        are supported by the context
        
        Args:
            answer: Generated answer
            context: Source context
            threshold: Similarity threshold for support (default: 0.7)
        
        Returns:
            Faithfulness score between 0 and 1
        """
        claims = self.extract_claims(answer)
        if not claims:
            return 1.0  # No claims to verify
        
        # Get embeddings for claims and context
        claim_embeddings = self.model.encode(claims)
        context_embeddings = self.model.encode([context])
        
        faithfulness_scores = []
        
        for claim_emb in claim_embeddings:
            # Find best matching context sentence
            similarities = cosine_similarity([claim_emb], context_embeddings)[0]
            max_similarity = np.max(similarities)
            
            # Check if claim is supported by context
            if max_similarity >= threshold:
                faithfulness_scores.append(1.0)
            else:
                # Check for partial support
                faithfulness_scores.append(max_similarity)
        
        return float(np.mean(faithfulness_scores))
    
    def detailed_faithfulness_analysis(self, answer: str, context: str) -> Dict:
        """
        Provide detailed analysis of faithfulness
        
        Args:
            answer: Generated answer
            context: Source context
        
        Returns:
            Detailed analysis dictionary
        """
        claims = self.extract_claims(answer)
        analysis = {
            "total_claims": len(claims),
            "supported_claims": 0,
            "unsupported_claims": 0,
            "claim_details": []
        }
        
        for claim in claims:
            claim_emb = self.model.encode([claim])
            context_emb = self.model.encode([context])
            similarity = cosine_similarity(claim_emb, context_emb)[0][0]
            
            is_supported = similarity >= 0.7
            if is_supported:
                analysis["supported_claims"] += 1
            else:
                analysis["unsupported_claims"] += 1
            
            analysis["claim_details"].append({
                "claim": claim,
                "similarity": float(similarity),
                "supported": is_supported
            })
        
        analysis["faithfulness_score"] = analysis["supported_claims"] / analysis["total_claims"] if analysis["total_claims"] > 0 else 1.0
        
        return analysis


def integrate_chroma_with_watcher():
    """
    Integration example for ChromaDB with existing watcher system
    
    This function demonstrates how to integrate ChromaDB with the
    existing file processing pipeline
    """
    
    # Initialize ChromaDB
    chroma_rag = ChromaRAG()
    
    def process_file_with_chroma(file_path, config, chunks, department="admin"):
        """
        Enhanced file processing with ChromaDB integration
        
        Args:
            file_path: Path to the file being processed
            config: Configuration dictionary
            chunks: List of text chunks from file
            department: Department classification
        
        Returns:
            True if successful
        """
        timestamp = datetime.now().isoformat()
        
        # Add each chunk to ChromaDB
        for i, chunk in enumerate(chunks):
            metadata = {
                "file_name": file_path.name,
                "file_type": file_path.suffix,
                "chunk_index": i + 1,
                "timestamp": timestamp,
                "department": department,
                "keywords": extract_keywords(chunk),
                "file_size": file_path.stat().st_size,
                "processing_time": 0
            }
            
            try:
                chunk_id = chroma_rag.add_chunk(chunk, metadata)
                logger.info(f"Added chunk to ChromaDB: {chunk_id}")
            except Exception as e:
                logger.error(f"Failed to add chunk to ChromaDB: {e}")
        
        return True
    
    return process_file_with_chroma


def extract_keywords(text: str, max_keywords: int = 5) -> List[str]:
    """
    Extract keywords from text using simple frequency analysis
    
    Args:
        text: Input text
        max_keywords: Maximum number of keywords to return
    
    Returns:
        List of extracted keywords
    """
    import re
    from collections import Counter
    
    # Remove special characters and split into words
    words = re.findall(r'\b\w+\b', text.lower())
    
    # Remove common stop words
    stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with'}
    keywords = [word for word in words if word not in stop_words]
    
    # Return most common keywords
    counter = Counter(keywords)
    return [word for word, _ in counter.most_common(max_keywords)]


def example_usage():
    """Example usage of ChromaRAG system"""
    
    # Initialize RAG system
    rag = ChromaRAG()
    
    # Add a chunk to the knowledge base
    metadata = {
        "file_name": "excel_guide.md",
        "file_type": ".md",
        "chunk_index": 1,
        "timestamp": "2025-10-27T12:00:00Z",
        "department": "admin",
        "keywords": ["excel", "vlookup", "formulas"]
    }
    
    chunk_id = rag.add_chunk(
        "VLOOKUP is used to find values in a table. Syntax: VLOOKUP(lookup_value, table_array, col_index_num, [range_lookup])",
        metadata
    )
    print(f"Added chunk: {chunk_id}")
    
    # Search for similar content
    results = rag.search_similar("How do I use vlookup in Excel?