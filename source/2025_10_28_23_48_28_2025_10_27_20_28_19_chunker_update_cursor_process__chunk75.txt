Args:
        file_path: Original file path
        chunks: List of content chunks
        config: Configuration dictionary
        
    Returns:
        List of created output file paths
    """
    try:
        timestamp = datetime.now().strftime('%Y_%m_%d_%H_%M_%S')
        output_dir = Path(config['output_dir']) / f'{timestamp}_{file_path.stem}'
        output_dir.mkdir(parents=True, exist_ok=True)
        
        output_files = []
        
        for i, chunk in enumerate(chunks, 1):
            chunk_file = output_dir / f'{timestamp}_chunk_{i}.txt'
            with open(chunk_file, 'w', encoding='utf-8') as f:
                f.write(chunk)
            output_files.append(str(chunk_file))
        
        # Create transcript file
        transcript_file = output_dir / f'{timestamp}_transcript.md'
        with open(transcript_file, 'w', encoding='utf-8') as f:
            f.write(f"# Transcript: {file_path.name}\n\n")
            f.write(f"**Processing Time:** {timestamp}\n")
            f.write(f"**Chunks Created:** {len(chunks)}\n\n")
            f.write("## Content\n\n")
            f.write("\n\n---\n\n".join(chunks))
        
        output_files.append(str(transcript_file))
        
        return output_files
        
    except Exception as e:
        logger.error(f"Error creating output files: {e}")
        return []

def archive_processed_file(file_path: Path, config: Dict[str, Any]) -> str:
    """
    Archive the processed file. Args:
        file_path: Path to the file
        config: Configuration dictionary
        
    Returns:
        Archive path
    """
    try:
        archive_dir = Path(config['archive_dir'])
        archive_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        archive_path = archive_dir / f"{file_path.stem}_{timestamp}{file_path.suffix}"
        
        file_path.rename(archive_path)
        
        return str(archive_path)
        
    except Exception as e:
        logger.error(f"Error archiving file: {e}")
        return ""

def copy_outputs_to_source(output_files: List[str], dest_path: str) -> None:
    """
    Copy output files back to source folder. Args:
        output_files: List of output file paths
        dest_path: Destination path
    """
    try:
        dest_dir = Path(dest_path).parent
        dest_dir.mkdir(parents=True, exist_ok=True)
        
        for output_file in output_files:
            src_path = Path(output_file)
            dst_path = dest_dir / src_path.name
            shutil.copy2(src_path, dst_path)
            logger.info(f"Copied output to source: {dst_path}")
        
    except Exception as e:
        logger.error(f"Error copying outputs to source: {e}")

def extract_keywords(text: str, max_keywords: int = 5) -> List[str]:
    """
    Extract keywords from text. Args:
        text: Input text
        max_keywords: Maximum number of keywords
        
    Returns:
        List of keywords
    """
    try:
        import re
        from collections import Counter
        
        # Simple keyword extraction
        words = re.findall(r'\b\w+\b', text.lower())
        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
        keywords = [word for word in words if word not in stop_words and len(word) > 2]
        
        counter = Counter(keywords)
        return [word for word, _ in counter.most_common(max_keywords)]
        
    except Exception as e:
        logger.error(f"Error extracting keywords: {e}")
        return []

def is_binary_file(file_path: Path) -> bool:
    """Check if file is binary.""" try:
        with open(file_path, 'rb') as f:
            chunk = f.read(1024)
            return b'\0' in chunk
    except Exception:
        return False

def detect_encoding(file_path: Path) -> str:
    """Detect file encoding.""" try:
        import chardet
        with open(file_path, 'rb') as f:
            raw_data = f.read(10000)
            result = chardet.detect(raw_data)
            return result.get('encoding', 'utf-8')
    except Exception:
        return 'utf-8'

def count_lines(file_path: Path) -> int:
    """Count lines in file.""" try:
        with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
            return sum(1 for _ in f)
    except Exception:
        return 0

# Celery worker startup
if __name__ == '__main__':
    app.start()

```

Now let me add comprehensive task definitions with chains and error handling:


```python
"""
Celery Task Queue System for Chunker_v2
Handles async file processing with race condition prevention and batching
"""

import os
import json
import logging
import time
import traceback
from typing import Dict, Any, List, Optional
from datetime import datetime
from pathlib import Path
from celery import Celery, chain, group
from celery.exceptions import Retry, WorkerLostError
from celery.signals import task_prerun, task_postrun, task_failure
import redis
from watchdog.events import FileSystemEvent

logger = logging.getLogger(__name__)

# Celery configuration
CELERY_CONFIG = {
    'broker_url': os.getenv('REDIS_URL', 'redis://localhost:6379/0'),
    'result_backend': os.getenv('REDIS_URL', 'redis://localhost:6379/0'),
    'task_serializer': 'json',
    'accept_content': ['json'],
    'result_serializer': 'json',
    'timezone': 'UTC',
    'enable_utc': True,
    'task_track_started': True,
    'task_time_limit': 300,  # 5 minutes
    'task_soft_time_limit': 240,  # 4 minutes
    'worker_prefetch_multiplier': 1,
    'task_acks_late': True,
    'worker_disable_rate_limits': True,
}

# Initialize Celery app
app = Celery('chunker_v2')
app.config_from_object(CELERY_CONFIG)

# Redis client for coordination
redis_client = redis.Redis.from_url(CELERY_CONFIG['broker_url'])

class TaskCoordinator:
    """
    Coordinates file processing tasks to prevent race conditions and enable batching.
    """ def __init__(self, config: Dict[str, Any]):
        """
        Initialize task coordinator. Args:
            config: Configuration dictionary
        """
        self.config = config
        self.debounce_window = config.get("debounce_window", 1.0)
        self.max_workers = config.get("max_workers", 4)
        self.batch_size = config.get("batch_size", 10)
        self.failed_dir = config.get("failed_dir", "03_archive/failed")
        
        # Create failed directory
        os.makedirs(self.failed_dir, exist_ok=True)
        
        logger.info("Initialized TaskCoordinator")
    
    def should_process_file(self, file_path: str) -> bool:
        """
        Check if file should be processed based on debouncing rules. Args:
            file_path: Path to the file
            
        Returns:
            True if should process, False otherwise
        """
        try:
            current_time = time.time()
            file_key = f"file_processing:{file_path}"
            
            # Check if file is already being processed
            if redis_client.exists(f"processing:{file_path}"):
                logger.debug(f"File already being processed: {file_path}")
                return False
            
            # Check debounce window
            last_processed = redis_client.get(f"last_processed:{file_path}")
            if last_processed:
                last_time = float(last_processed.decode())
                if current_time - last_time < self.debounce_window:
                    logger.debug(f"File within debounce window: {file_path}")
                    return False
            
            # Mark as processing
            redis_client.setex(f"processing:{file_path}", 300, current_time)  # 5 min expiry
            
            return True
            
        except Exception as e:
            logger.error(f"Error checking file processing status: {e}")
            return False
    
    def mark_file_completed(self, file_path: str) -> None:
        """
        Mark file as completed processing. Args:
            file_path: Path to the file
        """
        try:
            current_time = time.time()
            redis_client.setex(f"last_processed:{file_path}", 3600, current_time)  # 1 hour expiry
            redis_client.delete(f"processing:{file_path}")
            logger.debug(f"Marked file as completed: {file_path}")
        except Exception as e:
            logger.error(f"Error marking file completed: {e}")
    
    def mark_file_failed(self, file_path: str, error: str) -> None:
        """
        Mark file as failed processing. Args:
            file_path: Path to the file
            error: Error message
        """
        try:
            redis_client.delete(f"processing:{file_path}")
            redis_client.setex(f"failed:{file_path}", 3600, error)  # 1 hour expiry
            logger.error(f"Marked file as failed: {file_path} - {error}")
        except Exception as e:
            logger.error(f"Error marking file failed: {e}")

# Task routing and configuration
CELERY_CONFIG.update({
    'task_routes': {
        'celery_tasks.process_file_task': {'queue': 'file_processing'},
        'celery_tasks.batch_process_files_task': {'queue': 'batch_processing'},
        'celery_tasks.add_to_rag_task': {'queue': 'rag_processing'},
        'celery_tasks.evaluate_rag_task': {'queue': 'evaluation'},
        'celery_tasks.health_check_task': {'queue': 'monitoring'},
    },
    'task_annotations': {
        'celery_tasks.process_file_task': {'rate_limit': '10/m'},
        'celery_tasks.batch_process_files_task': {'rate_limit': '5/m'},
        'celery_tasks.add_to_rag_task': {'rate_limit': '20/m'},
        'celery_tasks.evaluate_rag_task': {'rate_limit': '2/m'},
    },
    'beat_schedule': {
        'health-check': {
            'task': 'celery_tasks.health_check_task',
            'schedule': 60.0,  # Every minute
        },
        'cleanup-old-tasks': {
            'task': 'celery_tasks.cleanup_old_tasks',
            'schedule': 3600.0,  # Every hour
        },
    },
})

# Initialize Celery app
app = Celery('chunker_v2')
app.config_from_object(CELERY_CONFIG)

# Redis client for coordination
redis_client = redis.Redis.from_url(CELERY_CONFIG['broker_url'])

# Task signals for monitoring
@task_prerun.connect
def task_prerun_handler(sender=None, task_id=None, task=None, args=None, kwargs=None, **kwds):
    """Log task start.""" logger.info(f"Task {task.name} started: {task_id}")

@task_postrun.connect
def task_postrun_handler(sender=None, task_id=None, task=None, args=None, kwargs=None, retval=None, state=None, **kwds):
    """Log task completion.""" logger.info(f"Task {task.name} completed: {task_id} - State: {state}")

@task_failure.connect
def task_failure_handler(sender=None, task_id=None, exception=None, traceback=None, einfo=None, **kwds):
    """Log task failures.""" logger.error(f"Task {sender.name} failed: {task_id} - {exception}")

@app.task(bind=True, max_retries=3, default_retry_delay=60)
def process_file_task(self, file_path: str, dest_path: Optional[str], 
                     event_type: str, config: Dict[str, Any]) -> Dict[str, Any]:
    """
    Celery task for processing a single file. Args:
        file_path: Path to the file
        dest_path: Destination path (for moved events)
        event_type: Type of event (moved, created)
        config: Configuration dictionary
        
    Returns:
        Processing result dictionary
    """
    coordinator = TaskCoordinator(config)
    
    try:
        # Check if file should be processed
        if not coordinator.should_process_file(file_path):
            return {"status": "skipped", "reason": "debounced_or_processing"}
        
        # Process the file
        result = process_file_with_rag(file_path, dest_path, event_type, config)
        
        # Mark as completed
        coordinator.mark_file_completed(file_path)
        
        return result
        
    except Exception as e:
        logger.error(f"Task failed for {file_path}: {e}")
        
        # Mark as failed
        coordinator.mark_file_failed(file_path, str(e))
        
        # Retry if possible
        if self.request.retries < self.max_retries:
            logger.info(f"Retrying task for {file_path} (attempt {self.request.retries + 1})")
            raise self.retry(countdown=60 * (self.request.retries + 1))
        
        return {"status": "failed", "error": str(e)}

@app.task
def batch_process_files_task(file_paths: List[str], config: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    Celery task for batch processing multiple files. Args:
        file_paths: List of file paths
        config: Configuration dictionary
        
    Returns:
        List of processing results
    """
    results = []
    
    for file_path in file_paths:
        try:
            result = process_file_task.delay(file_path, None, "batch", config)
            results.append({"file_path": file_path, "task_id": result.id})
        except Exception as e:
            logger.error(f"Failed to queue task for {file_path}: {e}")
            results.append({"file_path": file_path, "error": str(e)})
    
    return results

def process_file_with_rag(file_path: str, dest_path: Optional[str], 
                         event_type: str, config: Dict[str, Any]) -> Dict[str, Any]:
    """
    Process a file with RAG integration. Args:
        file_path: Path to the file
        dest_path: Destination path (for moved events)
        event_type: Type of event
        config: Configuration dictionary
        
    Returns:
        Processing result dictionary
    """
    try:
        file_path_obj = Path(file_path)
        
        # Check if file exists and is readable
        if not file_path_obj.exists():
            return {"status": "error", "message": "File does not exist"}
        
        if not os.access(file_path_obj, os.R_OK):
            return {"status": "error", "message": "File not readable"}
        
        # Extract metadata
        metadata = extract_comprehensive_metadata(file_path_obj)
        
        # Process file content
        content = extract_file_content(file_path_obj)
        
        if not content:
            return {"status": "error", "message": "No content extracted"}
        
        # Chunk the content
        chunks = chunk_content(content, config)
        
        if not chunks:
            return {"status": "error", "message": "No chunks created"}
        
        # Add to vector database
        vector_results = add_chunks_to_vector_db(chunks, metadata, config)
        
        # Create output files
        output_results = create_output_files(file_path_obj, chunks, config)
        
        # Handle file movement/archiving
        archive_result = archive_processed_file(file_path_obj, config)
        
        # Copy outputs back to source folder if needed
        if dest_path and event_type == "moved":
            copy_outputs_to_source(output_results, dest_path)
        
        return {
            "status": "success",
            "file_path": file_path,
            "chunks_created": len(chunks),
            "vector_results": vector_results,
            "output_files": output_results,
            "archive_result": archive_result,
            "metadata": metadata
        }
        
    except Exception as e:
        logger.error(f"Error processing file {file_path}: {e}")
        return {"status": "error", "message": str(e)}

def extract_comprehensive_metadata(file_path: Path) -> Dict[str, Any]:
    """
    Extract comprehensive metadata from a file. Args:
        file_path: Path to the file
        
    Returns:
        Metadata dictionary
    """
    try:
        stat = file_path.stat()
        
        metadata = {
            "file_info": {
                "name": file_path.name,
                "path": str(file_path),
                "size": stat.st_size,
                "created": datetime.fromtimestamp(stat.st_ctime).isoformat(),
                "modified": datetime.fromtimestamp(stat.st_mtime).isoformat(),
                "extension": file_path.suffix.lower(),
                "is_binary": is_binary_file(file_path)
            },
            "content_metadata": {
                "encoding": detect_encoding(file_path),
                "line_count": count_lines(file_path),
                "word_count": 0,  # Will be updated after content extraction
                "language": "unknown"  # Could be enhanced with language detection
            },
            "extracted_data": {
                "samples": [],
                "summaries": []
            },
            "tags": [],
            "keywords": [],
            "ai_context": {
                "summary": "",
                "category": "unknown",
                "complexity": "medium"
            }
        }
        
        return metadata
        
    except Exception as e:
        logger.error(f"Error extracting metadata from {file_path}: {e}")
        return {}

def extract_file_content(file_path: Path) -> str:
    """
    Extract content from various file types. Args:
        file_path: Path to the file
        
    Returns:
        Extracted content as string
    """
    try:
        extension = file_path.suffix.lower()
        
        if extension == ".xlsx":
            return extract_excel_content(file_path)
        elif extension == ".pdf":
            return extract_pdf_content(file_path)
        elif extension == ".py":
            return extract_python_content(file_path)
        elif extension == ".docx":
            return extract_docx_content(file_path)
        elif extension in [".txt", ".md", ".json", ".csv", ".sql", ".yaml", ".xml", ".log"]:
            return extract_text_content(file_path)
        else:
            logger.warning(f"Unsupported file type: {extension}")
            return ""
            
    except Exception as e:
        logger.error(f"Error extracting content from {file_path}: {e}")
        return ""

def extract_excel_content(file_path: Path) -> str:
    """Extract content from Excel files.""" try:
        import openpyxl
        wb = openpyxl.load_workbook(file_path)
        content = []
        
        for sheet_name in wb.sheetnames:
            sheet = wb[sheet_name]
            content.append(f"Sheet: {sheet_name}")
            
            # Extract data
            for row in sheet.iter_rows(values_only=True):
                if any(cell is not None for cell in row):
                    content.append(str(row))
            
            # Extract formulas
            formulas = []
            for row in sheet.iter_rows():
                for cell in row:
                    if cell.data_type == 'f' and cell.value:
                        formulas.append(f"{cell.coordinate}: {cell.value}")
            
            if formulas:
                content.append(f"Formulas: {formulas}")
        
        return "\n".join(content)
        
    except Exception as e:
        logger.error(f"Error extracting Excel content: {e}")
        return ""

def extract_pdf_content(file_path: Path) -> str:
    """Extract content from PDF files.""" try:
        import PyPDF2
        content = []
        
        with open(file_path, 'rb') as f:
            reader = PyPDF2.PdfReader(f)
            
            for page_num, page in enumerate(reader.pages):
                text = page.extract_text()
                if text:
                    content.append(f"Page {page_num + 1}: {text}")
        
        return "\n".join(content)
        
    except Exception as e:
        logger.error(f"Error extracting PDF content: {e}")
        return ""

def extract_python_content(file_path: Path) -> str:
    """Extract structured content from Python files.""" try:
        import ast
        
        with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
            content = f.read()
        
        # Parse AST
        tree = ast.parse(content)
        
        extracted = []
        
        # Extract functions
        functions = [node.name for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)]
        if functions:
            extracted.append(f"Functions: {functions}")
        
        # Extract classes
        classes = [node.name for node in ast.walk(tree) if isinstance(node, ast.ClassDef)]
        if classes:
            extracted.append(f"Classes: {classes}")
        
        # Extract imports
        imports = []
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                imports.extend([alias.name for alias in node.names])
            elif isinstance(node, ast.ImportFrom):
                imports.append(node.module)
        
        if imports:
            extracted.append(f"Imports: {imports}")
        
        # Add docstrings
        docstrings = []
        for node in ast.walk(tree):
            if isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.Module)):
                if (ast.get_docstring(node)):
                    docstrings.append(f"{node.name}: {ast.get_docstring(node)}")
        
        if docstrings:
            extracted.append(f"Docstrings: {docstrings}")
        
        # Combine with original content
        extracted.append(f"Source Code:\n{content}")
        
        return "\n".join(extracted)
        
    except Exception as e:
        logger.error(f"Error extracting Python content: {e}")
        return ""

def extract_docx_content(file_path: Path) -> str:
    """Extract content from Word documents.""" try:
        import docx
        doc = docx.Document(file_path)
        
        content = []
        for paragraph in doc.paragraphs:
            if paragraph.text.strip():
                content.append(paragraph.text)
        
        return "\n".join(content)
        
    except Exception as e:
        logger.error(f"Error extracting DOCX content: {e}")
        return ""

def extract_text_content(file_path: Path) -> str:
    """Extract content from text-based files.""" try:
        with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
            return f.read()
    except Exception as e:
        logger.error(f"Error extracting text content: {e}")
        return ""

def chunk_content(content: str, config: Dict[str, Any]) -> List[str]:
    """
    Chunk content into manageable pieces. Args:
        content: Content to chunk
        config: Configuration dictionary
        
    Returns:
        List of content chunks
    """
    try:
        chunk_size = config.get("chunk_size", 1000)
        chunk_overlap = config.get("chunk_overlap", 200)
        
        # Simple chunking by character count with overlap
        chunks = []
        start = 0
        
        while start < len(content):
            end = min(start + chunk_size, len(content))
            chunk = content[start:end]
            
            if chunk.strip():
                chunks.append(chunk)
            
            start = end - chunk_overlap
        
        return chunks
        
    except Exception as e:
        logger.error(f"Error chunking content: {e}")
        return []

def add_chunks_to_vector_db(chunks: List[str], metadata: Dict[str, Any], 
                           config: Dict[str, Any]) -> Dict[str, Any]:
    """
    Add chunks to vector database. Args:
        chunks: List of content chunks
        metadata: File metadata
        config: Configuration dictionary
        
    Returns:
        Vector database operation results
    """
    try:
        if not config.get("rag_enabled", False):
            return {"status": "disabled"}
        
        # Try ChromaDB first, fallback to FAISS
        try:
            from rag_integration import ChromaRAG
            chroma_rag = ChromaRAG(persist_directory=config.get("chroma_persist_dir", "./chroma_db"))
            
            results = []
            for i, chunk in enumerate(chunks):
                chunk_metadata = {
                    "file_name": metadata["file_info"]["name"],
                    "file_type": metadata["file_info"]["extension"],
                    "chunk_index": i + 1,
                    "timestamp": datetime.now().isoformat(),
                    "keywords": extract_keywords(chunk),
                    "file_size": metadata["file_info"]["size"]
                }
                
                chunk_id = chroma_rag.add_chunk(chunk, chunk_metadata)
                results.append(chunk_id)
            
            return {"status": "success", "method": "chromadb", "chunk_ids": results}
            
        except ImportError:
            # Fallback to FAISS
            from ollama_integration import initialize_ollama_rag
            rag_system = initialize_ollama_rag()
            
            chunk_metadatas = []
            for i, chunk in enumerate(chunks):
                chunk_metadata = {
                    "source_file": metadata["file_info"]["name"],
                    "file_type": metadata["file_info"]["extension"],
                    "chunk_index": i + 1,
                    "timestamp": datetime.now().isoformat(),
                    "keywords": extract_keywords(chunk),
                    "file_size": metadata["file_info"]["size"]
                }
                chunk_metadatas.append(chunk_metadata)
            
            rag_system.add_documents(chunks, chunk_metadatas)
            rag_system.save_index()
            
            return {"status": "success", "method": "faiss", "chunks_added": len(chunks)}
        
    except Exception as e:
        logger.error(f"Error adding chunks to vector DB: {e}")
        return {"status": "error", "message": str(e)}

def create_output_files(file_path: Path, chunks: List[str], config: Dict[str, Any]) -> List[str]:
    """
    Create output files for chunks. Args:
        file_path: Original file path
        chunks: List of content chunks
        config: Configuration dictionary
        
    Returns:
        List of created output file paths
    """
    try:
        timestamp = datetime.now().strftime('%Y_%m_%d_%H_%M_%S')
        output_dir = Path(config['output_dir']) / f'{timestamp}_{file_path.stem}'
        output_dir.mkdir(parents=True, exist_ok=True)
        
        output_files = []
        
        for i, chunk in enumerate(chunks, 1):
            chunk_file = output_dir / f'{timestamp}_chunk_{i}.txt'
            with open(chunk_file, 'w', encoding='utf-8') as f:
                f.write(chunk)
            output_files.append(str(chunk_file))
        
        # Create transcript file
        transcript_file = output_dir / f'{timestamp}_transcript.md'
        with open(transcript_file, 'w', encoding='utf-8') as f:
            f.write(f"# Transcript: {file_path.name}\n\n")
            f.write(f"**Processing Time:** {timestamp}\n")
            f.write(f"**Chunks Created:** {len(chunks)}\n\n")
            f.write("## Content\n\n")
            f.write("\n\n---\n\n".join(chunks))
        
        output_files.append(str(transcript_file))
        
        return output_files
        
    except Exception as e:
        logger.error(f"Error creating output files: {e}")
        return []

def archive_processed_file(file_path: Path, config: Dict[str, Any]) -> str:
    """
    Archive the processed file. Args:
        file_path: Path to the file
        config: Configuration dictionary
        
    Returns:
        Archive path
    """
    try:
        archive_dir = Path(config['archive_dir'])
        archive_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        archive_path = archive_dir / f"{file_path.stem}_{timestamp}{file_path.suffix}"
        
        file_path.rename(archive_path)
        
        return str(archive_path)
        
    except Exception as e:
        logger.error(f"Error archiving file: {e}")
        return ""

def copy_outputs_to_source(output_files: List[str], dest_path: str) -> None:
    """
    Copy output files back to source folder. Args:
        output_files: List of output file paths
        dest_path: Destination path
    """
    try:
        dest_dir = Path(dest_path).parent
        dest_dir.mkdir(parents=True, exist_ok=True)
        
        for output_file in output_files:
            src_path = Path(output_file)
            dst_path = dest_dir / src_path.name
            shutil.copy2(src_path, dst_path)
            logger.info(f"Copied output to source: {dst_path}")
        
    except Exception as e:
        logger.error(f"Error copying outputs to source: {e}")

def extract_keywords(text: str, max_keywords: int = 5) -> List[str]:
    """
    Extract keywords from text. Args:
        text: Input text
        max_keywords: Maximum number of keywords
        
    Returns:
        List of keywords
    """
    try:
        import re
        from collections import Counter
        
        # Simple keyword extraction
        words = re.findall(r'\b\w+\b', text.lower())
        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
        keywords = [word for word in words if word not in stop_words and len(word) > 2]
        
        counter = Counter(keywords)
        return [word for word, _ in counter.most_common(max_keywords)]
        
    except Exception as e:
        logger.error(f"Error extracting keywords: {e}")
        return []

def is_binary_file(file_path: Path) -> bool:
    """Check if file is binary.""" try:
        with open(file_path, 'rb') as f:
            chunk = f.read(1024)
            return b'\0' in chunk
    except Exception:
        return False

def detect_encoding(file_path: Path) -> str:
    """Detect file encoding.""" try:
        import chardet
        with open(file_path, 'rb') as f:
            raw_data = f.read(10000)
            result = chardet.detect(raw_data)
            return result.get('encoding', 'utf-8')
    except Exception:
        return 'utf-8'

def count_lines(file_path: Path) -> int:
    """Count lines in file."""