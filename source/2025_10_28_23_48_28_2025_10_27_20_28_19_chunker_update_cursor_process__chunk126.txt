Args:
        file_path: Path to the file to process
        config: Configuration dictionary
        
    Returns:
        True if processing was successful, False otherwise
    """
    try:
        if CELERY_AVAILABLE and config.get("celery_enabled", True):
            # Use Celery task chain for advanced processing
            logger.info(f"Queuing file for Celery processing: {file_path}")
            
            task_id = process_file_with_celery_chain(
                str(file_path),
                None,  # dest_path
                "watcher",  # event_type
                config
            )
            
            logger.info(f"File queued for Celery processing: {file_path} (task_id: {task_id})")
            
            # For immediate feedback, we'll return True and let Celery handle the rest
            # The actual processing will be handled by Celery workers
            return True
            
        else:
            # Fallback to direct processing
            logger.info(f"Using direct processing (Celery not available): {file_path}")
            return process_file_enhanced(file_path, config)
            
    except Exception as e:
        logger.error(f"Error in Celery processing: {e}")
        # Fallback to direct processing
        logger.info(f"Falling back to direct processing: {file_path}")
        return process_file_enhanced(file_path, config)

def process_file_enhanced(file_path, config):
    """Enhanced file processing with comprehensive tracking"""
    start_time = time.time()
    department_config = get_department_config(file_path)
    department = department_config.get("department", "default")
    
    # Safe filename logging to avoid encoding issues
    safe_filename = file_path.name.encode('ascii', 'replace').decode('ascii')
    logger.info(f"Processing file: {safe_filename} (Department: {department})")
    
    try:
        # Wait for file stability
        if not wait_for_file_stability(file_path):
            error_msg = f"File not stable, skipping: {file_path.name}"
            logger.error(error_msg)
            if db:
                try:
                    db.log_error("FileStabilityError", error_msg, filename=str(file_path))
                except Exception as db_error:
                    logger.warning(f"Failed to log stability error to database: {db_error}")
            return False

        # Read file with multiple attempts using appropriate processor
        text = None
        original_size = 0
        file_type = file_path.suffix.lower()
        
        for attempt in range(3):
            try:
                # Read file content first
                if file_type in [".txt", ".md", ".json", ".csv", ".sql", ".yaml", ".xml", ".log", ".py"]:
                    with open(file_path, "r", encoding="utf-8", errors='replace') as f:
                        text = f.read()
                elif file_type in [".xlsx", ".pdf", ".docx"]:
                    # Binary files - use processors directly
                    processor = get_file_processor(file_type)
                    text = processor(file_path)
                else:
                    # Default to text reading
                    with open(file_path, "r", encoding="utf-8", errors='replace') as f:
                        text = f.read()
                
                # Process text content if needed
                if text and file_type in [".py", ".yaml", ".xml", ".log", ".sql"]:
                    processor = get_file_processor(file_type)
                    text = processor(text)
                
                original_size = len(text.encode('utf-8'))
                break
            except Exception as e:
                logger.warning(f"Read attempt {attempt+1} failed for {file_path.name}: {e}")
                if attempt < 2:
                    time.sleep(1)
        
        if not text:
            error_msg = f"Could not read {file_path.name} after 3 attempts"
            logger.error(error_msg)
            if db:
                try:
                    db.log_error("FileReadError", error_msg, filename=str(file_path))
                except Exception as db_error:
                    logger.warning(f"Failed to log read error to database: {db_error}")
            return False

        # Validate input text
        min_size = department_config.get("min_file_size_bytes", 100)
        if len(text.strip()) < min_size:
            error_msg = f"File too short ({len(text)} chars), skipping: {file_path.name}"
            logger.warning(error_msg)
            if db:
                try:
                    db.log_processing(str(file_path), original_size, 0, 0, 
                                    time.time() - start_time, False, error_msg, department)
                except Exception as db_error:
                    logger.warning(f"Failed to log processing to database: {db_error}")
            return False

        # Chunk the text
        sentence_limit = department_config.get("chunk_size", 100)
        chunks = chunk_text_enhanced(text, sentence_limit, department_config)
        
        if not chunks:
            error_msg = f"No valid chunks created for {file_path.name}"
            logger.error(error_msg)
            if db:
                try:
                    db.log_processing(str(file_path), original_size, 0, 0, 
                                    time.time() - start_time, False, error_msg, department)
                except Exception as db_error:
                    logger.warning(f"Failed to log processing to database: {db_error}")
            return False

        # Prepare output with organized folder structure
        timestamp = datetime.now().strftime("%Y_%m_%d_%H_%M_%S")
        
        # Enhanced filename sanitization
        import re
        clean_base = Path(file_path.name).stem
        # Remove or replace problematic characters
        clean_base = re.sub(r'[^\w\s-]', '', clean_base)  # Remove special chars except word chars, spaces, hyphens
        clean_base = clean_base.replace(" ", "_")  # Replace spaces with underscores
        clean_base = re.sub(r'_+', '_', clean_base)  # Replace multiple underscores with single
        clean_base = clean_base.strip('_')  # Remove leading/trailing underscores
        
        # Ensure the name isn't too long (Windows path limit)
        # Account for timestamp prefix (19 chars) + separators + chunk files
        max_filename_length = 50  # Reduced to account for timestamp prefix
        if len(clean_base) > max_filename_length:
            clean_base = clean_base[:max_filename_length]
        
        output_folder = config.get("output_dir", "output")
        
        # Create folder named after the source file with timestamp prefix
        timestamp_prefix = datetime.now().strftime("%Y_%m_%d_%H_%M_%S")
        file_output_folder = Path(output_folder) / f"{timestamp_prefix}_{clean_base}"
        os.makedirs(file_output_folder, exist_ok=True)
        
        chunk_files = []
        valid_chunks = 0
        total_chunk_size = 0

        # Write chunks with validation
        for i, chunk in enumerate(chunks):
            if validate_chunk_content_enhanced(chunk, department_config=department_config):
                chunk_file = file_output_folder / f"{timestamp}_{clean_base}_chunk{i+1}.txt"
                try:
                    with open(chunk_file, "w", encoding="utf-8") as cf:
                        cf.write(chunk)
                    # Verify file was written correctly
                    written_size = os.path.getsize(chunk_file)
                    if written_size > 0:
                        chunk_files.append(chunk_file)
                        valid_chunks += 1
                        total_chunk_size += written_size
                        logger.info(f"Created chunk: {chunk_file.name} ({len(chunk)} chars, {written_size} bytes)")
                    else:
                        logger.warning(f"Zero-byte chunk prevented: {chunk_file.name}")
                        session_stats["zero_byte_prevented"] += 1
                        os.remove(chunk_file)
                except Exception as e:
                    logger.error(f"Failed to write chunk {i+1} for {file_path.name}: {e}")
                    if db:
                        try:
                            db.log_error("ChunkWriteError", str(e), traceback.format_exc(), str(file_path))
                        except Exception as db_error:
                            logger.warning(f"Failed to log chunk write error to database: {db_error}")
            else:
                logger.warning(f"Invalid chunk {i+1} skipped for {file_path.name}")

        # Concatenate all chunk files into a final transcript
        if chunk_files:
            # Use .md extension for admin files, .txt for others
            if department == "admin":
                transcript_file = file_output_folder / f"{timestamp}_{clean_base}_transcript.md"
            else:
                transcript_file = file_output_folder / f"{timestamp}_{clean_base}_transcript.txt"
            
            try:
                with open(transcript_file, "w", encoding="utf-8") as tf:
                    # Add markdown header for admin files
                    if department == "admin":
                        tf.write(f"# {clean_base.replace('_', ' ').title()}\n\n")
                        tf.write(f"**Processing Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                        tf.write(f"**Source File:** {file_path.name}\n")
                        tf.write(f"**Total Chunks:** {len(chunk_files)}\n\n")
                        tf.write("---\n\n")
                    
                    for chunk_file in chunk_files:
                        with open(chunk_file, "r", encoding="utf-8") as cf:
                            tf.write(cf.read())
                            tf.write("\n\n")
                logger.info(f"Final transcript created: {transcript_file.name}")
            except Exception as e:
                logger.error(f"Failed to create final transcript for {file_path.name}: {e}")

        session_stats["chunks_created"] += valid_chunks
        session_stats["total_bytes_created"] += total_chunk_size
        
        # Update department breakdown
        if department not in session_stats["department_breakdown"]:
            session_stats["department_breakdown"][department] = {
                "files": 0, "chunks": 0, "errors": 0
            }
        session_stats["department_breakdown"][department]["files"] += 1
        session_stats["department_breakdown"][department]["chunks"] += valid_chunks
        
        if not chunk_files:
            error_msg = f"No valid chunks created for {file_path.name}"
            logger.warning(error_msg)
            if db:
                try:
                    db.log_processing(str(file_path), original_size, 0, 0, 
                                    time.time() - start_time, False, error_msg, department)
                except Exception as db_error:
                    logger.warning(f"Failed to log processing to database: {db_error}")
            return False

        # RAG Integration - Add chunks to ChromaDB vector database
        if config.get("rag_enabled", False):
            try:
                logger.info(f"Adding {len(chunks)} chunks to ChromaDB for {file_path.name}")
                
                chunks_added = 0
                for i, chunk in enumerate(chunks):
                    # Apply security redaction if enabled
                    if department_config.get("enable_redaction", False):
                        chunk = redact_sensitive_data(chunk)
                    
                    metadata = {
                        "file_name": file_path.name,
                        "file_type": file_path.suffix,
                        "chunk_index": i + 1,
                        "timestamp": datetime.now().isoformat(),
                        "department": department,
                        "keywords": extract_keywords(chunk) if extract_keywords else [],
                        "file_size": file_path.stat().st_size,
                        "processing_time": time.time() - start_time
                    }
                    
                    chunk_id = safe_chroma_add(chunk, metadata, config)
                    if chunk_id:
                        chunks_added += 1
                        logger.debug(f"Added chunk {i+1} to ChromaDB: {chunk_id}")
                
                if chunks_added > 0:
                    logger.info(f"Successfully added {chunks_added}/{len(chunks)} chunks to ChromaDB")
                else:
                    logger.warning("No chunks were added to ChromaDB")
                
            except Exception as e:
                logger.error(f"RAG integration failed: {e}")
                # Don't fail the entire process if RAG fails
                if db:
                    try:
                        db.log_error("RAGError", str(e), traceback.format_exc(), str(file_path))
                    except Exception as db_error:
                        logger.warning(f"Failed to log RAG error to database: {db_error}")

        # Cloud copy with retry
        cloud_success = False
        if config.get("cloud_repo_root"):
            cloud_dir = Path(config["cloud_repo_root"]) / clean_base
            for attempt in range(3):
                if copy_to_cloud_enhanced(chunk_files, cloud_dir, department_config):
                    logger.info(f"Cloud sync successful: {cloud_dir}")
                    cloud_success = True
                    break
                logger.warning(f"Cloud sync attempt {attempt+1} failed for {file_path.name}")
                time.sleep(2)

        # Copy processed files back to source folder
        source_copy_success = False
        if config.get("copy_to_source", False) and chunk_files:
            source_folder = Path(config.get("source_folder", "source"))
            try:
                # Create source folder if it doesn't exist
                source_folder.mkdir(parents=True, exist_ok=True)
                
                files_copied = 0
                
                # Copy chunks if enabled
                if config.get("copy_chunks_only", True):
                    for chunk_file in chunk_files:
                        dest_file = source_folder / chunk_file.name
                        shutil.copy2(chunk_file, dest_file)
                        files_copied += 1
                        logger.info(f"Copied chunk to source: {dest_file.name}")
                
                # Copy transcript if enabled
                if config.get("copy_transcript_only", False) and 'transcript_file' in locals():
                    dest_transcript = source_folder / transcript_file.name
                    shutil.copy2(transcript_file, dest_transcript)
                    files_copied += 1
                    logger.info(f"Copied transcript to source: {dest_transcript.name}")
                
                if files_copied > 0:
                    source_copy_success = True
                    logger.info(f"Successfully copied {files_copied} files to source folder: {source_folder}")
                else:
                    logger.warning("No files were copied to source folder")
                    
            except Exception as e:
                logger.error(f"Failed to copy files to source folder: {e}")
                if db:
                    try:
                        db.log_error("SourceCopyError", str(e), traceback.format_exc(), str(file_path))
                    except Exception as db_error:
                        logger.warning(f"Failed to log source copy error to database: {db_error}")

        # Move to processed
        move_success = move_to_processed_enhanced(file_path, config.get("archive_dir", "processed"), department)
        
        processing_time = time.time() - start_time
        
        # Update performance metrics with enhanced tracking
        if session_stats["files_processed"] > 0:
            current_avg = session_stats["performance_metrics"]["avg_processing_time"]
            session_stats["performance_metrics"]["avg_processing_time"] = (
                (current_avg * session_stats["files_processed"] + processing_time) / 
                (session_stats["files_processed"] + 1)
            )
        else:
            session_stats["performance_metrics"]["avg_processing_time"] = processing_time
        
        # Track processing speed
        if not hasattr(session_stats["performance_metrics"], "files_per_minute"):
            session_stats["performance_metrics"]["files_per_minute"] = 0
        
        # Calculate files per minute
        elapsed_time = time.time() - session_stats.get("session_start_time", time.time())
        if elapsed_time > 0:
            session_stats["performance_metrics"]["files_per_minute"] = (
                session_stats["files_processed"] * 60 / elapsed_time
            )
        
        if move_success:
            session_stats["files_processed"] += 1
            logger.info(f"File processing complete: {file_path.name} -> {valid_chunks} chunks ({processing_time:.2f}s)")
            
        # Batch database operations to reduce locking
        if db and config.get("database_batch_size", 10) > 1:
            # Store processing data for batch logging
            if not hasattr(session_stats, 'pending_db_operations'):
                session_stats['pending_db_operations'] = []
            
            session_stats['pending_db_operations'].append({
                'file_path': str(file_path),
                'original_size': original_size,
                'valid_chunks': valid_chunks,
                'total_chunk_size': total_chunk_size,
                'processing_time': processing_time,
                'success': move_success,
                'department': department
            })
            
            # Process batch when it reaches the limit
            if len(session_stats['pending_db_operations']) >= config.get("database_batch_size", 10):
                try:
                    for op in session_stats['pending_db_operations']:
                        db.log_processing(op['file_path'], op['original_size'], 
                                        op['valid_chunks'], op['total_chunk_size'],
                                        op['processing_time'], op['success'], 
                                        None, op['department'], department_config)
                    session_stats['pending_db_operations'] = []
                    logger.debug(f"Batch logged {config.get('database_batch_size', 10)} operations to database")
                except Exception as db_error:
                    logger.warning(f"Failed to batch log to database: {db_error}")
                    session_stats['pending_db_operations'] = []
        else:
            # Individual database logging (fallback)
            if db:
                try:
                    db.log_processing(str(file_path), original_size, valid_chunks, total_chunk_size,
                                    processing_time, True, None, department, department_config)
                except Exception as db_error:
                    logger.warning(f"Failed to log processing to database: {db_error}")
        
        return move_success
        
    except Exception as e:
        error_msg = f"Critical error processing {file_path.name}: {str(e)}"
        logger.exception(error_msg)
        
        # Log to database and send alert with retry
        if db:
            try:
                db.log_error("ProcessingError", str(e), traceback.format_exc(), str(file_path))
            except Exception as db_error:
                logger.warning(f"Failed to log processing error to database: {db_error}")
        
        try:
            notifications.send_error_alert(error_msg, str(file_path), traceback.format_exc())
        except Exception as notify_error:
            logger.warning(f"Failed to send error alert: {notify_error}")
        
        # Update department breakdown
        department = get_department_config(file_path).get("department", "default")
        if department not in session_stats["department_breakdown"]:
            session_stats["department_breakdown"][department] = {"files": 0, "chunks": 0, "errors": 0}
        session_stats["department_breakdown"][department]["errors"] += 1
        
        session_stats["errors"] += 1
        return False

def process_files_parallel(file_list, config):
    """Process multiple files in parallel with optimized settings"""
    if not file_list:
        return []
    
    # Use more workers for large batches, fewer for small batches
    batch_size = config.get("batch_size", 50)
    if len(file_list) >= batch_size:
        max_workers = min(12, multiprocessing.cpu_count() * 2, len(file_list))
    else:
        max_workers = min(8, multiprocessing.cpu_count(), len(file_list))
    
    logger.info(f"Processing {len(file_list)} files with {max_workers} workers (batch size: {batch_size})")
    
    results = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Submit all jobs (use Celery if available, otherwise direct processing)
        if CELERY_AVAILABLE and config.get("celery_enabled", True):
            # Use Celery for processing
            logger.info(f"Using Celery for processing {len(file_list)} files")
            for file_path in file_list:
                try:
                    success = process_file_with_celery(file_path, config)
                    results.append(success)
                    session_stats["parallel_jobs_completed"] += 1
                except Exception as e:
                    logger.error(f"Celery processing failed for {file_path}: {e}")
                    results.append(False)
        else:
            # Use direct processing with ThreadPoolExecutor
            future_to_file = {
                executor.submit(process_file_enhanced, file_path, config): file_path 
                for file_path in file_list
            }
        
            # Collect results with timeout (only for direct processing)
            for future in future_to_file:
                try:
                    result = future.result(timeout=300)  # 5 minute timeout per file
                    results.append(result)
                    session_stats["parallel_jobs_completed"] += 1
                except Exception as e:
                    file_path = future_to_file[future]
                    logger.error(f"Parallel processing failed for {file_path}: {e}")
                    if db:
                        try:
                            db.log_error("ParallelProcessingError", str(e), traceback.format_exc(), str(file_path))
                        except Exception as db_error:
                            logger.warning(f"Failed to log parallel processing error to database: {db_error}")
                    results.append(False)
    
    successful = sum(1 for r in results if r)
    logger.info(f"Parallel processing complete: {successful}/{len(file_list)} files successful")
    return results

def wait_for_file_stability(file_path, min_wait=1, max_wait=15):
    """Enhanced file stability check with faster processing"""
    file_size = 0
    stable_count = 0
    wait_time = 0
    
    try:
        initial_size = os.path.getsize(file_path)
        if initial_size < 1000:
            target_stable = 1  # Reduced from 2
            check_interval = 0.3  # Reduced from 0.5
        else:
            target_stable = 2  # Reduced from 3
            check_interval = 0.5  # Reduced from 1
    except:
        target_stable = 1  # Reduced from 2
        check_interval = 0.5  # Reduced from 1
    
    while wait_time < max_wait:
        try:
            current_size = os.path.getsize(file_path)
            if current_size == file_size:
                stable_count += 1
                if stable_count >= target_stable:
                    logger.info(f"File stable after {wait_time:.1f}s: {file_path.name}")
                    return True
            else:
                file_size = current_size
                stable_count = 0
            
            time.sleep(check_interval)
            wait_time += check_interval
            
        except FileNotFoundError:
            logger.warning(f"File disappeared during stability check: {file_path}")
            return False
    
    logger.warning(f"File stability timeout after {max_wait}s: {file_path.name}")
    return True

def copy_to_cloud_enhanced(chunk_files, cloud_dir, department_config):
    """Enhanced cloud copy with department-specific handling"""
    try:
        os.makedirs(cloud_dir, exist_ok=True)
        successful_copies = 0
        
        # Create department-specific metadata
        metadata = {
            "department": department_config.get("department", "default"),
            "processing_time": datetime.now().isoformat(),
            "chunk_count": len(chunk_files),
            "audit_level": department_config.get("audit_level", "basic")
        }
        
        # Write metadata file
        metadata_file = cloud_dir / "metadata.json"
        with open(metadata_file, 'w') as f:
            json.dump(metadata, f, indent=2)
        
        for file_path in chunk_files:
            file_size = os.path.getsize(file_path)
            if file_size > 0:
                shutil.copy(file_path, cloud_dir)
                successful_copies += 1
            else:
                logger.warning(f"Skipped zero-byte file: {file_path}")
        
        logger.info(f"Cloud sync: {successful_copies}/{len(chunk_files)} files copied")
        return successful_copies > 0
        
    except Exception as e:
        logger.exception(f"Cloud copy failed: {e}")
        if db:
            try:
                db.log_error("CloudSyncError", str(e), traceback.format_exc())
            except Exception as db_error:
                logger.warning(f"Failed to log cloud sync error to database: {db_error}")
        return False

def move_to_processed_enhanced(file_path, processed_folder, department):
    """Enhanced file moving with department organization"""
    try:
        # Create department-specific processed folder
        dept_processed = Path(processed_folder) / department
        os.makedirs(dept_processed, exist_ok=True)
        
        dest_path = dept_processed / file_path.name
        
        # Handle duplicate names with timestamp
        counter = 1
        while dest_path.exists():
            timestamp = datetime.now().strftime("%H%M%S")
            stem = file_path.stem
            suffix = file_path.suffix
            dest_path = dept_processed / f"{stem}_{timestamp}_{counter}{suffix}"
            counter += 1
        
        shutil.move(str(file_path), str(dest_path))
        logger.info(f"Moved file to processed/{department}: {dest_path.name}")
        return True
        
    except Exception as e:
        logger.error(f"Failed to move {file_path.name}: {e}")
        if db:
            try:
                db.log_error("FileMoveError", str(e), traceback.format_exc(), str(file_path))
            except Exception as db_error:
                logger.warning(f"Failed to log file move error to database: {db_error}")
        return False

def log_session_stats():
    """Log comprehensive session statistics"""
    logger.info("=== ENHANCED SESSION STATISTICS ===")
    for key, value in session_stats.items():
        if key == "department_breakdown":
            logger.info("Department Breakdown:")
            for dept, stats in value.items():
                logger.info(f"  {dept}: {stats}")
        elif key == "performance_metrics":
            logger.info("Performance Metrics:")
            for metric, val in value.items():
                logger.info(f"  {metric}: {val}")
        else:
            logger.info(f"{key}: {value}")

def main():
    """Enhanced main loop with enterprise features"""
    watch_folder = CONFIG.get("watch_folder", "C:/Users/carucci_r/Documents/chunker")
    
    # Validate configuration
    if not validate_config(CONFIG):
        logger.error("Configuration validation failed. Exiting.")