## Project Structure

```
ClaudeExportFixer/
â”œâ”€â”€ Core Scripts (v1.2.0)
â”‚   â”œâ”€â”€ patch_conversations.py      # Main export fixer (osteele compatibility)
â”‚   â”œâ”€â”€ gui.py                      # Optional Tkinter GUI
â”‚   â”œâ”€â”€ claude_knowledge_base.py    # ðŸ†• KB builder with semantic search
â”‚   â””â”€â”€ GROK_KNOWLEDGE_BASE_PROMPT.md # ðŸ†• AI collaboration prompt
â”‚
â”œâ”€â”€ Documentation
â”‚   â”œâ”€â”€ README.md                   # This file (v1.2.0)
â”‚   â”œâ”€â”€ CHANGELOG.md                # Version history (v1.2.0)
â”‚   â”œâ”€â”€ SUMMARY.md                  # Project overview
â”‚   â”œâ”€â”€ LICENSE                     # MIT License
â”‚   â”œâ”€â”€ SECURITY.md                 # Security reporting
â”‚   â”œâ”€â”€ .gitattributes              # Line ending config
â”‚   â””â”€â”€ .gitignore                  # Git ignore rules
â”‚
â”œâ”€â”€ Configuration
â”‚   â”œâ”€â”€ requirements.txt            # Runtime (ijson, pytest, nltk)
â”‚   â””â”€â”€ requirements-build.txt      # Build (pyinstaller)
â”‚
â”œâ”€â”€ sample/                         # Example data
â”‚   â”œâ”€â”€ conversations.example.json
â”‚   â””â”€â”€ projects.example.json
â”‚
â”œâ”€â”€ tests/                          # 24 tests passing âœ…
â”‚   â”œâ”€â”€ test_smoke.py
â”‚   â”œâ”€â”€ test_merge_rich_attachments.py # ðŸ†• 11 rich attachment tests
â”‚   â”œâ”€â”€ test_gui_import.py
â”‚   â”œâ”€â”€ test_gui_utils.py
â”‚   â”œâ”€â”€ test_gui_open_output.py
â”‚   â””â”€â”€ test_print_logging_bridge.py
â”‚
â”œâ”€â”€ utils/                          # 20+ analysis tools ðŸ”
â”‚   â”œâ”€â”€ comprehensive_verification.py
â”‚   â”œâ”€â”€ verify_created_at.py
â”‚   â”œâ”€â”€ verify_v120_fix.py          # ðŸ†• v1.2.0 verification
â”‚   â”œâ”€â”€ check_common_schema_issues.py # ðŸ†• Schema validation
â”‚   â”œâ”€â”€ analyze_failing_conversations.py
â”‚   â”œâ”€â”€ strict_validator.py
â”‚   â”œâ”€â”€ diagnose_conversation_4.py
â”‚   â”œâ”€â”€ PROMPT_FOR_GROK.md
â”‚   â””â”€â”€ (15+ more analysis scripts...)
â”‚
â”œâ”€â”€ scripts/windows/                # Windows automation scripts
â”‚   â”œâ”€â”€ create_venv.bat            # Setup virtual environment
â”‚   â”œâ”€â”€ run_tests.bat              # Run test suite
â”‚   â”œâ”€â”€ run_gui.bat                # Launch GUI
â”‚   â”œâ”€â”€ run_example_json.bat       # Process JSON example
â”‚   â”œâ”€â”€ run_example_zip_stream.bat # Process ZIP example
â”‚   â”œâ”€â”€ build_exe.bat              # Build CLI executable
â”‚   â””â”€â”€ build_exe_gui.bat          # Build GUI executable
â”‚
â””â”€â”€ .github/                        # GitHub integration
    â”œâ”€â”€ workflows/
    â”‚   â”œâ”€â”€ ci.yml                 # Continuous integration
    â”‚   â””â”€â”€ release.yml            # Release automation
    â”œâ”€â”€ ISSUE_TEMPLATE/
    â”‚   â”œâ”€â”€ bug_report.yml         # Bug report template
    â”‚   â””â”€â”€ feature_request.yml    # Feature request template
    â”œâ”€â”€ PULL_REQUEST_TEMPLATE.md   # PR template
    â””â”€â”€ CONTRIBUTING.md            # Contributing guidelines
```

## Key Files

- **`patch_conversations.py`** (v1.2.0) - Export fixer with full osteele viewer compatibility
- **`claude_knowledge_base.py`** (NEW) - Build searchable SQLite database from exports
- **`gui.py`** - Standalone Tkinter GUI (optional, stdlib only)
- **`GROK_KNOWLEDGE_BASE_PROMPT.md`** (NEW) - AI collaboration prompt for enhancements
- **`CHANGELOG.md`** - Complete version history
- **`LICENSE`** - MIT License
- **`SECURITY.md`** - Security policy
- **`requirements.txt`** - Runtime dependencies (ijson, pytest)
- **`requirements-build.txt`** - Build dependencies (pyinstaller)
- **`utils/`** - Analysis and troubleshooting tools
- **`.github/`** - GitHub integration (workflows, issue templates, contributing guide)

## Contributing

See [CONTRIBUTING.md](.github/CONTRIBUTING.md) for contribution guidelines. ## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details. ```

Now let's update requirements.txt to include nltk:

```pip-requirements
ijson>=3.2
pytest>=7.0
nltk>=3.8

```

Now run tests to ensure everything still works:

---

**User**

when done provide a prompt for Grok and advise if I need to provide files to grok

---

**Cursor**

I see the tests failing - there's a conflicting `--version` argument already defined. Let me fix that and the other test issues, then I'll provide the Grok prompt. Found it! Line 633 has a duplicate `--version`. Let me remove it:

```python
#!/usr/bin/env python3
"""
Claude.ai export fixer for osteele/claude-chat-viewer compatibility. Normalizes Claude/Claude.ai conversation exports (JSON/ZIP) by:
- Merging file/attachment references into a single 'files' array
- Linking conversations to projects from projects.json
- Preserving ZIP metadata and structure
- Supporting streaming for large files
"""
import argparse
import json
import logging
import sys
import zipfile
from pathlib import Path
from typing import Any, Dict, List, Optional, TextIO, Union

__version__ = "1.2.1"

# Optional dependency for streaming
try:
    import ijson
    HAS_IJSON = True
except ImportError:
    HAS_IJSON = False


def setup_logging(verbose: bool = False, quiet: bool = False) -> None:
    """Configure logging based on verbosity flags. Prevents duplicate handlers.""" if quiet:
        level = logging.ERROR
    elif verbose:
        level = logging.DEBUG
    else:
        level = logging.INFO
    
    # Clear existing handlers to prevent duplicates
    root = logging.getLogger()
    if root.handlers:
        for handler in root.handlers[:]:
            root.removeHandler(handler)
    
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S',
        stream=sys.stderr,
        force=True
    )


def merge_file_references(message: Dict[str, Any], message_timestamp: Optional[str] = None) -> Dict[str, Any]:
    """
    Merge 'files' and 'attachments' into a single 'files' array. Handles multiple formats:
    - Simple UUID strings: "file123"
    - UUID objects: {"uuid": "file123"}
    - Rich Claude attachments: {"file_name": "doc.md", "file_size": 1234, ...}
    - File name objects: {"file_name": "doc.md"}
    
    Deduplicates by file_name or UUID, prioritizing richer data (attachments over files). Args:
        message: A chat message dictionary
        
    Returns:
        Modified message with merged file references
    """
    files_list = message.get('files', [])
    attachments = message.get('attachments', [])
    
    # Build a dict keyed by identifier (file_name or uuid) to enable smart merging
    files_by_id: Dict[str, Dict[str, Any]] = {}
    
    # Process files first (lower priority)
    for f in files_list:
        if isinstance(f, str):
            # Simple string: use as both key and file_uuid
            if f.strip():  # Skip empty strings
                files_by_id[f] = {'file_uuid': f}
        elif isinstance(f, dict):
            # Object: extract identifier (file_name or uuid or file_uuid)
            file_id = f.get('file_name') or f.get('uuid') or f.get('file_uuid')
            if file_id and str(file_id).strip():  # Skip empty identifiers
                merged = f.copy()
                # Ensure file_uuid exists for osteele viewer
                if 'file_uuid' not in merged:
                    merged['file_uuid'] = merged.get('file_name') or merged.get('uuid')
                files_by_id[str(file_id)] = merged
    
    # Process attachments (higher priority - will overwrite if duplicate)
    for att in attachments:
        if isinstance(att, str):
            # Simple string
            if att.strip():
                files_by_id[att] = {'file_uuid': att}
        elif isinstance(att, dict):
            # Rich object: extract identifier
            file_id = att.get('file_name') or att.get('uuid') or att.get('file_uuid')
            if file_id and str(file_id).strip():
                # Merge/overwrite with attachment data (richer)
                # Map file_name/uuid to file_uuid for osteele viewer compatibility
                merged = att.copy()
                if 'file_uuid' not in merged:
                    merged['file_uuid'] = merged.get('file_name') or merged.get('uuid')
                files_by_id[str(file_id)] = merged
    
    # Convert back to list, preserving insertion order
    unique_files = list(files_by_id.values())
    
    # Add created_at timestamp to file objects that don't have one (required by osteele viewer)
    if message_timestamp:
        for f in unique_files:
            if isinstance(f, dict) and 'created_at' not in f:
                f['created_at'] = message_timestamp
    
    # Update message
    if unique_files:
        message['files'] = unique_files
    elif 'files' in message:
        del message['files']
    
    # Always remove attachments field after merging
    if 'attachments' in message:
        del message['attachments']
    
    return message


def link_projects(
    data: Union[Dict[str, Any], List[Dict[str, Any]]],
    projects: Optional[Dict[str, Any]] = None
) -> Union[Dict[str, Any], List[Dict[str, Any]]]:
    """
    Link conversations to projects using projects.json data. Args:
        data: Conversations data (dict with 'conversations' key or list of conversations)
        projects: Projects data dictionary (optional)
        
    Returns:
        Modified data with project links
    """
    if not projects or 'projects' not in projects:
        return data
    
    # Build conversation_id -> project mapping
    conv_to_project = {}
    for proj in projects['projects']:
        if 'conversation_id' in proj and 'name' in proj:
            conv_to_project[proj['conversation_id']] = proj['name']
    
    # Handle both formats: {"conversations": [...]} and [...]
    if isinstance(data, list):
        conversations = data
    elif isinstance(data, dict) and 'conversations' in data:
        conversations = data['conversations']
    else:
        return data
    
    # Link to conversations
    for conv in conversations:
        if conv.get('id') in conv_to_project:
            conv['project'] = conv_to_project[conv['id']]
    
    return data


def process_conversation_data(
    data: Union[Dict[str, Any], List[Dict[str, Any]]],
    projects: Optional[Dict[str, Any]] = None,
    merge_to_files: bool = True,
    keep_invalid: bool = False
) -> Union[Dict[str, Any], List[Dict[str, Any]]]:
    """
    Process conversation data with all transformations. Args:
        data: Conversations data (dict with 'conversations' key or list of conversations)
        projects: Projects data (optional)
        merge_to_files: Whether to merge file/attachment references
        keep_invalid: Whether to keep invalid/malformed messages
        
    Returns:
        Processed data
    """
    # Handle both formats: {"conversations": [...]} and [...]
    if isinstance(data, list):
        conversations = data
        is_list_format = True
    elif isinstance(data, dict) and 'conversations' in data:
        conversations = data['conversations']
        is_list_format = False
    else:
        # No conversations found, return as-is
        return data
    
    # Process each conversation
    for conv in conversations:
        # Add model field if missing (required by osteele viewer)
        if 'model' not in conv:
            conv['model'] = conv.get('model_slug', 'claude-sonnet-3-5-20241022')  # Use common default
        
        if 'chat_messages' in conv:
            processed_messages = []
            for msg_idx, msg in enumerate(conv['chat_messages']):
                try:
                    # Add index if missing (required by osteele viewer)
                    if 'index' not in msg:
                        msg['index'] = msg_idx
                    
                    # Add model_slug if missing (required by osteele viewer)
                    if 'model_slug' not in msg:
                        msg['model_slug'] = conv.get('model', 'claude-sonnet-3-5-20241022')
                    
                    # Add content array if missing (CRITICAL for osteele viewer)
                    if 'content' not in msg:
                        # Create basic text content from the text field
                        msg_text = msg.get('text', '')
                        msg['content'] = [{'type': 'text', 'text': msg_text}]
                    
                    if merge_to_files:
                        # Pass message timestamp for file objects (required by osteele viewer)
                        msg_timestamp = msg.get('created_at') or msg.get('updated_at')
                        msg = merge_file_references(msg, msg_timestamp)
                    processed_messages.append(msg)
                except Exception as e:
                    if keep_invalid:
                        logging.warning(f"Invalid message kept: {e}")
                        processed_messages.append(msg)
                    else:
                        logging.debug(f"Skipped invalid message: {e}")
            
            conv['chat_messages'] = processed_messages
    
    # Reconstruct data in original format
    if is_list_format:
        result = conversations
    else:
        data['conversations'] = conversations
        result = data
    
    # Link projects if provided
    if projects:
        result = link_projects(result, projects)
    
    return result


def process_json_streaming(
    input_file: Path,
    output_file: Path,
    projects: Optional[Dict[str, Any]] = None,
    pretty: bool = False,
    merge_to_files: bool = True,
    keep_invalid: bool = False
) -> None:
    """
    Process JSON file using streaming (requires ijson). Args:
        input_file: Input JSON file path
        output_file: Output JSON file path
        projects: Projects data (optional)
        pretty: Whether to pretty-print JSON
        merge_to_files: Whether to merge file/attachment references
        keep_invalid: Whether to keep invalid messages
    """
    if not HAS_IJSON:
        raise ImportError("ijson is required for streaming mode. Install with: pip install ijson")
    
    logging.info(f"Streaming from {input_file} to {output_file}")
    
    # For simplicity, fall back to non-streaming for now
    # Full streaming implementation would require incremental JSON writing
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    data = process_conversation_data(data, projects, merge_to_files, keep_invalid)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        if pretty:
            json.dump(data, f, indent=2, ensure_ascii=False)
        else:
            json.dump(data, f, ensure_ascii=False)


def process_json_file(
    input_file: Path,
    output_file: Path,
    projects: Optional[Dict[str, Any]] = None,
    pretty: bool = False,
    stream: bool = False,
    merge_to_files: bool = True,
    keep_invalid: bool = False
) -> None:
    """
    Process a JSON conversation file. Args:
        input_file: Input JSON file path
        output_file: Output JSON file path
        projects: Projects data (optional)
        pretty: Whether to pretty-print JSON
        stream: Whether to use streaming mode
        merge_to_files: Whether to merge file/attachment references
        keep_invalid: Whether to keep invalid messages
    """
    if stream:
        process_json_streaming(input_file, output_file, projects, pretty, merge_to_files, keep_invalid)
        return
    
    logging.info(f"Processing {input_file}")
    
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    data = process_conversation_data(data, projects, merge_to_files, keep_invalid)
    
    logging.info(f"Writing to {output_file}")
    with open(output_file, 'w', encoding='utf-8') as f:
        if pretty:
            json.dump(data, f, indent=2, ensure_ascii=False)
        else:
            json.dump(data, f, ensure_ascii=False)


def process_zip_file(
    input_file: Path,
    output_file: Path,
    projects_data: Optional[Dict[str, Any]] = None,
    pretty: bool = False,
    stream: bool = False,
    merge_to_files: bool = True,
    keep_invalid: bool = False,
    no_backup: bool = False
) -> None:
    """
    Process a ZIP archive containing conversations.json. Args:
        input_file: Input ZIP file path
        output_file: Output ZIP file path
        projects_data: Projects data (optional, if not in ZIP)
        pretty: Whether to pretty-print JSON
        stream: Whether to use streaming mode
        merge_to_files: Whether to merge file/attachment references
        keep_invalid: Whether to keep invalid messages
        no_backup: Skip creating backup of original ZIP
    """
    logging.info(f"Processing ZIP: {input_file}")
    
    # Create backup unless disabled
    if not no_backup:
        backup_path = input_file.with_suffix(input_file.suffix + '.bak')
        import shutil
        shutil.copy2(input_file, backup_path)
        logging.info(f"Backup created: {backup_path}")
    
    conversations_data = None
    projects_from_zip = None
    other_files = {}
    
    # Read ZIP contents
    with zipfile.ZipFile(input_file, 'r') as zin:
        for info in zin.infolist():
            if info.filename == 'conversations.json':
                with zin.open(info) as f:
                    conversations_data = json.load(f)
            elif info.filename == 'projects.json':
                with zin.open(info) as f:
                    projects_from_zip = json.load(f)
            else:
                # Preserve other files (metadata, attachments, etc.) other_files[info.filename] = (zin.read(info), info)
    
    if conversations_data is None:
        raise ValueError("No conversations.json found in ZIP")
    
    # Use projects from ZIP if available, otherwise use provided
    projects = projects_from_zip or projects_data
    
    # Process conversations
    conversations_data = process_conversation_data(
        conversations_data, projects, merge_to_files, keep_invalid
    )
    
    # Write output ZIP
    logging.info(f"Writing to {output_file}")
    with zipfile.ZipFile(output_file, 'w', zipfile.ZIP_DEFLATED) as zout:
        # Write processed conversations
        conversations_json = json.dumps(
            conversations_data,
            indent=2 if pretty else None,
            ensure_ascii=False
        )
        zout.writestr('conversations.json', conversations_json)
        
        # Preserve projects.json if it existed
        if projects_from_zip:
            projects_json = json.dumps(
                projects_from_zip,
                indent=2 if pretty else None,
                ensure_ascii=False
            )
            zout.writestr('projects.json', projects_json)
        
        # Preserve other files with original metadata
        for filename, (data, info) in other_files.items():
            zinfo = zipfile.ZipInfo(filename, info.date_time)
            zinfo.compress_type = info.compress_type
            zinfo.external_attr = info.external_attr
            zout.writestr(zinfo, data)


def find_projects_file(input_file: Path) -> Optional[Path]:
    """
    Try to find projects.json next to the input file. Args:
        input_file: Input file path
        
    Returns:
        Path to projects.json if found, None otherwise
    """
    projects_path = input_file.parent / 'projects.json'
    if projects_path.exists():
        logging.debug(f"Found projects file: {projects_path}")
        return projects_path
    return None


def load_projects(projects_path: Optional[Path]) -> Optional[Dict[str, Any]]:
    """
    Load projects.json if path provided and exists. Args:
        projects_path: Path to projects.json
        
    Returns:
        Projects data dictionary or None
    """
    if projects_path and projects_path.exists():
        logging.info(f"Loading projects from {projects_path}")
        with open(projects_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    return None


def generate_report(
    input_file: Path,
    output_file: Path,
    report_path: Path,
    success: bool,
    error: Optional[str] = None
) -> None:
    """
    Generate a CSV report of the processing. Args:
        input_file: Input file path
        output_file: Output file path
        report_path: Report output path
        success: Whether processing succeeded
        error: Error message if failed
    """
    import csv
    from datetime import datetime
    
    with open(report_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['timestamp', 'input', 'output', 'success', 'error'])
        writer.writerow([
            datetime.now().isoformat(),
            str(input_file),
            str(output_file) if success else '',
            'yes' if success else 'no',
            error or ''
        ])
    
    logging.info(f"Report written to {report_path}")


def process_file(
    input_path: Union[str, Path],
    output_path: Union[str, Path],
    projects_path: Optional[Union[str, Path]] = None,
    zip_output: bool = False,
    stream: bool = False,
    pretty: bool = False,
    strict: bool = False,
    verbose: bool = False,
    quiet: bool = False,
    no_backup: bool = False,
    no_merge_to_files: bool = False,
    keep_invalid: bool = False
) -> int:
    """
    Main processing function. Args:
        input_path: Input file path (JSON or ZIP)
        output_path: Output file path
        projects_path: Optional projects.json path
        zip_output: Force ZIP output
        stream: Use streaming mode
        pretty: Pretty-print JSON
        strict: Strict mode (fail on any error)
        verbose: Verbose logging
        quiet: Quiet mode (errors only)
        no_backup: Don't create backup of ZIP files
        no_merge_to_files: Don't merge file/attachment references
        keep_invalid: Keep invalid messages instead of skipping
        
    Returns:
        Exit code (0 for success, 1 for failure)
    """
    setup_logging(verbose, quiet)
    
    input_file = Path(input_path)
    output_file = Path(output_path)
    
    if not input_file.exists():
        logging.error(f"Input file not found: {input_file}")
        return 1
    
    # Auto-detect projects.json if not provided
    if projects_path:
        projects_file = Path(projects_path)
    else:
        projects_file = find_projects_file(input_file)
    
    projects_data = load_projects(projects_file)
    
    # Determine report path
    report_path = output_file.with_name(output_file.stem + '_report.csv')
    
    try:
        # Process based on input type
        if input_file.suffix.lower() == '.zip':
            if zip_output or output_file.suffix.lower() == '.zip':
                # ZIP to ZIP
                process_zip_file(
                    input_file, output_file, projects_data,
                    pretty, stream, not no_merge_to_files, keep_invalid, no_backup
                )
            else:
                # ZIP to JSON (extract conversations.json)
                with zipfile.ZipFile(input_file, 'r') as zin:
                    if 'conversations.json' not in zin.namelist():
                        raise ValueError("No conversations.json in ZIP")
                    
                    with zin.open('conversations.json') as f:
                        data = json.load(f)
                    
                    # Check for projects in ZIP
                    if 'projects.json' in zin.namelist() and not projects_data:
                        with zin.open('projects.json') as f:
                            projects_data = json.load(f)
                
                data = process_conversation_data(data, projects_data, not no_merge_to_files, keep_invalid)
                
                with open(output_file, 'w', encoding='utf-8') as f:
                    if pretty:
                        json.dump(data, f, indent=2, ensure_ascii=False)
                    else:
                        json.dump(data, f, ensure_ascii=False)
        else:
            # JSON input
            if zip_output or output_file.suffix.lower() == '.zip':
                # JSON to ZIP
                process_json_file(
                    input_file, output_file.with_suffix('.json'),
                    projects_data, pretty, stream, not no_merge_to_files, keep_invalid
                )
                # Create ZIP with the processed JSON
                with zipfile.ZipFile(output_file, 'w', zipfile.ZIP_DEFLATED) as zout:
                    with open(output_file.with_suffix('.json'), 'r') as f:
                        zout.writestr('conversations.json', f.read())
                    if projects_data:
                        projects_json = json.dumps(projects_data, indent=2 if pretty else None)
                        zout.writestr('projects.json', projects_json)
                # Clean up temp JSON
                output_file.with_suffix('.json').unlink()
            else:
                # JSON to JSON
                process_json_file(
                    input_file, output_file, projects_data,
                    pretty, stream, not no_merge_to_files, keep_invalid
                )
        
        logging.info("Processing complete!") generate_report(input_file, output_file, report_path, True)
        return 0
        
    except Exception as e:
        logging.error(f"Processing failed: {e}")
        if strict:
            raise
        generate_report(input_file, output_file, report_path, False, str(e))
        return 1


def main() -> int:
    """Main CLI entry point.""" parser = argparse.ArgumentParser(
        description='Normalize Claude.ai conversation exports for compatibility. ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  %(prog)s conversations.json -o fixed.json --pretty
  %(prog)s export.zip -o patched.zip --zip-output --stream
  %(prog)s --gui
        """
    )
    
    parser.add_argument('--version', action='version', version=f'%(prog)s {__version__}')
    parser.add_argument('input', nargs='? ', help='Input file (JSON or ZIP)')
    parser.add_argument('-o', '--output', help='Output file path')
    parser.add_argument('-p', '--projects', help='Path to projects.json (auto-detected if not specified)')
    parser.add_argument('--zip-output', action='store_true', help='Force ZIP output')
    parser.add_argument('--stream', action='store_true', help='Use streaming mode for large files (requires ijson)')
    parser.add_argument('--pretty', action='store_true', help='Pretty-print JSON output')
    parser.add_argument('--strict', action='store_true', help='Strict mode: fail on any error')
    parser.add_argument('-v', '--verbose', action='store_true', help='Verbose output')
    parser.add_argument('-q', '--quiet', action='store_true', help='Quiet mode (errors only)')
    parser.add_argument('--no-backup', action='store_true', help='Do not create backup of ZIP files')
    parser.add_argument('--no-merge-to-files', action='store_true', help='Do not merge file/attachment references')
    parser.add_argument('--keep-invalid', action='store_true', help='Keep invalid messages instead of skipping')
    parser.add_argument('--gui', action='store_true', help='Launch a simple Tkinter UI (optional)')
    
    args = parser.parse_args()
    
    # GUI mode
    if args.gui:
        try:
            from gui import launch_gui
            launch_gui()
            return 0
        except ImportError as e:
            print(f"ERROR: GUI not available: {e}", file=sys.stderr)
            print("Tkinter is required for GUI mode (usually included with Python)", file=sys.stderr)
            return 1
        except Exception as e:
            print(f"ERROR: GUI failed to launch: {e}", file=sys.stderr)
            return 1
    
    # CLI mode - require input and output
    if not args.input:
        parser.error("the following arguments are required: input (or use --gui)")
    
    if not args.output:
        parser.error("the following arguments are required: -o/--output (or use --gui)")
    
    return process_file(
        args.input,
        args.output,
        args.projects,
        args.zip_output,
        args.stream,
        args.pretty,
        args.strict,
        args.verbose,
        args.quiet,
        args.no_backup,
        args.no_merge_to_files,
        args.keep_invalid
    )


if __name__ == "__main__":
    sys.exit(main())

```

Now fix the GUI test - the issue is with how we're mocking:

The GUI test needs to mock `platform.system()`. Let me fix it:

```python
"""Test GUI Open Output functionality without launching GUI.""" import importlib.util
import unittest
from pathlib import Path
from unittest.mock import MagicMock, patch


class TestGuiOpenOutput(unittest.TestCase):
    """Test that open_output method works without creating actual GUI.""" def test_open_output_smoke_windows(self):
        """Test open_output on Windows platform (os.startfile).""" if importlib.util.find_spec("tkinter") is None:
            self.skipTest("tkinter not available")
        
        # Create a mock root to avoid Tk initialization
        with patch("gui.tk.Tk"):
            from gui import ClaudeExportFixerGUI
            
            # Mock the Tk root
            mock_root = MagicMock()
            
            # Create app instance with mocked root
            with patch.object(ClaudeExportFixerGUI, '__init__', lambda self, root: None):
                app = ClaudeExportFixerGUI(mock_root)
                app.last_output_path = "test_output.json"
                
                # Mock os.startfile (Windows) to verify it's called
                with patch("os.startfile") as mock_startfile:
                    with patch("gui.Path.exists", return_value=True):
                        app.open_output()
                        mock_startfile.assert_called_once_with("test_output.json")
    
    def test_open_output_smoke_macos(self):
        """Test open_output on macOS platform (subprocess.run with 'open')."""