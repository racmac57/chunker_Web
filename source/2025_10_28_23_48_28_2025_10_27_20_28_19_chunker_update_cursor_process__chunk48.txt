return
    
    # Check processor dependencies
    processor_deps = check_processor_dependencies()
    missing_deps = [dep for dep, available in processor_deps.items() if not available]
    if missing_deps:
        logger.warning(f"Missing file processor dependencies: {', '.join(missing_deps)}")
        logger.info("Some file types may not be processed correctly")
    
    os.makedirs(CONFIG.get("output_dir", "output"), exist_ok=True)
    os.makedirs(CONFIG.get("archive_dir", "processed"), exist_ok=True)

    logger.info("=== ENTERPRISE CHUNKER STARTED ===")
    logger.info(f"Monitoring: {watch_folder}")
    supported_extensions = CONFIG.get("supported_extensions", [".txt", ".md"])
    filter_mode = CONFIG.get("file_filter_mode", "all")
    logger.info(f"File types: {', '.join(supported_extensions)} files")
    logger.info(f"Filter mode: {filter_mode}")
    if filter_mode == "patterns":
        patterns = CONFIG.get("file_patterns", ["_full_conversation"])
        logger.info(f"Required patterns: {', '.join(patterns)}")
    elif filter_mode == "suffix":
        logger.info("Required suffix: _full_conversation")
    logger.info(f"Parallel processing: {min(4, multiprocessing.cpu_count())} workers")
    logger.info(f"Database tracking: Enabled")
    logger.info(f"Notifications: {'Enabled' if notifications.config.get('enable_notifications') else 'Disabled'}")
    logger.info(f"RAG enabled: {CONFIG.get('rag_enabled', False)}")
    
    processed_files = set()
    loop_count = 0
    last_cleanup = datetime.now()
    last_report = datetime.now()
    
    # Send startup notification
    notifications.send_email(
        notifications.config["admin_emails"],
        "ðŸš€ Chunker System Started",
        f"Enterprise Chunker system started at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
        f"Monitoring: {watch_folder}\n"
        f"File types: {', '.join(supported_extensions)} files\n"
        f"Filter mode: {filter_mode}\n"
        f"Parallel workers: {min(4, multiprocessing.cpu_count())}\n"
        f"Database: Enabled\n"
        f"Dashboard: http://localhost:5000"
    )
    
    try:
        while True:
            try:
                # Look for files with supported extensions
                all_files = []
                supported_extensions = CONFIG.get("supported_extensions", [".txt", ".md"])
                for ext in supported_extensions:
                    all_files.extend(list(Path(watch_folder).glob(f"*{ext}")))
                
                # Filter files based on configuration
                excluded_files = {"watcher_splitter.py", "test_chunker.py", "chunker_db.py", "notification_system.py"}
                
                # Apply file filtering based on mode
                filter_mode = CONFIG.get("file_filter_mode", "all")
                file_patterns = CONFIG.get("file_patterns", ["_full_conversation"])
                exclude_patterns = CONFIG.get("exclude_patterns", [])
                
                filtered_files = []
                for f in all_files:
                    if f.name in processed_files or not f.is_file() or f.name in excluded_files:
                        continue
                    
                    # Check exclude patterns first
                    if any(pattern in f.name for pattern in exclude_patterns):
                        logger.debug(f"Skipping file with exclude pattern: {f.name}")
                        continue
                    
                    # Apply filter mode
                    if filter_mode == "all":
                        filtered_files.append(f)
                    elif filter_mode == "patterns":
                        if any(pattern in f.name for pattern in file_patterns):
                            filtered_files.append(f)
                        else:
                            logger.debug(f"Skipping file without required pattern: {f.name}")
                    elif filter_mode == "suffix":
                        # Only process files with _full_conversation suffix
                        if "_full_conversation" in f.name:
                            filtered_files.append(f)
                        else:
                            logger.debug(f"Skipping file without _full_conversation suffix: {f.name}")
                
                new_files = filtered_files
                
                if new_files:
                    logger.info(f"Found {len(new_files)} new files to process")
                    
                    # Process files in parallel if multiple files
                    if len(new_files) > 1 and CONFIG.get("enable_parallel_processing", True):
                        results = process_files_parallel(new_files, CONFIG)
                        for i, result in enumerate(results):
                            if result:
                                processed_files.add(new_files[i].name)
                    else:
                        # Process files sequentially
                        for file_path in new_files:
                            try:
                                if process_file_enhanced(file_path, CONFIG):
                                    processed_files.add(file_path.name)
                                    logger.info(f"Successfully processed: {file_path.name}")
                                else:
                                    logger.error(f"Failed to process: {file_path.name}")
                            except Exception as e:
                                logger.exception(f"Error processing {file_path.name}: {e}")
                                if db:
                                    try:
                                        db.log_error("ProcessingError", str(e), traceback.format_exc(), str(file_path))
                                    except Exception as db_error:
                                        logger.warning(f"Failed to log processing error to database: {db_error}")
                
                # Periodic maintenance
                loop_count += 1
                
                # Log session stats every minute
                if loop_count % 12 == 0:  # Every minute at 5s intervals
                    log_session_stats()
                
                # Log system metrics every 5 minutes
                if loop_count % 60 == 0:
                    log_system_metrics()
                
                # Daily cleanup and reporting
                if datetime.now() - last_cleanup > timedelta(hours=24):
                    if db:
                        try:
                            db.cleanup_old_data(days=30)
                        except Exception as db_error:
                            logger.warning(f"Failed to run database cleanup: {db_error}")
                    last_cleanup = datetime.now()
                
                # Send daily report
                if datetime.now() - last_report > timedelta(hours=24):
                    if db:
                        try:
                            analytics = db.get_analytics(days=1)
                            notifications.send_daily_summary(session_stats, analytics)
                        except Exception as db_error:
                            logger.warning(f"Failed to get analytics or send daily summary: {db_error}")
                    last_report = datetime.now()
                
                time.sleep(CONFIG.get("polling_interval", 5))
                
            except KeyboardInterrupt:
                logger.info("Watcher stopped by user")
                break
            except Exception as e:
                logger.exception("Critical error in main loop")
                if db:
                    try:
                        db.log_error("MainLoopError", str(e), traceback.format_exc())
                    except Exception as db_error:
                        logger.warning(f"Failed to log main loop error to database: {db_error}")
                try:
                    notifications.send_error_alert(f"Critical main loop error: {str(e)}", stack_trace=traceback.format_exc())
                except Exception as notify_error:
                    logger.warning(f"Failed to send error alert: {notify_error}")
                time.sleep(10)
                
    finally:
        # Final statistics and cleanup
        log_session_stats()
        
        # Send shutdown notification
        notifications.send_email(
            notifications.config["admin_emails"],
            "ðŸ›‘ Chunker System Stopped",
            f"Enterprise Chunker system stopped at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
            f"Session Summary:\n"
            f"Files Processed: {session_stats['files_processed']}\n"
            f"Chunks Created: {session_stats['chunks_created']}\n"
            f"Zero-byte Prevented: {session_stats['zero_byte_prevented']}\n"
            f"Errors: {session_stats['errors']}\n"
            f"Uptime: {datetime.now() - datetime.strptime(session_stats['session_start'], '%Y-%m-%d %H:%M:%S')}"
        )

if __name__ == "__main__":
    main()
```

Now let me update the rag_integration.py to add type hints and fix the NLTK import issue:


```python
"""
RAG Integration Module for Chunker_v2
Handles retrieval-augmented generation with ChromaDB and Ollama embeddings
"""

import chromadb
from chromadb.config import Settings
from datetime import datetime
import json
import logging
from typing import List, Dict, Any, Optional
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

logger = logging.getLogger(__name__)


class ChromaRAG:
    """
    ChromaDB-based RAG implementation for knowledge base management
    Provides CRUD operations for vector database with hybrid search
    """
    
    def __init__(self, persist_directory="./chroma_db"):
        """
        Initialize ChromaDB client and collection
        
        Args:
            persist_directory: Directory to persist ChromaDB database
        """
        self.client = chromadb.PersistentClient(
            path=persist_directory,
            settings=Settings(anonymized_telemetry=False)
        )
        
        # Create or get collection for chunker knowledge base
        self.collection = self.client.get_or_create_collection(
            name="chunker_knowledge_base",
            metadata={"description": "Enterprise chunker knowledge base with RAG capabilities"}
        )
        
        logger.info(f"Initialized ChromaDB collection with {self.collection.count()} existing chunks")
    
    def add_chunk(self, chunk_text: str, metadata: Dict[str, Any]) -> str:
        """
        Add a chunk to the vector database
        
        Args:
            chunk_text: The text content of the chunk
            metadata: Dictionary containing chunk metadata
        
        Returns:
            chunk_id: Unique identifier for the added chunk
        """
        # Generate unique chunk ID
        chunk_id = f"{metadata['timestamp']}_{metadata['file_name']}_chunk{metadata['chunk_index']}"
        
        # Prepare metadata for ChromaDB (ensure values are serializable)
        chroma_metadata = {
            "file_name": metadata["file_name"],
            "file_type": metadata["file_type"],
            "chunk_index": str(metadata["chunk_index"]),
            "timestamp": metadata["timestamp"],
            "department": metadata.get("department", "admin"),
            "keywords": json.dumps(metadata.get("keywords", [])),
            "file_size": str(metadata.get("file_size", 0)),
            "processing_time": str(metadata.get("processing_time", 0))
        }
        
        try:
            # Add to collection
            self.collection.add(
                documents=[chunk_text],
                metadatas=[chroma_metadata],
                ids=[chunk_id]
            )
            
            logger.info(f"Added chunk to ChromaDB: {chunk_id}")
            return chunk_id
            
        except Exception as e:
            logger.error(f"Failed to add chunk to ChromaDB: {e}")
            raise
    
    def search_similar(self, query: str, n_results: int = 5, 
                       file_type: Optional[str] = None, 
                       department: Optional[str] = None) -> List[Dict]:
        """
        Search for similar chunks using semantic similarity
        
        Args:
            query: Search query text
            n_results: Number of results to return
            file_type: Filter by file type (optional)
            department: Filter by department (optional)
        
        Returns:
            List of formatted search results
        """
        where_clause = {}
        
        if file_type:
            where_clause["file_type"] = file_type
        if department:
            where_clause["department"] = department
        
        try:
            # Query the collection
            results = self.collection.query(
                query_texts=[query],
                n_results=n_results,
                where=where_clause if where_clause else None
            )
            
            return self._format_results(results)
            
        except Exception as e:
            logger.error(f"Failed to search ChromaDB: {e}")
            return []
    
    def search_by_keywords(self, keywords: List[str], n_results: int = 5) -> List[Dict]:
        """
        Search by specific keywords
        
        Args:
            keywords: List of keywords to search for
            n_results: Number of results to return
        
        Returns:
            List of formatted search results
        """
        # Convert keywords to query string
        query = " ".join(keywords)
        return self.search_similar(query, n_results)
    
    def get_chunk_by_id(self, chunk_id: str) -> Dict:
        """
        Retrieve specific chunk by ID
        
        Args:
            chunk_id: Unique chunk identifier
        
        Returns:
            Chunk data as dictionary
        """
        try:
            results = self.collection.get(ids=[chunk_id])
            formatted = self._format_results(results)
            return formatted[0] if formatted else None
            
        except Exception as e:
            logger.error(f"Failed to get chunk by ID: {e}")
            return {}
    
    def update_chunk(self, chunk_id: str, new_text: str, new_metadata: Dict):
        """Update existing chunk in database"""
        try:
            self.collection.update(
                ids=[chunk_id],
                documents=[new_text],
                metadatas=[new_metadata]
            )
            logger.info(f"Updated chunk: {chunk_id}")
            
        except Exception as e:
            logger.error(f"Failed to update chunk: {e}")
            raise
    
    def delete_chunk(self, chunk_id: str):
        """Delete chunk from database"""
        try:
            self.collection.delete(ids=[chunk_id])
            logger.info(f"Deleted chunk: {chunk_id}")
            
        except Exception as e:
            logger.error(f"Failed to delete chunk: {e}")
            raise
    
    def get_collection_stats(self) -> Dict:
        """
        Get statistics about the collection
        
        Returns:
            Dictionary with collection statistics
        """
        count = self.collection.count()
        return {
            "total_chunks": count,
            "collection_name": self.collection.name,
            "last_updated": datetime.now().isoformat()
        }
    
    def _format_results(self, results: Dict) -> List[Dict]:
        """
        Format ChromaDB results for easier use
        
        Args:
            results: Raw ChromaDB query results
        
        Returns:
            List of formatted result dictionaries
        """
        formatted_results = []
        
        if results.get("documents") and results["documents"][0]:
            for i, doc in enumerate(results["documents"][0]):
                formatted_results.append({
                    "id": results["ids"][0][i],
                    "document": doc,
                    "metadata": results["metadatas"][0][i],
                    "distance": results["distances"][0][i] if results.get("distances") else None
                })
        
        return formatted_results


class FaithfulnessScorer:
    """
    Advanced faithfulness scoring for RAG evaluation
    Evaluates how well generated answers are grounded in source context
    """
    
    def __init__(self):
        """Initialize faithfulness scorer with sentence transformer model"""
        self.model = SentenceTransformer('all-MiniLM-L6-v2')
    
    def extract_claims(self, answer: str) -> List[str]:
        """
        Extract factual claims from the answer
        
        Args:
            answer: Generated answer text
        
        Returns:
            List of extracted claim sentences
        """
        import nltk
        import re
        
        sentences = nltk.sent_tokenize(answer)
        claims = []
        
        for sentence in sentences:
            # Remove questions and opinion indicators
            if not re.search(r'\?|I think|I believe|in my opinion', sentence, re.IGNORECASE):
                # Extract factual statements
                if re.search(r'\b(is|are|was|were|has|have|will|can|should)\b', sentence, re.IGNORECASE):
                    claims.append(sentence.strip())
        
        return claims
    
    def calculate_faithfulness(self, answer: str, context: str, threshold: float = 0.7) -> float:
        """
        Calculate faithfulness score (0-1) based on how well answer claims
        are supported by the context
        
        Args:
            answer: Generated answer
            context: Source context
            threshold: Similarity threshold for support (default: 0.7)
        
        Returns:
            Faithfulness score between 0 and 1
        """
        claims = self.extract_claims(answer)
        if not claims:
            return 1.0  # No claims to verify
        
        # Get embeddings for claims and context
        claim_embeddings = self.model.encode(claims)
        context_embeddings = self.model.encode([context])
        
        faithfulness_scores = []
        
        for claim_emb in claim_embeddings:
            # Find best matching context sentence
            similarities = cosine_similarity([claim_emb], context_embeddings)[0]
            max_similarity = np.max(similarities)
            
            # Check if claim is supported by context
            if max_similarity >= threshold:
                faithfulness_scores.append(1.0)
            else:
                # Check for partial support
                faithfulness_scores.append(max_similarity)
        
        return float(np.mean(faithfulness_scores))
    
    def detailed_faithfulness_analysis(self, answer: str, context: str) -> Dict:
        """
        Provide detailed analysis of faithfulness
        
        Args:
            answer: Generated answer
            context: Source context
        
        Returns:
            Detailed analysis dictionary
        """
        claims = self.extract_claims(answer)
        analysis = {
            "total_claims": len(claims),
            "supported_claims": 0,
            "unsupported_claims": 0,
            "claim_details": []
        }
        
        for claim in claims:
            claim_emb = self.model.encode([claim])
            context_emb = self.model.encode([context])
            similarity = cosine_similarity(claim_emb, context_emb)[0][0]
            
            is_supported = similarity >= 0.7
            if is_supported:
                analysis["supported_claims"] += 1
            else:
                analysis["unsupported_claims"] += 1
            
            analysis["claim_details"].append({
                "claim": claim,
                "similarity": float(similarity),
                "supported": is_supported
            })
        
        analysis["faithfulness_score"] = analysis["supported_claims"] / analysis["total_claims"] if analysis["total_claims"] > 0 else 1.0
        
        return analysis


def integrate_chroma_with_watcher():
    """
    Integration example for ChromaDB with existing watcher system
    
    This function demonstrates how to integrate ChromaDB with the
    existing file processing pipeline
    """
    
    # Initialize ChromaDB
    chroma_rag = ChromaRAG()
    
    def process_file_with_chroma(file_path, config, chunks, department="admin"):
        """
        Enhanced file processing with ChromaDB integration
        
        Args:
            file_path: Path to the file being processed
            config: Configuration dictionary
            chunks: List of text chunks from file
            department: Department classification
        
        Returns:
            True if successful
        """
        timestamp = datetime.now().isoformat()
        
        # Add each chunk to ChromaDB
        for i, chunk in enumerate(chunks):
            metadata = {
                "file_name": file_path.name,
                "file_type": file_path.suffix,
                "chunk_index": i + 1,
                "timestamp": timestamp,
                "department": department,
                "keywords": extract_keywords(chunk),
                "file_size": file_path.stat().st_size,
                "processing_time": 0
            }
            
            try:
                chunk_id = chroma_rag.add_chunk(chunk, metadata)
                logger.info(f"Added chunk to ChromaDB: {chunk_id}")
            except Exception as e:
                logger.error(f"Failed to add chunk to ChromaDB: {e}")
        
        return True
    
    return process_file_with_chroma


def extract_keywords(text: str, max_keywords: int = 5) -> List[str]:
    """
    Extract keywords from text using simple frequency analysis
    
    Args:
        text: Input text
        max_keywords: Maximum number of keywords to return
    
    Returns:
        List of extracted keywords
    """
    import re
    from collections import Counter
    
    try:
        # Import NLTK stopwords
        import nltk
        from nltk.corpus import stopwords
        
        # Download stopwords if not available
        try:
            stop_words = set(stopwords.words('english'))
        except LookupError:
            nltk.download('stopwords', quiet=True)
            stop_words = set(stopwords.words('english'))
            
    except ImportError:
        # Fallback if NLTK not available
        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with'}
    
    # Remove special characters and split into words
    words = re.findall(r'\b\w+\b', text.lower())
    
    # Remove common stop words
    keywords = [word for word in words if word not in stop_words and len(word) > 2]
    
    # Return most common keywords
    counter = Counter(keywords)
    return [word for word, _ in counter.most_common(max_keywords)]


def example_usage():
    """Example usage of ChromaRAG system"""
    
    # Initialize RAG system
    rag = ChromaRAG()
    
    # Add a chunk to the knowledge base
    metadata = {
        "file_name": "excel_guide.md",
        "file_type": ".md",
        "chunk_index": 1,
        "timestamp": "2025-10-27T12:00:00Z",
        "department": "admin",
        "keywords": ["excel", "vlookup", "formulas"]
    }
    
    chunk_id = rag.add_chunk(
        "VLOOKUP is used to find values in a table. Syntax: VLOOKUP(lookup_value, table_array, col_index_num, [range_lookup])",
        metadata
    )
    print(f"Added chunk: {chunk_id}")
    
    # Search for similar content
    results = rag.search_similar("How do I use vlookup in Excel? ", n_results=3)
    print(f"Found {len(results)} similar chunks")
    
    # Get collection statistics
    stats = rag.get_collection_stats()
    print(f"Total chunks in database: {stats['total_chunks']}")


if __name__ == "__main__":
    # Run example usage
    example_usage()


```

Now let me create the testing script:


```python
"""
RAG Testing Script for Chunker_v2
Automated testing of RAG evaluation pipeline
"""

import json
import logging
import sys
from pathlib import Path
from typing import List, Dict, Any

# Import RAG components
try:
    from rag_integration import ChromaRAG, FaithfulnessScorer
    from rag_evaluation import RAGEvaluator, run_evaluation_pipeline
    RAG_AVAILABLE = True
except ImportError as e:
    print(f"RAG components not available: {e}")
    RAG_AVAILABLE = False

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def load_test_queries(test_file: str = "test_queries.json") -> List[Dict[str, Any]]:
    """
    Load test queries from JSON file
    
    Args:
        test_file: Path to test queries file
        
    Returns:
        List of test query dictionaries
    """
    try:
        with open(test_file, 'r') as f:
            queries = json.load(f)
        logger.info(f"Loaded {len(queries)} test queries from {test_file}")
        return queries
    except FileNotFoundError:
        logger.error(f"Test queries file not found: {test_file}")
        return []
    except json.JSONDecodeError as e:
        logger.error(f"Invalid JSON in test queries file: {e}")
        return []


def setup_test_rag_system() -> ChromaRAG:
    """
    Setup RAG system for testing
    
    Returns:
        ChromaRAG instance
    """
    try:
        rag = ChromaRAG(persist_directory="./test_chroma_db")
        
        # Add some test documents
        test_docs = [
            "VLOOKUP is used to find values in a table. Syntax: VLOOKUP(lookup_value, table_array, col_index_num, [range_lookup])",
            "Power BI uses M language for Power Query. You can create custom functions and data transformations. ",
            "Python files can be processed using AST module to extract functions, classes, and imports. ",
            "Chunking parameters include chunk size, overlap, and sentence limits for text processing. ",
            "Database tracking uses SQLite to store processing metrics, errors, and system performance data."