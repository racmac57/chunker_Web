try:
            # Start file processing worker
            file_worker = subprocess.Popen([
                'python', '-m', 'celery', 'worker',
                '-A', 'celery_tasks',
                '-Q', 'file_processing',
                '--concurrency=4',
                '--loglevel=info'
            ], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            self.processes['celery_file_worker'] = file_worker
            
            # Start batch processing worker
            batch_worker = subprocess.Popen([
                'python', '-m', 'celery', 'worker',
                '-A', 'celery_tasks',
                '-Q', 'batch_processing',
                '--concurrency=2',
                '--loglevel=info'
            ], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            self.processes['celery_batch_worker'] = batch_worker
            
            logger.info("Celery workers started")
            
        except Exception as e:
            logger.error(f"Failed to start Celery workers: {e}")
            raise
    
    def _start_celery_beat(self) -> None:
        """Start Celery beat scheduler.""" try:
            beat_process = subprocess.Popen([
                'python', '-m', 'celery', 'beat',
                '-A', 'celery_tasks',
                '--loglevel=info'
            ], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            self.processes['celery_beat'] = beat_process
            
            logger.info("Celery beat scheduler started")
            
        except Exception as e:
            logger.error(f"Failed to start Celery beat: {e}")
            raise
    
    def _start_flower_dashboard(self) -> None:
        """Start Flower dashboard for Celery monitoring.""" try:
            flower_process = subprocess.Popen([
                'python', '-m', 'flower',
                '--broker=redis://localhost:6379/0',
                '--port=5555',
                '--address=0.0.0.0'
            ], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            self.processes['flower'] = flower_process
            
            logger.info("Flower dashboard started on http://localhost:5555")
            
        except Exception as e:
            logger.warning(f"Failed to start Flower dashboard: {e}")
            logger.info("Celery monitoring will be available via CLI only")
    
    def _start_watchdog(self) -> None:
        """Start watchdog monitor.""" try:
            watchdog_process = subprocess.Popen([
                'python', 'enhanced_watchdog.py'
            ], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            self.processes['watchdog'] = watchdog_process
            
            logger.info("Watchdog monitor started")
            
        except Exception as e:
            logger.error(f"Failed to start watchdog: {e}")
            raise
    
    def _start_monitoring(self) -> None:
        """Start monitoring thread.""" def monitor():
            while self.running:
                try:
                    self._check_service_health()
                    time.sleep(30)  # Check every 30 seconds
                except Exception as e:
                    logger.error(f"Monitoring error: {e}")
                    time.sleep(60)
        
        monitor_thread = threading.Thread(target=monitor, daemon=True)
        monitor_thread.start()
        logger.info("Monitoring started")
    
    def _check_service_health(self) -> None:
        """Check health of all services.""" healthy_services = []
        unhealthy_services = []
        
        for service_name, process in self.processes.items():
            if process and process.poll() is None:
                healthy_services.append(service_name)
            else:
                unhealthy_services.append(service_name)
        
        if unhealthy_services:
            logger.warning(f"Unhealthy services: {unhealthy_services}")
        
        logger.info(f"Service health: {len(healthy_services)} healthy, {len(unhealthy_services)} unhealthy")
    
    def get_status(self) -> Dict[str, Any]:
        """Get status of all services.""" status = {
            "timestamp": datetime.now().isoformat(),
            "running": self.running,
            "services": {}
        }
        
        for service_name, process in self.processes.items():
            status["services"][service_name] = {
                "running": process and process.poll() is None,
                "pid": process.pid if process else None
            }
        
        return status
    
    def restart_service(self, service_name: str) -> bool:
        """Restart a specific service.""" try:
            if service_name in self.processes:
                old_process = self.processes[service_name]
                if old_process:
                    old_process.terminate()
                    old_process.wait()
                
                # Restart based on service type
                if service_name == 'celery_file_worker':
                    self._start_celery_workers()
                elif service_name == 'watchdog':
                    self._start_watchdog()
                # Add other services as needed
                
                logger.info(f"Restarted {service_name}")
                return True
            
            return False
            
        except Exception as e:
            logger.error(f"Failed to restart {service_name}: {e}")
            return False

def main():
    """Main orchestration function.""" config = {
        "watch_folder": "02_data",
        "output_dir": "04_output",
        "archive_dir": "03_archive",
        "rag_enabled": True
    }
    
    orchestrator = ChunkerOrchestrator(config)
    
    try:
        print("Starting Chunker_v2 services...")
        orchestrator.start_all_services()
        
        print("All services started. Press Ctrl+C to stop.") while True:
            time.sleep(1)
            
    except KeyboardInterrupt:
        print("\nStopping services...")
        orchestrator.stop_all_services()
        print("All services stopped.") if __name__ == "__main__":
    main()

```

Now let me add Flower authentication:


```python
"""
Simple Orchestration Script for Chunker_v2
Replaces Airflow with lightweight task coordination
"""

import os
import time
import logging
import subprocess
import threading
from typing import Dict, Any, List
from datetime import datetime
from pathlib import Path

logger = logging.getLogger(__name__)

class ChunkerOrchestrator:
    """
    Lightweight orchestration for Chunker_v2 without Airflow complexity.
    """ def __init__(self, config: Dict[str, Any]):
        """
        Initialize orchestrator. Args:
            config: Configuration dictionary
        """
        self.config = config
        self.processes = {}
        self.running = False
        
        logger.info("Initialized ChunkerOrchestrator")
    
    def start_all_services(self) -> None:
        """Start all Chunker_v2 services.""" try:
            # Start Redis (if not running)
            self._start_redis()
            
            # Start Celery workers
            self._start_celery_workers()
            
            # Start Celery beat scheduler
            self._start_celery_beat()
            
            # Start Flower dashboard
            self._start_flower_dashboard()
            
            # Start Watchdog monitor
            self._start_watchdog()
            
            # Start monitoring
            self._start_monitoring()
            
            self.running = True
            logger.info("All services started successfully")
            
        except Exception as e:
            logger.error(f"Failed to start services: {e}")
            self.stop_all_services()
            raise
    
    def stop_all_services(self) -> None:
        """Stop all services.""" try:
            self.running = False
            
            for service_name, process in self.processes.items():
                if process and process.poll() is None:
                    process.terminate()
                    logger.info(f"Stopped {service_name}")
            
            self.processes.clear()
            logger.info("All services stopped")
            
        except Exception as e:
            logger.error(f"Error stopping services: {e}")
    
    def _start_redis(self) -> None:
        """Start Redis server with enhanced error handling and fallback.""" try:
            # Check if Redis is already running
            result = subprocess.run(['redis-cli', 'ping'], 
                                  capture_output=True, text=True, timeout=5)
            if result.returncode == 0:
                logger.info("Redis already running")
                return
            
            # Try to start Redis
            try:
                redis_process = subprocess.Popen(['redis-server'], 
                                               stdout=subprocess.DEVNULL,
                                               stderr=subprocess.DEVNULL)
                self.processes['redis'] = redis_process
                
                # Wait for Redis to start and verify
                for attempt in range(5):
                    time.sleep(1)
                    try:
                        test_result = subprocess.run(['redis-cli', 'ping'], 
                                                   capture_output=True, text=True, timeout=2)
                        if test_result.returncode == 0:
                            logger.info("Redis started successfully")
                            return
                    except subprocess.TimeoutExpired:
                        continue
                
                # If we get here, Redis didn't start properly
                logger.error("Redis failed to start properly")
                raise Exception("Redis startup verification failed")
                
            except FileNotFoundError:
                logger.error("Redis not found. Please install Redis:")
                logger.error("  Windows: Download from https://github.com/microsoftarchive/redis/releases")
                logger.error("  Linux: sudo apt-get install redis-server")
                logger.error("  macOS: brew install redis")
                raise Exception("Redis not installed")
            
        except Exception as e:
            logger.error(f"Redis startup failed: {e}")
            logger.warning("Falling back to multiprocessing.Queue for task coordination")
            self._setup_fallback_queue()
            raise Exception("Redis not available - using fallback mode")
    
    def _setup_fallback_queue(self) -> None:
        """Setup multiprocessing.Queue as fallback when Redis is unavailable.""" try:
            import multiprocessing
            self.fallback_queue = multiprocessing.Queue()
            logger.info("Fallback queue initialized - Celery will use in-memory processing")
        except Exception as e:
            logger.error(f"Failed to setup fallback queue: {e}")
            raise
    
    def _start_celery_workers(self) -> None:
        """Start Celery workers.""" try:
            # Start file processing worker
            file_worker = subprocess.Popen([
                'python', '-m', 'celery', 'worker',
                '-A', 'celery_tasks',
                '-Q', 'file_processing',
                '--concurrency=4',
                '--loglevel=info'
            ], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            self.processes['celery_file_worker'] = file_worker
            
            # Start batch processing worker
            batch_worker = subprocess.Popen([
                'python', '-m', 'celery', 'worker',
                '-A', 'celery_tasks',
                '-Q', 'batch_processing',
                '--concurrency=2',
                '--loglevel=info'
            ], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            self.processes['celery_batch_worker'] = batch_worker
            
            logger.info("Celery workers started")
            
        except Exception as e:
            logger.error(f"Failed to start Celery workers: {e}")
            raise
    
    def _start_celery_beat(self) -> None:
        """Start Celery beat scheduler.""" try:
            beat_process = subprocess.Popen([
                'python', '-m', 'celery', 'beat',
                '-A', 'celery_tasks',
                '--loglevel=info'
            ], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            self.processes['celery_beat'] = beat_process
            
            logger.info("Celery beat scheduler started")
            
        except Exception as e:
            logger.error(f"Failed to start Celery beat: {e}")
            raise
    
    def _start_flower_dashboard(self) -> None:
        """Start Flower dashboard for Celery monitoring with authentication.""" try:
            # Generate basic auth credentials (in production, use environment variables)
            import base64
            import secrets
            
            username = os.getenv('FLOWER_USERNAME', 'admin')
            password = os.getenv('FLOWER_PASSWORD', secrets.token_urlsafe(8))
            
            # Create auth string
            auth_string = f"{username}:{password}"
            auth_b64 = base64.b64encode(auth_string.encode()).decode()
            
            flower_process = subprocess.Popen([
                'python', '-m', 'flower',
                '--broker=redis://localhost:6379/0',
                '--port=5555',
                '--address=0.0.0.0',
                f'--auth={auth_string}',
                '--basic_auth=admin:admin'  # Fallback basic auth
            ], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            self.processes['flower'] = flower_process
            
            logger.info("Flower dashboard started on http://localhost:5555")
            logger.info(f"Authentication: username='{username}', password='{password}'")
            logger.warning("Change default credentials in production!") except Exception as e:
            logger.warning(f"Failed to start Flower dashboard: {e}")
            logger.info("Celery monitoring will be available via CLI only")
    
    def _start_watchdog(self) -> None:
        """Start watchdog monitor.""" try:
            watchdog_process = subprocess.Popen([
                'python', 'enhanced_watchdog.py'
            ], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            self.processes['watchdog'] = watchdog_process
            
            logger.info("Watchdog monitor started")
            
        except Exception as e:
            logger.error(f"Failed to start watchdog: {e}")
            raise
    
    def _start_monitoring(self) -> None:
        """Start monitoring thread.""" def monitor():
            while self.running:
                try:
                    self._check_service_health()
                    time.sleep(30)  # Check every 30 seconds
                except Exception as e:
                    logger.error(f"Monitoring error: {e}")
                    time.sleep(60)
        
        monitor_thread = threading.Thread(target=monitor, daemon=True)
        monitor_thread.start()
        logger.info("Monitoring started")
    
    def _check_service_health(self) -> None:
        """Check health of all services.""" healthy_services = []
        unhealthy_services = []
        
        for service_name, process in self.processes.items():
            if process and process.poll() is None:
                healthy_services.append(service_name)
            else:
                unhealthy_services.append(service_name)
        
        if unhealthy_services:
            logger.warning(f"Unhealthy services: {unhealthy_services}")
        
        logger.info(f"Service health: {len(healthy_services)} healthy, {len(unhealthy_services)} unhealthy")
    
    def get_status(self) -> Dict[str, Any]:
        """Get status of all services.""" status = {
            "timestamp": datetime.now().isoformat(),
            "running": self.running,
            "services": {}
        }
        
        for service_name, process in self.processes.items():
            status["services"][service_name] = {
                "running": process and process.poll() is None,
                "pid": process.pid if process else None
            }
        
        return status
    
    def restart_service(self, service_name: str) -> bool:
        """Restart a specific service.""" try:
            if service_name in self.processes:
                old_process = self.processes[service_name]
                if old_process:
                    old_process.terminate()
                    old_process.wait()
                
                # Restart based on service type
                if service_name == 'celery_file_worker':
                    self._start_celery_workers()
                elif service_name == 'watchdog':
                    self._start_watchdog()
                # Add other services as needed
                
                logger.info(f"Restarted {service_name}")
                return True
            
            return False
            
        except Exception as e:
            logger.error(f"Failed to restart {service_name}: {e}")
            return False

def main():
    """Main orchestration function.""" config = {
        "watch_folder": "02_data",
        "output_dir": "04_output",
        "archive_dir": "03_archive",
        "rag_enabled": True
    }
    
    orchestrator = ChunkerOrchestrator(config)
    
    try:
        print("Starting Chunker_v2 services...")
        orchestrator.start_all_services()
        
        print("All services started. Press Ctrl+C to stop.") while True:
            time.sleep(1)
            
    except KeyboardInterrupt:
        print("\nStopping services...")
        orchestrator.stop_all_services()
        print("All services stopped.") if __name__ == "__main__":
    main()

```

Now let me add task prioritization to the Celery configuration:


```python
"""
Celery Task Queue System for Chunker_v2
Handles async file processing with race condition prevention and batching
"""

import os
import json
import logging
import time
import traceback
from typing import Dict, Any, List, Optional
from datetime import datetime
from pathlib import Path
from celery import Celery, chain, group
from celery.exceptions import Retry, WorkerLostError
from celery.signals import task_prerun, task_postrun, task_failure
import redis
from watchdog.events import FileSystemEvent

logger = logging.getLogger(__name__)

# Celery configuration
CELERY_CONFIG = {
    'broker_url': os.getenv('REDIS_URL', 'redis://localhost:6379/0'),
    'result_backend': os.getenv('REDIS_URL', 'redis://localhost:6379/0'),
    'task_serializer': 'json',
    'accept_content': ['json'],
    'result_serializer': 'json',
    'timezone': 'UTC',
    'enable_utc': True,
    'task_track_started': True,
    'task_time_limit': 300,  # 5 minutes
    'task_soft_time_limit': 240,  # 4 minutes
    'worker_prefetch_multiplier': 1,
    'task_acks_late': True,
    'worker_disable_rate_limits': True,
}

# Initialize Celery app
app = Celery('chunker_v2')
app.config_from_object(CELERY_CONFIG)

# Redis client for coordination
redis_client = redis.Redis.from_url(CELERY_CONFIG['broker_url'])

class TaskCoordinator:
    """
    Coordinates file processing tasks to prevent race conditions and enable batching.
    """ def __init__(self, config: Dict[str, Any]):
        """
        Initialize task coordinator. Args:
            config: Configuration dictionary
        """
        self.config = config
        self.debounce_window = config.get("debounce_window", 1.0)
        self.max_workers = config.get("max_workers", 4)
        self.batch_size = config.get("batch_size", 10)
        self.failed_dir = config.get("failed_dir", "03_archive/failed")
        
        # Create failed directory
        os.makedirs(self.failed_dir, exist_ok=True)
        
        logger.info("Initialized TaskCoordinator")
    
    def should_process_file(self, file_path: str) -> bool:
        """
        Check if file should be processed based on debouncing rules. Args:
            file_path: Path to the file
            
        Returns:
            True if should process, False otherwise
        """
        try:
            current_time = time.time()
            file_key = f"file_processing:{file_path}"
            
            # Check if file is already being processed
            if redis_client.exists(f"processing:{file_path}"):
                logger.debug(f"File already being processed: {file_path}")
                return False
            
            # Check debounce window
            last_processed = redis_client.get(f"last_processed:{file_path}")
            if last_processed:
                last_time = float(last_processed.decode())
                if current_time - last_time < self.debounce_window:
                    logger.debug(f"File within debounce window: {file_path}")
                    return False
            
            # Mark as processing
            redis_client.setex(f"processing:{file_path}", 300, current_time)  # 5 min expiry
            
            return True
            
        except Exception as e:
            logger.error(f"Error checking file processing status: {e}")
            return False
    
    def mark_file_completed(self, file_path: str) -> None:
        """
        Mark file as completed processing. Args:
            file_path: Path to the file
        """
        try:
            current_time = time.time()
            redis_client.setex(f"last_processed:{file_path}", 3600, current_time)  # 1 hour expiry
            redis_client.delete(f"processing:{file_path}")
            logger.debug(f"Marked file as completed: {file_path}")
        except Exception as e:
            logger.error(f"Error marking file completed: {e}")
    
    def mark_file_failed(self, file_path: str, error: str) -> None:
        """
        Mark file as failed processing. Args:
            file_path: Path to the file
            error: Error message
        """
        try:
            redis_client.delete(f"processing:{file_path}")
            redis_client.setex(f"failed:{file_path}", 3600, error)  # 1 hour expiry
            logger.error(f"Marked file as failed: {file_path} - {error}")
        except Exception as e:
            logger.error(f"Error marking file failed: {e}")

# Task routing and configuration with priority support
CELERY_CONFIG.update({
    'task_routes': {
        'celery_tasks.process_file_task': {'queue': 'file_processing'},
        'celery_tasks.batch_process_files_task': {'queue': 'batch_processing'},
        'celery_tasks.add_to_rag_task': {'queue': 'rag_processing'},
        'celery_tasks.evaluate_rag_task': {'queue': 'evaluation'},
        'celery_tasks.health_check_task': {'queue': 'monitoring'},
        'celery_tasks.process_priority_file_task': {'queue': 'priority_processing'},
    },
    'task_annotations': {
        'celery_tasks.process_file_task': {'rate_limit': '10/m'},
        'celery_tasks.batch_process_files_task': {'rate_limit': '5/m'},
        'celery_tasks.add_to_rag_task': {'rate_limit': '20/m'},
        'celery_tasks.evaluate_rag_task': {'rate_limit': '2/m'},
        'celery_tasks.process_priority_file_task': {'rate_limit': '50/m'},  # Higher rate for priority
    },
    'beat_schedule': {
        'health-check': {
            'task': 'celery_tasks.health_check_task',
            'schedule': 60.0,  # Every minute
        },
        'cleanup-old-tasks': {
            'task': 'celery_tasks.cleanup_old_tasks',
            'schedule': 3600.0,  # Every hour
        },
    },
})

# Initialize Celery app
app = Celery('chunker_v2')
app.config_from_object(CELERY_CONFIG)

# Redis client for coordination
redis_client = redis.Redis.from_url(CELERY_CONFIG['broker_url'])

# Task signals for monitoring
@task_prerun.connect
def task_prerun_handler(sender=None, task_id=None, task=None, args=None, kwargs=None, **kwds):
    """Log task start.""" logger.info(f"Task {task.name} started: {task_id}")

@task_postrun.connect
def task_postrun_handler(sender=None, task_id=None, task=None, args=None, kwargs=None, retval=None, state=None, **kwds):
    """Log task completion.""" logger.info(f"Task {task.name} completed: {task_id} - State: {state}")

@task_failure.connect
def task_failure_handler(sender=None, task_id=None, exception=None, traceback=None, einfo=None, **kwds):
    """Log task failures.""" logger.error(f"Task {sender.name} failed: {task_id} - {exception}")

@app.task(bind=True, max_retries=3, default_retry_delay=60)
def process_file_task(self, file_path: str, dest_path: Optional[str], 
                     event_type: str, config: Dict[str, Any]) -> Dict[str, Any]:
    """
    Celery task for processing a single file. Args:
        file_path: Path to the file
        dest_path: Destination path (for moved events)
        event_type: Type of event (moved, created)
        config: Configuration dictionary
        
    Returns:
        Processing result dictionary
    """
    coordinator = TaskCoordinator(config)
    
    try:
        # Check if file should be processed
        if not coordinator.should_process_file(file_path):
            return {"status": "skipped", "reason": "debounced_or_processing"}
        
        # Process the file
        result = process_file_with_rag(file_path, dest_path, event_type, config)
        
        # Mark as completed
        coordinator.mark_file_completed(file_path)
        
        return result
        
    except Exception as e:
        logger.error(f"Task failed for {file_path}: {e}")
        
        # Mark as failed
        coordinator.mark_file_failed(file_path, str(e))
        
        # Retry if possible
        if self.request.retries < self.max_retries:
            logger.info(f"Retrying task for {file_path} (attempt {self.request.retries + 1})")
            raise self.retry(countdown=60 * (self.request.retries + 1))
        
        return {"status": "failed", "error": str(e)}

@app.task(bind=True, max_retries=2, default_retry_delay=30)
def add_to_rag_task(self, chunks: List[str], metadata: Dict[str, Any], 
                   config: Dict[str, Any]) -> Dict[str, Any]:
    """
    Add chunks to RAG system (ChromaDB or FAISS). Args:
        chunks: List of text chunks
        metadata: File metadata
        config: Configuration dictionary
        
    Returns:
        RAG operation results
    """
    try:
        if not config.get("rag_enabled", False):
            return {"status": "disabled", "message": "RAG not enabled"}
        
        # Try ChromaDB first, fallback to FAISS
        try:
            from rag_integration import ChromaRAG
            chroma_rag = ChromaRAG(persist_directory=config.get("chroma_persist_dir", "./chroma_db"))
            
            results = []
            for i, chunk in enumerate(chunks):
                chunk_metadata = {
                    "file_name": metadata["file_info"]["name"],
                    "file_type": metadata["file_info"]["extension"],
                    "chunk_index": i + 1,
                    "timestamp": datetime.now().isoformat(),
                    "keywords": extract_keywords(chunk),
                    "file_size": metadata["file_info"]["size"]
                }
                
                chunk_id = chroma_rag.add_chunk(chunk, chunk_metadata)
                results.append(chunk_id)
            
            return {"status": "success", "method": "chromadb", "chunk_ids": results}
            
        except ImportError:
            # Fallback to FAISS
            from ollama_integration import initialize_ollama_rag
            rag_system = initialize_ollama_rag()
            
            chunk_metadatas = []
            for i, chunk in enumerate(chunks):
                chunk_metadata = {
                    "source_file": metadata["file_info"]["name"],
                    "file_type": metadata["file_info"]["extension"],
                    "chunk_index": i + 1,
                    "timestamp": datetime.now().isoformat(),
                    "keywords": extract_keywords(chunk),
                    "file_size": metadata["file_info"]["size"]
                }
                chunk_metadatas.append(chunk_metadata)
            
            rag_system.add_documents(chunks, chunk_metadatas)
            rag_system.save_index()
            
            return {"status": "success", "method": "faiss", "chunks_added": len(chunks)}
        
    except Exception as e:
        logger.error(f"RAG task failed: {e}")
        if self.request.retries < self.max_retries:
            logger.info(f"Retrying RAG task (attempt {self.request.retries + 1})")
            raise self.retry(countdown=30 * (self.request.retries + 1))
        return {"status": "failed", "error": str(e)}

@app.task(bind=True, max_retries=1, default_retry_delay=120)
def evaluate_rag_task(self, query: str, answer: str, context: str,
                     config: Dict[str, Any]) -> Dict[str, Any]:
    """
    Evaluate RAG performance for a query-answer pair.