]
    
    test_metadata = [
        {"source": "excel_guide.md", "topic": "excel"},
        {"source": "powerbi_guide.md", "topic": "powerbi"},
        {"source": "python_guide.md", "topic": "python"}
    ]
    
    rag.add_documents(test_docs, test_metadata)
    
    # Search
    results = rag.hybrid_search("How do I use vlookup? ", top_k=3)
    print(f"Found {len(results)} results:")
    for result in results:
        print(f"- {result['content'][:50]}... (score: {result['score']:.3f})")
    
    # Save index
    rag.save_index()
    
    # Get stats
    stats = rag.get_stats()
    print(f"Vector store stats: {stats}")
```

Now let me create the embedding helpers:


```python
"""
Embedding Helpers for Chunker_v2
Utilities for generating, storing, and managing embeddings
"""

import os
import json
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime
from pathlib import Path
import numpy as np
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document

logger = logging.getLogger(__name__)

class EmbeddingManager:
    """
    Manages embedding generation, storage, and retrieval for chunks.
    """ def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):
        """
        Initialize embedding manager. Args:
            chunk_size: Size of text chunks
            chunk_overlap: Overlap between chunks
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            separators=["\n\n", "\n", " ", ""]
        )
    
    def generate_chunk_metadata(self, chunk: str, filename: str, file_type: str, 
                               timestamp: str, index: int, keywords: List[str] = None) -> Dict[str, Any]:
        """
        Generate comprehensive metadata for a chunk. Args:
            chunk: Text content of the chunk
            filename: Source filename
            file_type: File extension
            timestamp: Processing timestamp
            index: Chunk index within file
            keywords: Extracted keywords
            
        Returns:
            Metadata dictionary
        """
        return {
            "source_file": filename,
            "file_type": file_type,
            "chunk_index": index,
            "timestamp": timestamp,
            "keywords": keywords or [],
            "chunk_size": len(chunk),
            "word_count": len(chunk.split()),
            "sentence_count": len([s for s in chunk.split('.') if s.strip()]),
            "processing_date": datetime.now().isoformat()
        }
    
    def split_text(self, text: str) -> List[str]:
        """
        Split text into chunks using the configured splitter. Args:
            text: Input text to split
            
        Returns:
            List of text chunks
        """
        try:
            chunks = self.splitter.split_text(text)
            logger.info(f"Split text into {len(chunks)} chunks")
            return chunks
        except Exception as e:
            logger.error(f"Failed to split text: {e}")
            return [text]  # Fallback to single chunk
    
    def create_documents(self, texts: List[str], metadatas: List[Dict[str, Any]]) -> List[Document]:
        """
        Create LangChain Document objects from texts and metadata. Args:
            texts: List of text chunks
            metadatas: List of metadata dictionaries
            
        Returns:
            List of Document objects
        """
        try:
            documents = []
            for text, metadata in zip(texts, metadatas):
                doc = Document(page_content=text, metadata=metadata)
                documents.append(doc)
            
            logger.info(f"Created {len(documents)} Document objects")
            return documents
        except Exception as e:
            logger.error(f"Failed to create documents: {e}")
            return []
    
    def extract_file_metadata(self, file_path: Path) -> Dict[str, Any]:
        """
        Extract metadata from a file. Args:
            file_path: Path to the file
            
        Returns:
            File metadata dictionary
        """
        try:
            stat = file_path.stat()
            return {
                "file_name": file_path.name,
                "file_path": str(file_path),
                "file_size": stat.st_size,
                "created_time": datetime.fromtimestamp(stat.st_ctime).isoformat(),
                "modified_time": datetime.fromtimestamp(stat.st_mtime).isoformat(),
                "file_extension": file_path.suffix.lower(),
                "is_binary": self._is_binary_file(file_path)
            }
        except Exception as e:
            logger.error(f"Failed to extract file metadata: {e}")
            return {}
    
    def _is_binary_file(self, file_path: Path) -> bool:
        """
        Check if a file is binary. Args:
            file_path: Path to the file
            
        Returns:
            True if binary, False otherwise
        """
        try:
            with open(file_path, 'rb') as f:
                chunk = f.read(1024)
                return b'\0' in chunk
        except Exception:
            return False
    
    def save_metadata(self, metadata: Dict[str, Any], output_path: str) -> None:
        """
        Save metadata to JSON file. Args:
            metadata: Metadata dictionary
            output_path: Output file path
        """
        try:
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)
            logger.info(f"Saved metadata to {output_path}")
        except Exception as e:
            logger.error(f"Failed to save metadata: {e}")
    
    def load_metadata(self, input_path: str) -> Optional[Dict[str, Any]]:
        """
        Load metadata from JSON file. Args:
            input_path: Input file path
            
        Returns:
            Metadata dictionary or None if failed
        """
        try:
            with open(input_path, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            logger.info(f"Loaded metadata from {input_path}")
            return metadata
        except Exception as e:
            logger.error(f"Failed to load metadata: {e}")
            return None

def generate_chunk_metadata(chunk: str, filename: str, file_type: str, 
                           timestamp: str, index: int, keywords: List[str] = None) -> Dict[str, Any]:
    """
    Generate metadata for a chunk (convenience function). Args:
        chunk: Text content of the chunk
        filename: Source filename
        file_type: File extension
        timestamp: Processing timestamp
        index: Chunk index within file
        keywords: Extracted keywords
        
    Returns:
        Metadata dictionary
    """
    manager = EmbeddingManager()
    return manager.generate_chunk_metadata(chunk, filename, file_type, timestamp, index, keywords)

def save_embeddings(vectorstore, path: str = "faiss_index") -> None:
    """
    Save FAISS index to disk. Args:
        vectorstore: FAISS vector store
        path: Directory path to save to
    """
    try:
        os.makedirs(path, exist_ok=True)
        vectorstore.save_local(path)
        logger.info(f"Saved FAISS index to {path}")
    except Exception as e:
        logger.error(f"Failed to save FAISS index: {e}")
        raise

def load_embeddings(embeddings, path: str = "faiss_index"):
    """
    Load FAISS index from disk. Args:
        embeddings: Embeddings object
        path: Directory path to load from
        
    Returns:
        Loaded FAISS vector store
    """
    try:
        if os.path.exists(path):
            from langchain_community.vectorstores import FAISS
            vectorstore = FAISS.load_local(path, embeddings, allow_dangerous_deserialization=True)
            logger.info(f"Loaded FAISS index from {path}")
            return vectorstore
        else:
            logger.warning(f"No FAISS index found at {path}")
            return None
    except Exception as e:
        logger.error(f"Failed to load FAISS index: {e}")
        return None

def create_embedding_index(texts: List[str], metadatas: List[Dict[str, Any]], 
                          embeddings, output_path: str = "faiss_index") -> None:
    """
    Create and save FAISS index from texts and metadata. Args:
        texts: List of text chunks
        metadatas: List of metadata dictionaries
        embeddings: Embeddings object
        output_path: Directory path to save to
    """
    try:
        from langchain_community.vectorstores import FAISS
        
        # Create FAISS index
        vectorstore = FAISS.from_texts(texts=texts, embedding=embeddings, metadatas=metadatas)
        
        # Save to disk
        save_embeddings(vectorstore, output_path)
        
        logger.info(f"Created FAISS index with {len(texts)} documents")
        
    except Exception as e:
        logger.error(f"Failed to create embedding index: {e}")
        raise

def batch_process_files(file_paths: List[Path], embedding_manager: EmbeddingManager, 
                       extract_keywords_func) -> List[Dict[str, Any]]:
    """
    Batch process multiple files for embedding. Args:
        file_paths: List of file paths to process
        embedding_manager: EmbeddingManager instance
        extract_keywords_func: Function to extract keywords from text
        
    Returns:
        List of processed file results
    """
    results = []
    
    for file_path in file_paths:
        try:
            # Extract file metadata
            file_metadata = embedding_manager.extract_file_metadata(file_path)
            
            # Read file content
            if file_metadata.get("is_binary", False):
                logger.warning(f"Skipping binary file: {file_path}")
                continue
            
            with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
                content = f.read()
            
            # Split into chunks
            chunks = embedding_manager.split_text(content)
            
            # Generate metadata for each chunk
            timestamp = datetime.now().isoformat()
            chunk_metadatas = []
            
            for i, chunk in enumerate(chunks):
                keywords = extract_keywords_func(chunk) if extract_keywords_func else []
                metadata = embedding_manager.generate_chunk_metadata(
                    chunk, file_path.name, file_path.suffix, timestamp, i, keywords
                )
                metadata.update(file_metadata)  # Add file-level metadata
                chunk_metadatas.append(metadata)
            
            results.append({
                "file_path": str(file_path),
                "chunks": chunks,
                "metadatas": chunk_metadatas,
                "file_metadata": file_metadata,
                "success": True
            })
            
            logger.info(f"Processed {file_path.name}: {len(chunks)} chunks")
            
        except Exception as e:
            logger.error(f"Failed to process {file_path}: {e}")
            results.append({
                "file_path": str(file_path),
                "chunks": [],
                "metadatas": [],
                "file_metadata": {},
                "success": False,
                "error": str(e)
            })
    
    return results

# Example usage
if __name__ == "__main__":
    # Initialize embedding manager
    manager = EmbeddingManager(chunk_size=500, chunk_overlap=50)
    
    # Example text
    sample_text = """
    This is a sample document for testing the embedding manager. It contains multiple sentences and paragraphs to demonstrate
    how text splitting and metadata generation works. The system should handle various file types and extract
    meaningful metadata for each chunk.
    """ # Split text
    chunks = manager.split_text(sample_text)
    print(f"Split into {len(chunks)} chunks")
    
    # Generate metadata
    timestamp = datetime.now().isoformat()
    metadatas = []
    
    for i, chunk in enumerate(chunks):
        metadata = manager.generate_chunk_metadata(
            chunk, "sample.txt", ".txt", timestamp, i, ["sample", "test"]
        )
        metadatas.append(metadata)
    
    # Create documents
    documents = manager.create_documents(chunks, metadatas)
    print(f"Created {len(documents)} documents")
    
    # Save metadata
    manager.save_metadata({"chunks": metadatas}, "sample_metadata.json")
    
    print("Embedding manager test completed successfully!") ```

Now let me create the comprehensive RAG evaluation framework:


```python
"""
Comprehensive RAG Evaluation Framework for Chunker_v2
Implements retrieval metrics, generation metrics, and end-to-end evaluation
"""

import logging
import json
import time
from typing import List, Dict, Any, Optional, Tuple
import numpy as np
from rouge_score import rouge_scorer
from nltk.translate.bleu_score import sentence_bleu
from bert_score import score as bertscore
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer

logger = logging.getLogger(__name__)

class RAGEvaluator:
    """
    Comprehensive RAG evaluation system with multiple metrics.
    """ def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        """
        Initialize RAG evaluator. Args:
            model_name: SentenceTransformer model for semantic similarity
        """
        self.model_name = model_name
        self.sentence_model = SentenceTransformer(model_name)
        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
        
    def precision_at_k(self, retrieved_docs: List[str], relevant_docs: List[str], k: int = 5) -> float:
        """
        Calculate Precision@K.
        
        Args:
            retrieved_docs: List of retrieved document IDs
            relevant_docs: List of relevant document IDs
            k: Number of top results to consider
            
        Returns:
            Precision@K score
        """
        if not retrieved_docs or k == 0:
            return 0.0
        
        top_k = retrieved_docs[:k]
        relevant_count = sum(1 for doc in top_k if doc in relevant_docs)
        return relevant_count / k
    
    def recall_at_k(self, retrieved_docs: List[str], relevant_docs: List[str], k: int = 5) -> float:
        """
        Calculate Recall@K.
        
        Args:
            retrieved_docs: List of retrieved document IDs
            relevant_docs: List of relevant document IDs
            k: Number of top results to consider
            
        Returns:
            Recall@K score
        """
        if not relevant_docs:
            return 0.0
        
        top_k = retrieved_docs[:k]
        relevant_count = sum(1 for doc in top_k if doc in relevant_docs)
        return relevant_count / len(relevant_docs)
    
    def mean_reciprocal_rank(self, retrieved_docs: List[str], relevant_docs: List[str]) -> float:
        """
        Calculate Mean Reciprocal Rank (MRR). Args:
            retrieved_docs: List of retrieved document IDs
            relevant_docs: List of relevant document IDs
            
        Returns:
            MRR score
        """
        if not relevant_docs:
            return 0.0
        
        for i, doc in enumerate(retrieved_docs):
            if doc in relevant_docs:
                return 1.0 / (i + 1)
        return 0.0
    
    def normalized_dcg_at_k(self, retrieved_docs: List[str], relevant_docs: List[str], k: int = 5) -> float:
        """
        Calculate Normalized Discounted Cumulative Gain@K.
        
        Args:
            retrieved_docs: List of retrieved document IDs
            relevant_docs: List of relevant document IDs
            k: Number of top results to consider
            
        Returns:
            NDCG@K score
        """
        def dcg_at_k(relevance_scores: List[float], k: int) -> float:
            relevance_scores = relevance_scores[:k]
            return sum(score / np.log2(i + 2) for i, score in enumerate(relevance_scores))
        
        # Calculate relevance scores
        relevance_scores = [1.0 if doc in relevant_docs else 0.0 for doc in retrieved_docs]
        
        # Calculate DCG
        dcg = dcg_at_k(relevance_scores, k)
        
        # Calculate IDCG (ideal DCG)
        ideal_relevance = [1.0] * min(len(relevant_docs), k)
        idcg = dcg_at_k(ideal_relevance, k)
        
        return dcg / idcg if idcg > 0 else 0.0
    
    def evaluate_retrieval(self, retrieved_docs: List[str], relevant_docs: List[str], k_values: List[int] = [1, 3, 5, 10]) -> Dict[str, float]:
        """
        Evaluate retrieval performance with multiple metrics. Args:
            retrieved_docs: List of retrieved document IDs
            relevant_docs: List of relevant document IDs
            k_values: List of k values to evaluate
            
        Returns:
            Dictionary of retrieval metrics
        """
        metrics = {}
        
        for k in k_values:
            metrics[f"precision_at_{k}"] = self.precision_at_k(retrieved_docs, relevant_docs, k)
            metrics[f"recall_at_{k}"] = self.recall_at_k(retrieved_docs, relevant_docs, k)
            metrics[f"ndcg_at_{k}"] = self.normalized_dcg_at_k(retrieved_docs, relevant_docs, k)
        
        metrics["mrr"] = self.mean_reciprocal_rank(retrieved_docs, relevant_docs)
        
        return metrics
    
    def evaluate_generation(self, reference: str, generated: str) -> Dict[str, float]:
        """
        Evaluate generation quality with multiple metrics. Args:
            reference: Reference answer
            generated: Generated answer
            
        Returns:
            Dictionary of generation metrics
        """
        metrics = {}
        
        # ROUGE scores
        rouge_scores = self.rouge_scorer.score(reference, generated)
        metrics["rouge1"] = rouge_scores["rouge1"].fmeasure
        metrics["rouge2"] = rouge_scores["rouge2"].fmeasure
        metrics["rougeL"] = rouge_scores["rougeL"].fmeasure
        
        # BLEU score
        try:
            bleu_score = sentence_bleu([reference.split()], generated.split())
            metrics["bleu"] = bleu_score
        except Exception as e:
            logger.warning(f"BLEU calculation failed: {e}")
            metrics["bleu"] = 0.0
        
        # BERTScore
        try:
            P, R, F1 = bertscore([generated], [reference], lang="en")
            metrics["bertscore_precision"] = P.mean().item()
            metrics["bertscore_recall"] = R.mean().item()
            metrics["bertscore_f1"] = F1.mean().item()
        except Exception as e:
            logger.warning(f"BERTScore calculation failed: {e}")
            metrics["bertscore_precision"] = 0.0
            metrics["bertscore_recall"] = 0.0
            metrics["bertscore_f1"] = 0.0
        
        return metrics
    
    def calculate_faithfulness(self, answer: str, context: str, threshold: float = 0.7) -> float:
        """
        Calculate faithfulness score based on answer-context alignment. Args:
            answer: Generated answer
            context: Source context
            threshold: Similarity threshold for claim support
            
        Returns:
            Faithfulness score (0-1)
        """
        try:
            # Get embeddings
            answer_embedding = self.sentence_model.encode([answer])
            context_embedding = self.sentence_model.encode([context])
            
            # Calculate similarity
            similarity = cosine_similarity(answer_embedding, context_embedding)[0][0]
            
            # Apply threshold
            return 1.0 if similarity >= threshold else similarity
            
        except Exception as e:
            logger.error(f"Faithfulness calculation failed: {e}")
            return 0.0
    
    def calculate_relevance(self, query: str, answer: str) -> float:
        """
        Calculate relevance score between query and answer. Args:
            query: Input query
            answer: Generated answer
            
        Returns:
            Relevance score (0-1)
        """
        try:
            # Get embeddings
            query_embedding = self.sentence_model.encode([query])
            answer_embedding = self.sentence_model.encode([answer])
            
            # Calculate similarity
            similarity = cosine_similarity(query_embedding, answer_embedding)[0][0]
            return similarity
            
        except Exception as e:
            logger.error(f"Relevance calculation failed: {e}")
            return 0.0
    
    def calculate_context_utilization(self, answer: str, context: str) -> float:
        """
        Calculate how much of the context is utilized in the answer. Args:
            answer: Generated answer
            context: Source context
            
        Returns:
            Context utilization score (0-1)
        """
        try:
            # Split into sentences
            answer_sentences = [s.strip() for s in answer.split('.') if s.strip()]
            context_sentences = [s.strip() for s in context.split('.') if s.strip()]
            
            if not answer_sentences or not context_sentences:
                return 0.0
            
            # Calculate sentence-level similarities
            answer_embeddings = self.sentence_model.encode(answer_sentences)
            context_embeddings = self.sentence_model.encode(context_sentences)
            
            similarities = cosine_similarity(answer_embeddings, context_embeddings)
            
            # Find best matching context sentence for each answer sentence
            max_similarities = np.max(similarities, axis=1)
            
            # Calculate utilization as average of best matches
            utilization = np.mean(max_similarities)
            return float(utilization)
            
        except Exception as e:
            logger.error(f"Context utilization calculation failed: {e}")
            return 0.0
    
    def evaluate_end_to_end(self, query: str, answer: str, context: str, 
                           retrieved_docs: List[str] = None, relevant_docs: List[str] = None) -> Dict[str, float]:
        """
        Evaluate end-to-end RAG performance. Args:
            query: Input query
            answer: Generated answer
            context: Retrieved context
            retrieved_docs: List of retrieved document IDs
            relevant_docs: List of relevant document IDs
            
        Returns:
            Dictionary of end-to-end metrics
        """
        metrics = {}
        
        # Faithfulness
        metrics["faithfulness"] = self.calculate_faithfulness(answer, context)
        
        # Relevance
        metrics["relevance"] = self.calculate_relevance(query, answer)
        
        # Context utilization
        metrics["context_utilization"] = self.calculate_context_utilization(answer, context)
        
        # Retrieval metrics (if provided)
        if retrieved_docs and relevant_docs:
            retrieval_metrics = self.evaluate_retrieval(retrieved_docs, relevant_docs)
            metrics.update(retrieval_metrics)
        
        return metrics
    
    def evaluate_latency(self, start_time: float, end_time: float) -> Dict[str, float]:
        """
        Evaluate latency metrics. Args:
            start_time: Start timestamp
            end_time: End timestamp
            
        Returns:
            Dictionary of latency metrics
        """
        total_time = end_time - start_time
        
        return {
            "total_latency": total_time,
            "latency_ms": total_time * 1000,
            "throughput_qps": 1.0 / total_time if total_time > 0 else 0.0
        }
    
    def comprehensive_evaluation(self, test_cases: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Run comprehensive evaluation on multiple test cases. Args:
            test_cases: List of test cases with query, answer, context, etc. Returns:
            Comprehensive evaluation results
        """
        results = {
            "individual_results": [],
            "aggregate_metrics": {},
            "summary": {}
        }
        
        all_metrics = []
        
        for i, test_case in enumerate(test_cases):
            start_time = time.time()
            
            # Extract test case data
            query = test_case.get("query", "")
            answer = test_case.get("answer", "")
            context = test_case.get("context", "")
            reference = test_case.get("reference", "")
            retrieved_docs = test_case.get("retrieved_docs", [])
            relevant_docs = test_case.get("relevant_docs", [])
            
            # Run evaluations
            generation_metrics = self.evaluate_generation(reference, answer) if reference else {}
            end_to_end_metrics = self.evaluate_end_to_end(query, answer, context, retrieved_docs, relevant_docs)
            latency_metrics = self.evaluate_latency(start_time, time.time())
            
            # Combine metrics
            case_metrics = {
                "test_case_id": i,
                "query": query,
                **generation_metrics,
                **end_to_end_metrics,
                **latency_metrics
            }
            
            results["individual_results"].append(case_metrics)
            all_metrics.append(case_metrics)
        
        # Calculate aggregate metrics
        if all_metrics:
            numeric_keys = [k for k in all_metrics[0].keys() if isinstance(all_metrics[0][k], (int, float))]
            
            for key in numeric_keys:
                values = [m[key] for m in all_metrics if key in m]
                if values:
                    results["aggregate_metrics"][key] = {
                        "mean": np.mean(values),
                        "std": np.std(values),
                        "min": np.min(values),
                        "max": np.max(values),
                        "median": np.median(values)
                    }
        
        # Generate summary
        results["summary"] = {
            "total_test_cases": len(test_cases),
            "evaluation_timestamp": time.time(),
            "model_used": self.model_name
        }
        
        return results

def run_evaluation_pipeline(test_queries: List[Dict], rag_system, faithfulness_scorer, llm=None) -> List[Dict]:
    """
    Run evaluation pipeline on test queries. Args:
        test_queries: List of test queries
        rag_system: RAG system for retrieval
        faithfulness_scorer: Faithfulness scorer instance
        llm: Optional LLM for answer generation
        
    Returns:
        List of evaluation results
    """
    evaluator = RAGEvaluator()
    results = []
    
    for query_data in test_queries:
        query = query_data["query"]
        expected_answer = query_data.get("expected_answer", "")
        expected_sources = query_data.get("expected_sources", [])
        
        start_time = time.time()
        
        try:
            # Retrieve relevant documents
            retrieved_results = rag_system.hybrid_search(query, top_k=5)
            retrieved_docs = [r["metadata"]["source_file"] for r in retrieved_results]
            
            # Generate context
            context = "\n".join([r["content"] for r in retrieved_results])
            
            # Generate answer (if LLM provided)
            if llm:
                answer = llm.invoke(f"Context: {context}\nQuery: {query}")
            else:
                answer = f"Based on the context: {context[:200]}..."
            
            # Evaluate
            retrieval_metrics = evaluator.evaluate_retrieval(retrieved_docs, expected_sources)
            generation_metrics = evaluator.evaluate_generation(expected_answer, answer)
            end_to_end_metrics = evaluator.evaluate_end_to_end(query, answer, context, retrieved_docs, expected_sources)
            latency_metrics = evaluator.evaluate_latency(start_time, time.time())
            
            # Combine results
            result = {
                "query": query,
                "answer": answer,
                "context": context,
                "retrieved_sources": retrieved_docs,
                "expected_sources": expected_sources,
                **retrieval_metrics,
                **generation_metrics,
                **end_to_end_metrics,
                **latency_metrics
            }
            
            results.append(result)
            
        except Exception as e:
            logger.error(f"Evaluation failed for query '{query}': {e}")
            results.append({
                "query": query,
                "error": str(e),
                "success": False
            })
    
    return results

# Example usage
if __name__ == "__main__":
    # Initialize evaluator
    evaluator = RAGEvaluator()
    
    # Example test case
    test_case = {
        "query": "How do I fix vlookup errors? ",
        "answer": "Check data types and table references for vlookup errors. ",
        "context": "VLOOKUP requires exact data types. Check table references and use FALSE for exact matches. ",
        "reference": "To fix vlookup errors, verify data types and table references.