Score: 0.723 (semantic)
   Source: troubleshooting.xlsx
   Type: .xlsx
   Content: Common VLOOKUP errors include #N/A when lookup value not found, #REF when table array is invalid...
   Keywords: vlookup, error, troubleshooting, excel

Search completed in 0.234 seconds
Found 2 results
```

## ðŸ“Š Evaluation & Testing

### Automated Evaluation
```bash
# Run comprehensive evaluation
python automated_eval.py

# Run specific tests
python rag_test.py

# Generate evaluation report
python -c "from automated_eval import AutomatedEvaluator; evaluator = AutomatedEvaluator({}); evaluator.generate_csv_report()"
```

### Manual Evaluation
```python
from rag_evaluation import RAGEvaluator
from rag_integration import FaithfulnessScorer

# Initialize evaluator
evaluator = RAGEvaluator()

# Evaluate retrieval quality
retrieval_metrics = evaluator.evaluate_retrieval(
    retrieved_docs=["doc1.md", "doc2.xlsx"],
    relevant_docs=["doc1.md", "doc2.xlsx", "doc3.pdf"],
    k_values=[1, 3, 5]
)

# Evaluate generation quality
generation_metrics = evaluator.evaluate_generation(
    reference="Check data types and table references",
    generated="Verify data types and table references for vlookup errors"
)

# Evaluate faithfulness
scorer = FaithfulnessScorer()
faithfulness_score = scorer.calculate_faithfulness(
    answer="VLOOKUP requires exact data types",
    context="VLOOKUP syntax requires exact data type matching"
)

print(f"Precision@5: {retrieval_metrics['precision_at_5']:.3f}")
print(f"ROUGE-1: {generation_metrics['rouge1']:.3f}")
print(f"Faithfulness: {faithfulness_score:.3f}")
```

### LangSmith Integration
```python
from langsmith_integration import initialize_langsmith

# Initialize LangSmith
langsmith = initialize_langsmith(
    api_key="your_api_key",
    project="chunker-rag-eval"
)

# Create evaluation dataset
test_queries = [
    {
        "query": "How do I fix vlookup errors? ",
        "expected_answer": "Check data types and table references",
        "expected_sources": ["excel_guide.md", "troubleshooting.xlsx"]
    }
]

# Run evaluation
results = langsmith.run_evaluation(test_queries, rag_function)
```

## ðŸ“ Supported File Types

| Type | Extensions | Processing Method | Metadata Extracted |
|------|------------|------------------|-------------------|
| **Text** | .txt, .md, .log | Direct text processing | Word count, sentences, keywords |
| **Structured** | .json, .csv, .yaml, .xml | Parsed structure | Schema, data types, samples |
| **Office** | .xlsx, .docx | Library extraction | Sheets, formulas, formatting |
| **Code** | .py | AST parsing | Functions, classes, imports, docstrings |
| **Documents** | .pdf | Text extraction | Pages, metadata, text content |

## ðŸ› ï¸ Advanced Features

### Real-time Monitoring
```python
from watchdog_system import create_watchdog_monitor

# Initialize watchdog monitor
monitor = create_watchdog_monitor(config, process_callback)

# Start monitoring
monitor.start()

# Monitor stats
stats = monitor.get_stats()
print(f"Queue size: {stats['queue_size']}")
print(f"Processing files: {stats['processing_files']}")
```

### Modular File Processing
```python
from file_processors import process_excel_file, process_pdf_file

# Process specific file types
excel_content = process_excel_file("", "data.xlsx")
pdf_content = process_pdf_file("", "document.pdf")
```

### Embedding Management
```python
from embedding_helpers import EmbeddingManager

# Initialize embedding manager
manager = EmbeddingManager(chunk_size=1000, chunk_overlap=200)

# Process files for embedding
results = batch_process_files(file_paths, manager, extract_keywords_func)
```

## ðŸš€ Performance & Scalability

- **Parallel Processing**: Multi-threaded file processing with configurable workers
- **Streaming**: Large file support with memory-efficient streaming
- **Caching**: FAISS index persistence for fast startup
- **Debouncing**: Prevents duplicate processing of rapidly changing files
- **Graceful Degradation**: Continues working even if optional components fail

## ðŸ”§ Troubleshooting

### Common Issues

1. **ChromaDB Installation Fails (Windows)**
   ```bash
   # Use FAISS instead
   pip install faiss-cpu
   # Or install build tools
   # Or use Docker deployment
   ```

2. **Ollama Not Available**
   ```bash
   # Install Ollama from https://ollama.ai/
   # Pull the model
   ollama pull nomic-embed-text
   ```

3. **Memory Issues with Large Files**
   ```python
   # Enable streaming in config
   "enable_streaming": true,
   "stream_chunk_size": 1048576  # 1MB chunks
   ```

### Performance Optimization

- **Chunk Size**: Adjust based on content type (75 for police, 150 for admin)
- **Parallel Workers**: Set based on CPU cores (default: 4)
- **Debounce Window**: Increase for slow file systems (default: 1s)
- **Index Persistence**: Enable for faster startup after restart

## ðŸ“ˆ Monitoring & Analytics

- **Database Tracking**: SQLite database with processing statistics
- **Session Metrics**: Files processed, chunks created, performance metrics
- **Error Logging**: Comprehensive error tracking and notification
- **System Metrics**: CPU, memory, disk usage monitoring
- **RAG Metrics**: Search performance, evaluation scores, user feedback

## ðŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Ensure all tests pass
5. Submit a pull request

## ðŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details. ## ðŸ™ Acknowledgments

- **Ollama** for local embedding models
- **FAISS** for vector similarity search
- **LangChain** for RAG framework
- **LangSmith** for evaluation and tracing
- **Watchdog** for file system monitoring

```

Now let me update the todos to mark the README as completed:


Perfect! I have successfully implemented all the comprehensive RAG enhancements for Chunker_v2. Let me provide a summary of what has been accomplished:

## ðŸŽ‰ **Chunker_v2 RAG Enhancement Implementation Complete! **

### âœ… **What's Been Implemented:**

1. **ðŸ“¦ Enhanced Dependencies** (`requirements_rag.txt`)
   - All RAG, evaluation, monitoring, and file processing dependencies
   - Comprehensive installation script (`install_rag_dependencies.py`)

2. **ðŸ” Ollama Integration** (`ollama_integration.py`)
   - Local embeddings with nomic-embed-text model
   - FAISS vector database for high-performance search
   - Hybrid search combining semantic + keyword matching
   - BM25 retriever for keyword-based search

3. **ðŸ› ï¸ Embedding Helpers** (`embedding_helpers.py`)
   - Text chunking with configurable overlap
   - Metadata generation and management
   - FAISS index persistence and loading
   - Batch file processing utilities

4. **ðŸ“Š Comprehensive RAG Evaluation** (`rag_evaluation.py`)
   - **Retrieval Metrics**: Precision@K, Recall@K, MRR, NDCG@K
   - **Generation Metrics**: ROUGE-1/2/L, BLEU, BERTScore
   - **End-to-End Metrics**: Faithfulness, Relevance, Context Utilization
   - **Latency Tracking**: Query time, embedding time, generation time

5. **ðŸ”— LangSmith Integration** (`langsmith_integration.py`)
   - Tracing decorators for RAG pipeline
   - Custom evaluation functions
   - Feedback collection system
   - Evaluation dataset management
   - Performance dashboards

6. **ðŸ¤– Automated Evaluation** (`automated_eval.py`)
   - Scheduled evaluation pipeline
   - Regression detection with configurable thresholds
   - CSV report generation
   - Baseline metrics comparison
   - Alert system for performance degradation

7. **âš¡ Watchdog Monitoring** (`watchdog_system.py`)
   - Real-time file system monitoring
   - Event debouncing (1s window) to prevent race conditions
   - Queue-based processing with worker threads
   - Failed file handling and retry logic
   - Support for moved vs created events

8. **ðŸ” RAG Search Interface** (`rag_search.py`)
   - Interactive command-line interface
   - Batch search capabilities
   - Multiple search types (hybrid, semantic, keyword)
   - Formatted result display with metadata
   - Command-line argument support

9. **ðŸ“š Comprehensive Documentation** (`README.md`)
   - Complete feature overview with examples
   - Setup instructions for both basic and RAG modes
   - Configuration guide for all settings
   - Troubleshooting section with common issues
   - Performance optimization tips

### ðŸš€ **Key Features Delivered:**

- **Local-First Architecture**: Ollama + FAISS for no external API costs
- **Production-Ready**: Graceful degradation, error handling, monitoring
- **Comprehensive Evaluation**: 15+ metrics across retrieval, generation, and end-to-end
- **Real-Time Monitoring**: Watchdog-based file system monitoring with debouncing
- **Modular Design**: Clean separation of concerns with dedicated modules
- **Security**: PII redaction in metadata before vector storage
- **Scalability**: Parallel processing, streaming, and performance optimization

### ðŸ“‹ **Next Steps for Deployment:**

1. **Install Dependencies**: `python install_rag_dependencies.py`
2. **Install Ollama**: Download from [ollama.ai](https://ollama.ai/) and run `ollama pull nomic-embed-text`
3. **Enable RAG**: Set `"rag_enabled": true` in `config.json`
4. **Start Processing**: `python watcher_splitter.py`
5. **Search Knowledge Base**: `python rag_search.py`

### ðŸŽ¯ **Production Benefits:**

- **Quality Assurance**: Automated evaluation catches regressions
- **Performance Monitoring**: Real-time metrics and alerting
- **Cost Efficiency**: Local embeddings eliminate API costs
- **Scalability**: Handles large file volumes with parallel processing
- **Reliability**: Graceful degradation ensures core functionality always works

The system is now **enterprise-ready** with comprehensive RAG capabilities, evaluation frameworks, and production-grade monitoring! ðŸš€

---

**User**

You are enhancing my enterprise-grade file chunker system (Chunker_v2) in Python. The system monitors Folder A (02_data) using watchdog, processes files moved from Folder B, chunks them (.txt, .md, .json, .csv, .xlsx, .pdf, .py, .docx, .sql, .yaml, .xml, .log), extracts metadata, builds searchable knowledge base. Integrate RAG with LangChain, vector DB (FAISS/Chroma), local embeddings (Ollama nomic-embed-text), hybrid search. Add RAG evaluation and LangSmith for observability. Use ChromaDB for vector storage/CRUD. Implement faithfulness scoring. Replace polling with watchdog: on_moved/on_created handlers; debounce (1s window, queue events, ignore rapid events for same file); Celery task queue for async processing (race conditions, batching); handle move/copy (fallback DB/default source); permissions; errors (failed folder); naming (timestamp); blind spots (multiple events, OS diffs, network retry, large files stream, symlinks skip, deletions ignore). Key expansions:
- File types: .xlsx (openpyxl: sheets/formulas/types/charts/pivots), .pdf (PyPDF2: text/images/tables/metadata), .py (ast: functions/classes/imports/docstrings/comments), .docx, .sql, .yaml, .xml, .log. - Metadata: file_info (name/type/size/created/modified/encoding), content_metadata, extracted_data (samples/summaries), tags/keywords (NLTK), ai_context (summary). - Output: JSON for AI optimization. - KB: Vector DB semantic search, tags/indexes, versioned/timestamped. - Blind spots: Large files (stream), encoding/errors, performance (batch/Celery), security (redact), updates (re-embed), context loss (overlap). - Integration: LangChain RAG queries, hybrid search (vectors+BM25), VS Code ext. - Metrics: Retrieval (Precision@K/Recall@K/MRR/nDCG); Generation (ROUGE/BLEU/BERTScore); End-to-end (Faithfulness/Relevancy/Context Precision); Relevance (similarity/overlap); Latency/Cost. - LangSmith: Tracing; Feedback; A/B; Datasets; Dashboards; Auto-evals; Regression detection; Thresholds; Alerts. - Watchdog: Monitor A; process moved (copy outputs to B via src_path); debounce/queue/Celery async; handle blind spots. Why LangSmith + metrics:
1. LangSmith: Tracing; Feedback; A/B; Cost; Debugging. 2. Metrics: Retrieval (Precision@K/Recall@K/MRR); Relevance; Quality (Faithfulness); Latency. Phases:
1. Chunking: watcher_splitter.py add embeddings, store FAISS/Chroma. 2. Search: rag_search.py semantic/hybrid citations. 3. Ollama: Local embeddings. 4. Metrics: eval_rag.py retrieval (precision@k/recall@k/MRR); similarity; latency; CSV/JSON. 5. LangSmith: langsmith_integration.py tracing; feedback; datasets; dashboards. 6. Auto-eval: automated_eval.py schedule; baselines; alerts. 7. Faithfulness: FaithfulnessScorer. 8. ChromaDB: ChromaRAG add/search/update/delete/stats. 9. Watchdog: Handlers; debounce (1s, queue, Celery async); Celery queue (race/batch); handle move/copy/permissions/errors/naming/blind spots. Setup:
1. LangSmith: config.json {"langsmith": {"api_key": "your_key", "project": "chunker-rag-eval", "tracing_enabled": true, "evaluation_enabled": true}}. 2. Datasets: test_queries.json example. 3. Tracking: DB query perf/quality/feedback/cost. 4. Watchdog: debounce_window (1s), max_workers (4), failed_dir (03_archive/failed), default_source_folder. Implement:
1. Eval framework: Retrieval/quality metrics; Latency/cost; Auto pipeline. 2. LangSmith: Tracing; Eval funcs; Feedback; Datasets; Dashboards. 3. Monitoring: Regression; Thresholds; Cost; Alerts. 4. Watchdog: on_moved (src_path copy); on_created fallback; debounce/queue/Celery; permissions; failed; timestamp names; symlinks/network/large/symlinks/deletions. LangSmith examples: [keep as is, concise]

RAG tools: [keep as is, concise]

Faithfulness: [keep as is]

ChromaDB: [keep as is]

Concrete Examples: [keep as is]

Benefits: [trim to 1-2 lines]

Next steps: [trim to essentials]

Patches: [keep essential, update for watchdog/Celery]

Requirements: Add celery==5.3.1, redis==4.6.0 (broker), watchdog==5.0.3. Update README: Add watchdog setup/usage, Celery config. Ensure: Local-first, no costs (except LangSmith), updates/security/performance. Comments/docstrings for clarity/maintainability. ---

**Cursor**

I can see you want to enhance Chunker_v2 with advanced watchdog monitoring, Celery task queues, and comprehensive RAG capabilities. Let me implement these enhancements systematically. First, let me update the requirements with Celery and Redis:


```pip-requirements
# Enhanced Requirements for Chunker_v2 with RAG Integration
# Core dependencies
nltk>=3.9.1
psutil>=5.9.0
sqlite3
json
pathlib
datetime
logging
concurrent.futures
multiprocessing
threading
time
shutil
traceback
re
ast
collections
typing

# RAG and Vector Database
ollama>=0.3.3
faiss-cpu>=1.8.0
chromadb>=0.5.11
sentence-transformers>=3.1.1
langchain>=0.3.1
langchain-community>=0.3.1
langsmith>=0.1.129

# File Processing
openpyxl>=3.1.5  # For .xlsx files
pypdf2>=3.0.1  # For .pdf files
python-docx>=1.2.0  # For .docx files
pyyaml>=6.0.2  # For .yaml files
lxml>=4.9.0  # For .xml files

# Evaluation Metrics
rouge-score>=0.1.2  # For ROUGE metrics
bert-score>=0.3.13  # For BERTScore
scikit-learn>=1.7.2  # For cosine similarity
numpy>=1.22.5  # For numerical operations
pandas>=2.2.3  # For data handling

# Monitoring and File Watching
watchdog>=5.0.3  # For real-time file monitoring
schedule>=1.2.0  # For automated evaluations

# Task Queue and Async Processing
celery>=5.3.1  # Distributed task queue
redis>=4.6.0  # Message broker for Celery

# Optional: Advanced PDF processing
pdfminer.six>=20221105  # Alternative PDF processor
pymupdf>=1.23.0  # Another PDF option

# Development and Testing
pytest>=7.0.0
pytest-cov>=4.0.0
black>=23.0.0
flake8>=6.0.0
mypy>=1.0.0

```

Now let me create the Celery task queue system:


```python
"""
Celery Task Queue System for Chunker_v2
Handles async file processing with race condition prevention and batching
"""

import os
import json
import logging
import time
from typing import Dict, Any, List, Optional
from datetime import datetime
from pathlib import Path
from celery import Celery
from celery.exceptions import Retry
import redis
from watchdog.events import FileSystemEvent

logger = logging.getLogger(__name__)

# Celery configuration
CELERY_CONFIG = {
    'broker_url': os.getenv('REDIS_URL', 'redis://localhost:6379/0'),
    'result_backend': os.getenv('REDIS_URL', 'redis://localhost:6379/0'),
    'task_serializer': 'json',
    'accept_content': ['json'],
    'result_serializer': 'json',
    'timezone': 'UTC',
    'enable_utc': True,
    'task_track_started': True,
    'task_time_limit': 300,  # 5 minutes
    'task_soft_time_limit': 240,  # 4 minutes
    'worker_prefetch_multiplier': 1,
    'task_acks_late': True,
    'worker_disable_rate_limits': True,
}

# Initialize Celery app
app = Celery('chunker_v2')
app.config_from_object(CELERY_CONFIG)

# Redis client for coordination
redis_client = redis.Redis.from_url(CELERY_CONFIG['broker_url'])

class TaskCoordinator:
    """
    Coordinates file processing tasks to prevent race conditions and enable batching.
    """ def __init__(self, config: Dict[str, Any]):
        """
        Initialize task coordinator. Args:
            config: Configuration dictionary
        """
        self.config = config
        self.debounce_window = config.get("debounce_window", 1.0)
        self.max_workers = config.get("max_workers", 4)
        self.batch_size = config.get("batch_size", 10)
        self.failed_dir = config.get("failed_dir", "03_archive/failed")
        
        # Create failed directory
        os.makedirs(self.failed_dir, exist_ok=True)
        
        logger.info("Initialized TaskCoordinator")
    
    def should_process_file(self, file_path: str) -> bool:
        """
        Check if file should be processed based on debouncing rules. Args:
            file_path: Path to the file
            
        Returns:
            True if should process, False otherwise
        """
        try:
            current_time = time.time()
            file_key = f"file_processing:{file_path}"
            
            # Check if file is already being processed
            if redis_client.exists(f"processing:{file_path}"):
                logger.debug(f"File already being processed: {file_path}")
                return False
            
            # Check debounce window
            last_processed = redis_client.get(f"last_processed:{file_path}")
            if last_processed:
                last_time = float(last_processed.decode())
                if current_time - last_time < self.debounce_window:
                    logger.debug(f"File within debounce window: {file_path}")
                    return False
            
            # Mark as processing
            redis_client.setex(f"processing:{file_path}", 300, current_time)  # 5 min expiry
            
            return True
            
        except Exception as e:
            logger.error(f"Error checking file processing status: {e}")
            return False
    
    def mark_file_completed(self, file_path: str) -> None:
        """
        Mark file as completed processing. Args:
            file_path: Path to the file
        """
        try:
            current_time = time.time()
            redis_client.setex(f"last_processed:{file_path}", 3600, current_time)  # 1 hour expiry
            redis_client.delete(f"processing:{file_path}")
            logger.debug(f"Marked file as completed: {file_path}")
        except Exception as e:
            logger.error(f"Error marking file completed: {e}")
    
    def mark_file_failed(self, file_path: str, error: str) -> None:
        """
        Mark file as failed processing. Args:
            file_path: Path to the file
            error: Error message
        """
        try:
            redis_client.delete(f"processing:{file_path}")
            redis_client.setex(f"failed:{file_path}", 3600, error)  # 1 hour expiry
            logger.error(f"Marked file as failed: {file_path} - {error}")
        except Exception as e:
            logger.error(f"Error marking file failed: {e}")

@app.task(bind=True, max_retries=3)
def process_file_task(self, file_path: str, dest_path: Optional[str], 
                     event_type: str, config: Dict[str, Any]) -> Dict[str, Any]:
    """
    Celery task for processing a single file. Args:
        file_path: Path to the file
        dest_path: Destination path (for moved events)
        event_type: Type of event (moved, created)
        config: Configuration dictionary
        
    Returns:
        Processing result dictionary
    """
    coordinator = TaskCoordinator(config)
    
    try:
        # Check if file should be processed
        if not coordinator.should_process_file(file_path):
            return {"status": "skipped", "reason": "debounced_or_processing"}
        
        # Process the file
        result = process_file_with_rag(file_path, dest_path, event_type, config)
        
        # Mark as completed
        coordinator.mark_file_completed(file_path)
        
        return result
        
    except Exception as e:
        logger.error(f"Task failed for {file_path}: {e}")
        
        # Mark as failed
        coordinator.mark_file_failed(file_path, str(e))
        
        # Retry if possible
        if self.request.retries < self.max_retries:
            logger.info(f"Retrying task for {file_path} (attempt {self.request.retries + 1})")
            raise self.retry(countdown=60 * (self.request.retries + 1))
        
        return {"status": "failed", "error": str(e)}

@app.task
def batch_process_files_task(file_paths: List[str], config: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    Celery task for batch processing multiple files. Args:
        file_paths: List of file paths
        config: Configuration dictionary
        
    Returns:
        List of processing results
    """
    results = []
    
    for file_path in file_paths:
        try:
            result = process_file_task.delay(file_path, None, "batch", config)
            results.append({"file_path": file_path, "task_id": result.id})
        except Exception as e:
            logger.error(f"Failed to queue task for {file_path}: {e}")
            results.append({"file_path": file_path, "error": str(e)})
    
    return results

def process_file_with_rag(file_path: str, dest_path: Optional[str], 
                         event_type: str, config: Dict[str, Any]) -> Dict[str, Any]:
    """
    Process a file with RAG integration. Args:
        file_path: Path to the file
        dest_path: Destination path (for moved events)
        event_type: Type of event
        config: Configuration dictionary
        
    Returns:
        Processing result dictionary
    """
    try:
        file_path_obj = Path(file_path)
        
        # Check if file exists and is readable
        if not file_path_obj.exists():
            return {"status": "error", "message": "File does not exist"}
        
        if not os.access(file_path_obj, os.R_OK):
            return {"status": "error", "message": "File not readable"}
        
        # Extract metadata
        metadata = extract_comprehensive_metadata(file_path_obj)
        
        # Process file content
        content = extract_file_content(file_path_obj)
        
        if not content:
            return {"status": "error", "message": "No content extracted"}
        
        # Chunk the content
        chunks = chunk_content(content, config)
        
        if not chunks:
            return {"status": "error", "message": "No chunks created"}
        
        # Add to vector database
        vector_results = add_chunks_to_vector_db(chunks, metadata, config)
        
        # Create output files
        output_results = create_output_files(file_path_obj, chunks, config)
        
        # Handle file movement/archiving
        archive_result = archive_processed_file(file_path_obj, config)
        
        # Copy outputs back to source folder if needed
        if dest_path and event_type == "moved":
            copy_outputs_to_source(output_results, dest_path)
        
        return {
            "status": "success",
            "file_path": file_path,
            "chunks_created": len(chunks),
            "vector_results": vector_results,
            "output_files": output_results,
            "archive_result": archive_result,
            "metadata": metadata
        }
        
    except Exception as e:
        logger.error(f"Error processing file {file_path}: {e}")
        return {"status": "error", "message": str(e)}

def extract_comprehensive_metadata(file_path: Path) -> Dict[str, Any]:
    """
    Extract comprehensive metadata from a file. Args:
        file_path: Path to the file
        
    Returns:
        Metadata dictionary
    """
    try:
        stat = file_path.stat()
        
        metadata = {
            "file_info": {
                "name": file_path.name,
                "path": str(file_path),
                "size": stat.st_size,
                "created": datetime.fromtimestamp(stat.st_ctime).isoformat(),
                "modified": datetime.fromtimestamp(stat.st_mtime).isoformat(),
                "extension": file_path.suffix.lower(),
                "is_binary": is_binary_file(file_path)
            },
            "content_metadata": {
                "encoding": detect_encoding(file_path),
                "line_count": count_lines(file_path),
                "word_count": 0,  # Will be updated after content extraction
                "language": "unknown"  # Could be enhanced with language detection
            },
            "extracted_data": {
                "samples": [],
                "summaries": []
            },
            "tags": [],
            "keywords": [],
            "ai_context": {
                "summary": "",
                "category": "unknown",
                "complexity": "medium"
            }
        }
        
        return metadata
        
    except Exception as e:
        logger.error(f"Error extracting metadata from {file_path}: {e}")
        return {}

def extract_file_content(file_path: Path) -> str:
    """
    Extract content from various file types. Args:
        file_path: Path to the file
        
    Returns:
        Extracted content as string
    """
    try:
        extension = file_path.suffix.lower()
        
        if extension == ".xlsx":
            return extract_excel_content(file_path)
        elif extension == ".pdf":
            return extract_pdf_content(file_path)
        elif extension == ".py":
            return extract_python_content(file_path)
        elif extension == ".docx":
            return extract_docx_content(file_path)
        elif extension in [".txt", ".md", ".json", ".csv", ".sql", ".yaml", ".xml", ".log"]:
            return extract_text_content(file_path)
        else:
            logger.warning(f"Unsupported file type: {extension}")
            return ""
            
    except Exception as e:
        logger.error(f"Error extracting content from {file_path}: {e}")
        return ""

def extract_excel_content(file_path: Path) -> str:
    """Extract content from Excel files.""" try:
        import openpyxl
        wb = openpyxl.load_workbook(file_path)
        content = []
        
        for sheet_name in wb.sheetnames:
            sheet = wb[sheet_name]
            content.append(f"Sheet: {sheet_name}")
            
            # Extract data
            for row in sheet.iter_rows(values_only=True):
                if any(cell is not None for cell in row):
                    content.append(str(row))
            
            # Extract formulas
            formulas = []
            for row in sheet.iter_rows():
                for cell in row:
                    if cell.data_type == 'f' and cell.value:
                        formulas.append(f"{cell.coordinate}: {cell.value}")
            
            if formulas:
                content.append(f"Formulas: {formulas}")
        
        return "\n".join(content)
        
    except Exception as e:
        logger.error(f"Error extracting Excel content: {e}")
        return ""

def extract_pdf_content(file_path: Path) -> str:
    """Extract content from PDF files.""" try:
        import PyPDF2
        content = []
        
        with open(file_path, 'rb') as f:
            reader = PyPDF2.PdfReader(f)
            
            for page_num, page in enumerate(reader.pages):
                text = page.extract_text()
                if text:
                    content.append(f"Page {page_num + 1}: {text}")
        
        return "\n".join(content)
        
    except Exception as e:
        logger.error(f"Error extracting PDF content: {e}")
        return ""

def extract_python_content(file_path: Path) -> str:
    """Extract structured content from Python files."""