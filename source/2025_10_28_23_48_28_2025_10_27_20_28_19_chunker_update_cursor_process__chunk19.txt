---

**User**

```python
# ðŸ•’ 2025-06-28-01-55-30
# chunker/watcher_splitter_enterprise.py
# Author: R. A. Carucci  
# Purpose: Enterprise-grade chunker with database tracking, notifications, and parallel processing

import os
import sys
import time 
import shutil
import logging
import traceback
from datetime import datetime, timedelta
from pathlib import Path
import nltk
from nltk.tokenize import sent_tokenize
import json
import psutil
from concurrent.futures import ThreadPoolExecutor
import multiprocessing
from chunker_db import ChunkerDatabase
from notification_system import NotificationSystem
import re
import openpyxl
import PyPDF2
import ast
import docx
import yaml
from rag_integration import ChromaRAG, extract_keywords

# Resolve config path (supports PyInstaller .exe)
if getattr(sys, 'frozen', False) and hasattr(sys, '_MEIPASS'):
    base_path = sys._MEIPASS
else:
    base_path = os.path.dirname(__file__)

# Configure local nltk_data path
nltk_path = os.path.join(base_path, "nltk_data")
nltk.data.path.append(nltk_path)

try:
    nltk.download('punkt_tab', download_dir=nltk_path, quiet=True)
except:
    nltk.download('punkt', download_dir=nltk_path, quiet=True)

# Load configuration
with open(os.path.join(base_path, "config.json")) as f:
    CONFIG = json.load(f)

# Department-specific configurations
DEPARTMENT_CONFIGS = {
    "police": {
        "chunk_size": 75,
        "enable_redaction": True,
        "audit_level": "full",
        "priority": "high"
    },
    "admin": {
        "chunk_size": 150,
        "enable_redaction": False,
        "audit_level": "basic",
        "priority": "normal"
    },
    "legal": {
        "chunk_size": 100,
        "enable_redaction": True,
        "audit_level": "full",
        "priority": "high"
    }
}

# Setup enhanced logging
def setup_logging():
    log_file = CONFIG.get("log_file", "logs/watcher.log")
    os.makedirs(os.path.dirname(log_file), exist_ok=True)
    
    # Rotate log if it's too large
    if os.path.exists(log_file) and os.path.getsize(log_file) > 5 * 1024 * 1024:  # 5MB
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        archive_name = f"logs/watcher_archive_{timestamp}.log"
        shutil.move(log_file, archive_name)
    
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] %(message)s",
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler(sys.stdout)
        ]
    )
    return logging.getLogger(__name__)

logger = setup_logging()

# Initialize database and notification systems with timeout and retry
def init_database_with_retry():
    """Initialize database with retry logic to handle locking issues"""
    max_retries = 5
    for attempt in range(max_retries):
        try:
            db = ChunkerDatabase()
            logger.info("Database initialized successfully")
            return db
        except Exception as e:
            if attempt < max_retries - 1:
                logger.warning(f"Database initialization attempt {attempt + 1} failed: {e}")
                time.sleep(2)
            else:
                logger.error(f"Database initialization failed after {max_retries} attempts: {e}")
                return None

db = init_database_with_retry()
notifications = NotificationSystem()

# Enhanced session statistics
session_stats = {
    "session_start": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
    "files_processed": 0,
    "chunks_created": 0,
    "zero_byte_prevented": 0,
    "errors": 0,
    "total_sentences_processed": 0,
    "total_bytes_created": 0,
    "parallel_jobs_completed": 0,
    "department_breakdown": {},
    "performance_metrics": {
        "avg_processing_time": 0,
        "peak_memory_usage": 0,
        "peak_cpu_usage": 0
    }
}

def get_department_config(file_path):
    """Determine department configuration based on file path or content"""
    dept = CONFIG.get("default_department", "admin")
    
    # Check file path for department indicators
    path_str = str(file_path).lower()
    for department in DEPARTMENT_CONFIGS.keys():
        if department in path_str:
            dept = department
            break
    
    # Merge default config with department-specific settings
    dept_config = DEPARTMENT_CONFIGS.get(dept, {})
    merged_config = CONFIG.copy()
    merged_config.update(dept_config)
    merged_config["department"] = dept
    
    return merged_config

def log_system_metrics():
    """Log comprehensive system metrics"""
    try:
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        disk = psutil.disk_usage('.') # Count active processes
        active_processes = len([p for p in psutil.process_iter() if p.is_running()])
        
        # Update session stats
        session_stats["performance_metrics"]["peak_cpu_usage"] = max(
            session_stats["performance_metrics"]["peak_cpu_usage"], cpu_percent
        )
        session_stats["performance_metrics"]["peak_memory_usage"] = max(
            session_stats["performance_metrics"]["peak_memory_usage"], memory.percent
        )
        
        # Log to database with retry
        if db:
            try:
                db.log_system_metrics(cpu_percent, memory.percent, 
                                     (disk.used / disk.total) * 100, active_processes)
            except Exception as e:
                logger.warning(f"Failed to log system metrics to database: {e}")
        
        logger.info(f"System metrics - CPU: {cpu_percent}%, Memory: {memory.percent}%, "
                   f"Disk: {(disk.used / disk.total) * 100:.1f}%, Processes: {active_processes}")
        
        # Send alerts if thresholds exceeded
        if cpu_percent > 90:
            notifications.send_threshold_alert("CPU Usage", f"{cpu_percent}%", "90%", "critical")
        elif cpu_percent > 80:
            notifications.send_threshold_alert("CPU Usage", f"{cpu_percent}%", "80%", "warning")
        
        if memory.percent > 90:
            notifications.send_threshold_alert("Memory Usage", f"{memory.percent}%", "90%", "critical")
        elif memory.percent > 80:
            notifications.send_threshold_alert("Memory Usage", f"{memory.percent}%", "80%", "warning")
            
    except Exception as e:
        logger.error(f"Failed to log system metrics: {e}")

def chunk_text_enhanced(text, limit, department_config):
    """Enhanced chunking with department-specific rules"""
    logger.info(f"Starting chunking process - Text length: {len(text)} chars, Chunk limit: {limit} sentences")
    
    if not text or len(text.strip()) < 10:
        logger.warning("Text too short for chunking - skipping")
        return []
    
    try:
        sentences = sent_tokenize(text)
        logger.info(f"Tokenized text into {len(sentences)} sentences")
        
        if not sentences:
            logger.warning("No sentences found in text - skipping")
            return []
        
        # Apply department-specific chunking rules
        original_sentence_count = len(sentences)
        if department_config.get("enable_redaction"):
            sentences = apply_redaction_rules(sentences)
            logger.info(f"Applied redaction rules - {original_sentence_count} -> {len(sentences)} sentences")
        
        chunks = []
        max_chars = department_config.get("max_chunk_chars", CONFIG.get("max_chunk_chars", 30000))
        logger.info(f"Chunking parameters - Max chars per chunk: {max_chars}, Target sentences per chunk: {limit}")
        
        current_chunk = []
        current_length = 0
        chunk_count = 0
        
        for sentence in sentences:
            sentence_length = len(sentence)
            
            # Check if adding this sentence would exceed limits
            if (len(current_chunk) >= limit or 
                current_length + sentence_length > max_chars) and current_chunk:
                
                chunk_text = " ".join(current_chunk)
                if len(chunk_text.strip()) > 0:
                    chunks.append(chunk_text)
                    chunk_count += 1
                    logger.debug(f"Created chunk {chunk_count}: {len(current_chunk)} sentences, {len(chunk_text)} chars")
                
                current_chunk = [sentence]
                current_length = sentence_length
            else:
                current_chunk.append(sentence)
                current_length += sentence_length
        
        # Add final chunk
        if current_chunk:
            chunk_text = " ".join(current_chunk)
            if len(chunk_text.strip()) > 0:
                chunks.append(chunk_text)
                chunk_count += 1
                logger.debug(f"Created final chunk {chunk_count}: {len(current_chunk)} sentences, {len(chunk_text)} chars")
        
        logger.info(f"Completed chunking - Created {chunk_count} chunks, Total sentences: {len(sentences)}")
        return chunks
    
    except Exception as e:
        logger.error(f"Chunking failed: {e}", exc_info=True)
        return []

def apply_redaction_rules(sentences):
    """Apply department-specific redaction rules"""
    # Example redaction for police/legal
    redacted = []
    for sentence in sentences:
        # Redact sensitive information
        sentence = re.sub(r'\b\d{3}-\d{2}-\d{4}\b', '[SSN]', sentence)  # SSN
        sentence = re.sub(r'\b\d{4} \d{4} \d{4} \d{4}\b', '[CC]', sentence)  # Credit card
        sentence = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\. [A-Z|a-z]{2,}\b', '[EMAIL]', sentence)
        redacted.append(sentence)
    return redacted

def process_excel_file(file_path):
    """Extract data from Excel files"""
    wb = openpyxl.load_workbook(file_path)
    content = []
    for sheet_name in wb.sheetnames:
        sheet = wb[sheet_name]
        content.append(f"Sheet: {sheet_name}")
        # Extract formulas, data types, etc. for row in sheet.iter_rows(values_only=True):
            content.append(str(row))
    return "\n".join(content)

def process_pdf_file(file_path):
    """Extract text from PDF files"""
    content = []
    with open(file_path, 'rb') as f:
        reader = PyPDF2.PdfReader(f)
        for page in reader.pages:
            content.append(page.extract_text())
    return "\n".join(content)

def process_python_file(file_path):
    """Extract code structure from Python files"""
    with open(file_path, 'r') as f:
        tree = ast.parse(f.read())
    # Extract functions, classes, imports
    functions = [node.name for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)]
    classes = [node.name for node in ast.walk(tree) if isinstance(node, ast.ClassDef)]
    imports = []
    for node in ast.walk(tree):
        if isinstance(node, ast.Import):
            imports.extend([alias.name for alias in node.names])
        elif isinstance(node, ast.ImportFrom):
            imports.append(node.module)
    return f"Functions: {functions}\nClasses: {classes}\nImports: {imports}"

def process_docx_file(file_path):
    """Extract text from DOCX files"""
    doc = docx.Document(file_path)
    return "\n".join([para.text for para in doc.paragraphs])

def process_yaml_file(file_path):
    """Extract data from YAML files"""
    with open(file_path, 'r') as f:
        data = yaml.safe_load(f)
    return json.dumps(data, indent=2)

def process_xml_file(file_path):
    """Extract data from XML files"""
    with open(file_path, 'r') as f:
        return f.read()

def process_log_file(file_path):
    """Extract text from LOG files"""
    with open(file_path, 'r') as f:
        return f.read()

def process_sql_file(file_path):
    """Extract text from SQL files"""
    with open(file_path, 'r') as f:
        return f.read()

def process_file_enhanced(file_path, config):
    """Enhanced file processing with comprehensive logging"""
    start_time = time.time()
    logger.info(f"Processing file: {file_path.name} - Size: {file_path.stat().st_size} bytes")
    try:
        file_type = file_path.suffix.lower()
        text = ""
        if file_type in [".txt", ".md", ".json", ".csv", ".sql", ".yaml", ".xml", ".log", ".py"]:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                text = f.read()
            if file_type == ".py":
                text = process_python_file(file_path)
            elif file_type == ".yaml":
                text = process_yaml_file(file_path)
            elif file_type == ".xml":
                text = process_xml_file(file_path)
            elif file_type == ".log":
                text = process_log_file(file_path)
            elif file_type == ".sql":
                text = process_sql_file(file_path)
        elif file_type == ".xlsx":
            text = process_excel_file(file_path)
        elif file_type == ".pdf":
            text = process_pdf_file(file_path)
        elif file_type == ".docx":
            text = process_docx_file(file_path)
        logger.info(f"Read {len(text)} characters from file")
        if not text.strip():
            logger.warning(f"Empty file after stripping whitespace: {file_path.name}")
            session_stats['zero_byte_prevented'] += 1
            return False
        # Get department config
        dept_config = get_department_config(file_path)
        # Chunk text
        chunks = chunk_text_enhanced(text, dept_config['chunk_size'], dept_config)
        if not chunks:
            logger.warning(f"No chunks created for file: {file_path.name}")
            return False
        # Create timestamp prefix
        timestamp = datetime.now().strftime('%Y_%m_%d_%H_%M_%S')
        # Create output directory with timestamp
        output_dir = Path(config['output_dir']) / f'{timestamp}_{file_path.stem}'
        output_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"Created output directory: {output_dir}")
        # Create chunks
        chunk_files = []
        for i, chunk in enumerate(chunks, 1):
            chunk_file = output_dir / f'{timestamp}_chunk_{i}.txt'
            with open(chunk_file, 'w', encoding='utf-8') as f:
                f.write(chunk)
            chunk_files.append(chunk_file)
        session_stats['chunks_created'] += len(chunks)
        session_stats['total_sentences_processed'] += len(sent_tokenize(text))
        session_stats['total_bytes_created'] += sum(len(chunk) for chunk in chunks)
        # Update department breakdown
        dept = dept_config['department']
        if dept not in session_stats['department_breakdown']:
            session_stats['department_breakdown'][dept] = {'files': 0, 'chunks': 0}
        session_stats['department_breakdown'][dept]['files'] += 1
        session_stats['department_breakdown'][dept]['chunks'] += len(chunks)
        # Log to database
        if db:
            try:
                db.log_processing(file_path.name, dept, len(chunks), len(text), time.time() - start_time)
            except Exception as db_error:
                logger.warning(f"Failed to log processing to database: {db_error}")
        # Send notification
        notifications.send_file_processed_notification(file_path.name, len(chunks), dept)
        # RAG integration
        if config.get("rag_enabled"):
            try:
                chroma_rag = ChromaRAG()
                for i, chunk in enumerate(chunks):
                    metadata = {
                        "file_name": file_path.name,
                        "file_type": file_path.suffix,
                        "chunk_index": i + 1,
                        "timestamp": datetime.now().isoformat(),
                        "department": dept,
                        "keywords": extract_keywords(chunk),
                        "file_size": file_path.stat().st_size,
                        "processing_time": time.time() - start_time
                    }
                    chunk_id = chroma_rag.add_chunk(chunk, metadata)
                    logger.info(f"Added chunk to ChromaDB: {chunk_id}")
            except Exception as e:
                logger.error(f"Failed to add to ChromaDB: {e}")
        # Archive original file
        archive_dir = Path(config['archive_dir'])
        archive_dir.mkdir(parents=True, exist_ok=True)
        archive_path = archive_dir / file_path.name
        shutil.move(str(file_path), str(archive_path))
        logger.info(f"Archived file to: {archive_path}")
        # Update processing time metric
        processing_time = time.time() - start_time
        session_stats['performance_metrics']['avg_processing_time'] = (
            session_stats['performance_metrics']['avg_processing_time'] * session_stats['files_processed'] + processing_time
        ) / (session_stats['files_processed'] + 1)
        session_stats['files_processed'] += 1
        return True
    except Exception as e:
        logger.exception(f"Error processing file {file_path.name}: {e}")
        session_stats['errors'] += 1
        if db:
            try:
                db.log_error('ProcessingError', str(e), traceback.format_exc(), file_path.name)
            except Exception as db_error:
                logger.warning(f"Failed to log error to database: {db_error}")
        try:
            notifications.send_error_alert(str(e), file_path.name, traceback.format_exc())
        except Exception as notify_error:
            logger.warning(f"Failed to send error alert: {notify_error}")
        return False

def process_files_parallel(files, config):
    """Process files in parallel using ThreadPoolExecutor"""
    with ThreadPoolExecutor(max_workers=min(4, multiprocessing.cpu_count())) as executor:
        futures = [executor.submit(process_file_enhanced, file, config) for file in files]
        results = [f.result() for f in futures]
    session_stats['parallel_jobs_completed'] += len(results)
    return results

def log_session_stats():
    """Log session statistics"""
    logger.info("=== Session Statistics ===")
    logger.info(f"Files Processed: {session_stats['files_processed']}")
    logger.info(f"Chunks Created: {session_stats['chunks_created']}")
    logger.info(f"Zero-byte Prevented: {session_stats['zero_byte_prevented']}")
    logger.info(f"Errors: {session_stats['errors']}")
    logger.info(f"Avg Processing Time: {session_stats['performance_metrics']['avg_processing_time']:.2f}s")
    logger.info(f"Peak CPU: {session_stats['performance_metrics']['peak_cpu_usage']}%")
    logger.info(f"Peak Memory: {session_stats['performance_metrics']['peak_memory_usage']}%")
    logger.info("Department Breakdown:")
    for dept, stats in session_stats['department_breakdown'].items():
        logger.info(f"  {dept}: {stats['files']} files, {stats['chunks']} chunks")
    logger.info("==========================")
    if db:
        try:
            db.log_session_stats(session_stats)
        except Exception as db_error:
            logger.warning(f"Failed to log session stats to database: {db_error}")

def main():
    watch_folder = CONFIG['watch_folder']
    logger.info("=== ENTERPRISE CHUNKER STARTED ===")
    logger.info(f"Monitoring: {watch_folder}")
    supported_extensions = CONFIG.get("supported_extensions", [".txt", ".md"])
    logger.info(f"File types: {', '.join(supported_extensions)} files")
    filter_mode = CONFIG.get("file_filter_mode", "all")
    logger.info(f"Filter mode: {filter_mode}")
    if filter_mode == "patterns":
        patterns = CONFIG.get("file_patterns", ["_full_conversation"])
        logger.info(f"Required patterns: {', '.join(patterns)}")
    elif filter_mode == "suffix":
        logger.info("Required suffix: _full_conversation")
    logger.info(f"Parallel processing: {min(4, multiprocessing.cpu_count())} workers")
    logger.info(f"Database tracking: Enabled")
    logger.info(f"Notifications: {'Enabled' if notifications.config.get('enable_notifications') else 'Disabled'}")
    
    processed_files = set()
    loop_count = 0
    last_cleanup = datetime.now()
    last_report = datetime.now()
    
    # Send startup notification
    notifications.send_email(
        notifications.config["admin_emails"],
        "ðŸš€ Chunker System Started",
        f"Enterprise Chunker system started at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
        f"Monitoring: {watch_folder}\n"
        f"File types: {', '.join(supported_extensions)} files\n"
        f"Filter mode: {filter_mode}\n"
        f"Parallel workers: {min(4, multiprocessing.cpu_count())}\n"
        f"Database: Enabled\n"
        f"Dashboard: http://localhost:5000"
    )
    
    try:
        while True:
            try:
                # Look for files with supported extensions
                all_files = []
                supported_extensions = CONFIG.get("supported_extensions", [".txt", ".md"])
                for ext in supported_extensions:
                    all_files.extend(list(Path(watch_folder).glob(f"*{ext}")))
                
                # Filter files based on configuration
                excluded_files = {"watcher_splitter.py", "test_chunker.py", "chunker_db.py", "notification_system.py"}
                
                # Apply file filtering based on mode
                filter_mode = CONFIG.get("file_filter_mode", "all")
                file_patterns = CONFIG.get("file_patterns", ["_full_conversation"])
                exclude_patterns = CONFIG.get("exclude_patterns", [])
                
                filtered_files = []
                for f in all_files:
                    if f.name in processed_files or not f.is_file() or f.name in excluded_files:
                        continue
                    
                    # Check exclude patterns first
                    if any(pattern in f.name for pattern in exclude_patterns):
                        logger.debug(f"Skipping file with exclude pattern: {f.name}")
                        continue
                    
                    # Apply filter mode
                    if filter_mode == "all":
                        filtered_files.append(f)
                    elif filter_mode == "patterns":
                        if any(pattern in f.name for pattern in file_patterns):
                            filtered_files.append(f)
                        else:
                            logger.debug(f"Skipping file without required pattern: {f.name}")
                    elif filter_mode == "suffix":
                        # Only process files with _full_conversation suffix
                        if "_full_conversation" in f.name:
                            filtered_files.append(f)
                        else:
                            logger.debug(f"Skipping file without _full_conversation suffix: {f.name}")
                
                new_files = filtered_files
                
                if new_files:
                    logger.info(f"Found {len(new_files)} new files to process")
                    
                    # Process files in parallel if multiple files
                    if len(new_files) > 1 and CONFIG.get("enable_parallel_processing", True):
                        results = process_files_parallel(new_files, CONFIG)
                        for i, result in enumerate(results):
                            if result:
                                processed_files.add(new_files[i].name)
                    else:
                        # Process files sequentially
                        for file_path in new_files:
                            try:
                                if process_file_enhanced(file_path, CONFIG):
                                    processed_files.add(file_path.name)
                                    logger.info(f"Successfully processed: {file_path.name}")
                                else:
                                    logger.error(f"Failed to process: {file_path.name}")
                            except Exception as e:
                                logger.exception(f"Error processing {file_path.name}: {e}")
                                if db:
                                    try:
                                        db.log_error("ProcessingError", str(e), traceback.format_exc(), str(file_path))
                                    except Exception as db_error:
                                        logger.warning(f"Failed to log processing error to database: {db_error}")
                
                # Periodic maintenance
                loop_count += 1
                
                # Log session stats every minute
                if loop_count % 12 == 0:  # Every minute at 5s intervals
                    log_session_stats()
                
                # Log system metrics every 5 minutes
                if loop_count % 60 == 0:
                    log_system_metrics()
                
                # Daily cleanup and reporting
                if datetime.now() - last_cleanup > timedelta(hours=24):
                    if db:
                        try:
                            db.cleanup_old_data(days=30)
                        except Exception as db_error:
                            logger.warning(f"Failed to run database cleanup: {db_error}")
                    last_cleanup = datetime.now()
                
                # Send daily report
                if datetime.now() - last_report > timedelta(hours=24):
                    if db:
                        try:
                            analytics = db.get_analytics(days=1)
                            notifications.send_daily_summary(session_stats, analytics)
                        except Exception as db_error:
                            logger.warning(f"Failed to get analytics or send daily summary: {db_error}")
                    last_report = datetime.now()
                
                time.sleep(CONFIG.get("polling_interval", 5))
                
            except KeyboardInterrupt:
                logger.info("Watcher stopped by user")
                break
            except Exception as e:
                logger.exception("Critical error in main loop")
                if db:
                    try:
                        db.log_error("MainLoopError", str(e), traceback.format_exc())
                    except Exception as db_error:
                        logger.warning(f"Failed to log main loop error to database: {db_error}")
                try:
                    notifications.send_error_alert(f"Critical main loop error: {str(e)}", stack_trace=traceback.format_exc())
                except Exception as notify_error:
                    logger.warning(f"Failed to send error alert: {notify_error}")
                time.sleep(10)
                
    finally:
        # Final statistics and cleanup
        log_session_stats()
        
        # Send shutdown notification
        notifications.send_email(
            notifications.config["admin_emails"],
            "ðŸ›‘ Chunker System Stopped",
            f"Enterprise Chunker system stopped at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
            f"Session Summary:\n"
            f"Files Processed: {session_stats['files_processed']}\n"
            f"Chunks Created: {session_stats['chunks_created']}\n"
            f"Zero-byte Prevented: {session_stats['zero_byte_prevented']}\n"
            f"Errors: {session_stats['errors']}\n"
            f"Uptime: {datetime.now() - datetime.strptime(session_stats['session_start'], '%Y-%m-%d %H:%M:%S')}"
        )

if __name__ == "__main__":
    main()
```

---

**Cursor**

I'll integrate the RAG capabilities into your existing watcher_splitter.py. The code looks good but needs some adjustments to properly integrate with the RAG system we created. ```python
import re
from rag_integration import ChromaRAG, extract_keywords
```

Let me read the current file to see its structure:


I see the file doesn't have the RAG imports yet.