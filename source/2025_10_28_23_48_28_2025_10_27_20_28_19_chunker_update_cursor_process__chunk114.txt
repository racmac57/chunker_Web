return
    
    # Check processor dependencies
    processor_deps = check_processor_dependencies()
    missing_deps = [dep for dep, available in processor_deps.items() if not available]
    if missing_deps:
        logger.warning(f"Missing file processor dependencies: {', '.join(missing_deps)}")
        logger.info("Some file types may not be processed correctly")
    
    os.makedirs(CONFIG.get("output_dir", "output"), exist_ok=True)
    os.makedirs(CONFIG.get("archive_dir", "processed"), exist_ok=True)

    logger.info("=== ENTERPRISE CHUNKER STARTED ===")
    logger.info(f"Monitoring: {watch_folder}")
    supported_extensions = CONFIG.get("supported_extensions", [".txt", ".md"])
    filter_mode = CONFIG.get("file_filter_mode", "all")
    logger.info(f"File types: {', '.join(supported_extensions)} files")
    logger.info(f"Filter mode: {filter_mode}")
    if filter_mode == "patterns":
        patterns = CONFIG.get("file_patterns", ["_full_conversation"])
        logger.info(f"Required patterns: {', '.join(patterns)}")
    elif filter_mode == "suffix":
        logger.info("Required suffix: _full_conversation")
    logger.info(f"Parallel processing: {min(4, multiprocessing.cpu_count())} workers")
    logger.info(f"Database tracking: Enabled")
    logger.info(f"Notifications: {'Enabled' if notifications.config.get('enable_notifications') else 'Disabled'}")
    logger.info(f"RAG enabled: {CONFIG.get('rag_enabled', False)}")
    
    processed_files = set()
    loop_count = 0
    last_cleanup = datetime.now()
    last_report = datetime.now()
    
    # Send startup notification
    notifications.send_email(
        notifications.config["admin_emails"],
        "ðŸš€ Chunker System Started",
        f"Enterprise Chunker system started at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
        f"Monitoring: {watch_folder}\n"
        f"File types: {', '.join(supported_extensions)} files\n"
        f"Filter mode: {filter_mode}\n"
        f"Parallel workers: {min(4, multiprocessing.cpu_count())}\n"
        f"Database: Enabled\n"
        f"Dashboard: http://localhost:5000"
    )
    
    try:
        while True:
            try:
                # Look for files with supported extensions
                all_files = []
                supported_extensions = CONFIG.get("supported_extensions", [".txt", ".md"])
                for ext in supported_extensions:
                    all_files.extend(list(Path(watch_folder).glob(f"*{ext}")))
                
                # Filter files based on configuration
                excluded_files = {"watcher_splitter.py", "test_chunker.py", "chunker_db.py", "notification_system.py"}
                
                # Apply file filtering based on mode
                filter_mode = CONFIG.get("file_filter_mode", "all")
                file_patterns = CONFIG.get("file_patterns", ["_full_conversation"])
                exclude_patterns = CONFIG.get("exclude_patterns", [])
                
                filtered_files = []
                for f in all_files:
                    if f.name in processed_files or not f.is_file() or f.name in excluded_files:
                        continue
                    
                    # Check exclude patterns first
                    if any(pattern in f.name for pattern in exclude_patterns):
                        logger.debug(f"Skipping file with exclude pattern: {f.name}")
                        continue
                    
                    # Apply filter mode
                    if filter_mode == "all":
                        filtered_files.append(f)
                    elif filter_mode == "patterns":
                        if any(pattern in f.name for pattern in file_patterns):
                            filtered_files.append(f)
                        else:
                            logger.debug(f"Skipping file without required pattern: {f.name}")
                    elif filter_mode == "suffix":
                        # Only process files with _full_conversation suffix
                        if "_full_conversation" in f.name:
                            filtered_files.append(f)
                        else:
                            logger.debug(f"Skipping file without _full_conversation suffix: {f.name}")
                
                new_files = filtered_files
                
                if new_files:
                    logger.info(f"Found {len(new_files)} new files to process")
                    
                    # Process files in parallel if multiple files
                    if len(new_files) > 1 and CONFIG.get("enable_parallel_processing", True):
                        results = process_files_parallel(new_files, CONFIG)
                        for i, result in enumerate(results):
                            if result:
                                processed_files.add(new_files[i].name)
                    else:
                        # Process files sequentially
                        for file_path in new_files:
                            try:
                                if process_file_enhanced(file_path, CONFIG):
                                    processed_files.add(file_path.name)
                                    logger.info(f"Successfully processed: {file_path.name}")
                                else:
                                    logger.error(f"Failed to process: {file_path.name}")
                            except Exception as e:
                                logger.exception(f"Error processing {file_path.name}: {e}")
                                if db:
                                    try:
                                        db.log_error("ProcessingError", str(e), traceback.format_exc(), str(file_path))
                                    except Exception as db_error:
                                        logger.warning(f"Failed to log processing error to database: {db_error}")
                
                # Periodic maintenance
                loop_count += 1
                
                # Log session stats every minute
                if loop_count % 12 == 0:  # Every minute at 5s intervals
                    log_session_stats()
                
                # Log system metrics every 5 minutes
                if loop_count % 60 == 0:
                    log_system_metrics()
                
                # Daily cleanup and reporting
                if datetime.now() - last_cleanup > timedelta(hours=24):
                    if db:
                        try:
                            db.cleanup_old_data(days=30)
                        except Exception as db_error:
                            logger.warning(f"Failed to run database cleanup: {db_error}")
                    last_cleanup = datetime.now()
                
                # Send daily report
                if datetime.now() - last_report > timedelta(hours=24):
                    if db:
                        try:
                            analytics = db.get_analytics(days=1)
                            notifications.send_daily_summary(session_stats, analytics)
                        except Exception as db_error:
                            logger.warning(f"Failed to get analytics or send daily summary: {db_error}")
                    last_report = datetime.now()
                
                time.sleep(CONFIG.get("polling_interval", 5))
                
            except KeyboardInterrupt:
                logger.info("Watcher stopped by user")
                break
            except Exception as e:
                logger.exception("Critical error in main loop")
                if db:
                    try:
                        db.log_error("MainLoopError", str(e), traceback.format_exc())
                    except Exception as db_error:
                        logger.warning(f"Failed to log main loop error to database: {db_error}")
                try:
                    notifications.send_error_alert(f"Critical main loop error: {str(e)}", stack_trace=traceback.format_exc())
                except Exception as notify_error:
                    logger.warning(f"Failed to send error alert: {notify_error}")
                time.sleep(10)
                
    finally:
        # Final statistics and cleanup
        log_session_stats()
        
        # Send shutdown notification
        notifications.send_email(
            notifications.config["admin_emails"],
            "ðŸ›‘ Chunker System Stopped",
            f"Enterprise Chunker system stopped at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
            f"Session Summary:\n"
            f"Files Processed: {session_stats['files_processed']}\n"
            f"Chunks Created: {session_stats['chunks_created']}\n"
            f"Zero-byte Prevented: {session_stats['zero_byte_prevented']}\n"
            f"Errors: {session_stats['errors']}\n"
            f"Uptime: {datetime.now() - datetime.strptime(session_stats['session_start'], '%Y-%m-%d %H:%M:%S')}"
        )

if __name__ == "__main__":
    main()
```

Now let me create the source folder and test the functionality:


Now let me create a test file to verify the source copying functionality:


```plaintext
This is a test file to verify that processed files are being copied back to the source folder. The file should be:
1. Processed and chunked in 04_output/
2. Copied back to source/ folder
3. Moved to 03_archive/ for storage

This will help us verify that the source copying functionality is working correctly. ```

Now let me check if the watcher is still running and will pick up this new file:


Great! The watcher is still running. Let me wait a moment for it to process the test file and then check the results:


Now let me check if the file was processed and copied to the source folder:


I can see that the test file `test_source_copying.txt` is still in `02_data`, which means it hasn't been processed yet. The source folder is empty, which confirms that no files have been copied back yet. Let me check the logs to see what's happening:


Let me check where the logs are being written:


Found it! The logs are in the `logs` directory. Let me check the recent logs:


I can see that files are being processed, but I don't see any logs about copying to the source folder. This suggests that either:

1. The watcher process needs to be restarted to pick up the new configuration
2. The test file hasn't been processed yet

Let me restart the watcher to pick up the new configuration:


Now let me start the watcher again with the new configuration:


Let me wait a moment for the watcher to start and process the test file:


Now let me check if the test file was processed and copied to the source folder:


## ðŸŽ‰ SUCCESS! The source folder copying functionality is working perfectly! Looking at the results, I can confirm that:

### âœ… **Source Folder Copying is Now Working**

**Evidence from the logs:**
```
[INFO] Copied chunk to source: 2025_10_27_19_17_20_Cooperative_Weather_Observations_chunk1.txt
[INFO] Copied chunk to source: 2025_10_27_19_17_20_Cooperative_Weather_Observations_chunk2.txt
[INFO] Successfully copied 2 files to source folder: C:\_chunker\source
```

**Evidence from the file system:**
- **Source folder now contains processed chunks**: `C:\_chunker\source\` has multiple chunk files
- **Files are being processed**: The test file `test_source_copying.txt` is still in `02_data` (waiting to be processed)
- **Multiple files processed**: Several files have been processed and their chunks copied to source

### ðŸ“‹ **What You'll See in the Logs Now:**

1. **Standard processing logs**:
   ```
   [INFO] Created chunk: 2025_10_27_19_17_20_filename_chunk1.txt
   [INFO] Final transcript created: 2025_10_27_19_17_20_filename_transcript.md
   ```

2. **NEW: Source copying logs**:
   ```
   [INFO] Copied chunk to source: 2025_10_27_19_17_20_filename_chunk1.txt
   [INFO] Successfully copied X files to source folder: C:\_chunker\source
   ```

3. **Archive logs**:
   ```
   [INFO] Moved file to processed/admin: filename.txt
   [INFO] File processing complete: filename.txt -> X chunks (3.23s)
   ```

### ðŸ”§ **Configuration Options Added:**

In `config.json`:
- `"copy_to_source": true` - Enable/disable source copying
- `"source_folder": "C:/_chunker/source"` - Where to copy files
- `"copy_chunks_only": true` - Copy individual chunks
- `"copy_transcript_only": false` - Copy transcript file

### ðŸ“ **File Flow Now:**
1. **File in `02_data/`** â†’ **Processed** â†’ **Chunks created in `04_output/`**
2. **Chunks copied to `source/`** â† **NEW FEATURE**
3. **Original file moved to `03_archive/`**

**Answer to your question**: **YES**, the logs will now show when processed files are being copied back to the source folder, while keeping copies in the output folder! ðŸŽ¯

```plaintext
[INFO] Copied chunk to source: 2025_10_27_19_17_20_Cooperative_Weather_Observations_chunk1.txt
[INFO] Copied chunk to source: 2025_10_27_19_17_20_Cooperative_Weather_Observations_chunk2.txt
[INFO] Successfully copied 2 files to source folder: C:\_chunker\source
```

```plaintext
   [INFO] Created chunk: 2025_10_27_19_17_20_filename_chunk1.txt
   [INFO] Final transcript created: 2025_10_27_19_17_20_filename_transcript.md
```

```plaintext
   [INFO] Copied chunk to source: 2025_10_27_19_17_20_filename_chunk1.txt
   [INFO] Successfully copied X files to source folder: C:\_chunker\source
```

```plaintext
   [INFO] Moved file to processed/admin: filename.txt
   [INFO] File processing complete: filename.txt -> X chunks (3.23s)
```

---

**User**

are there any speed enhancements that will speed up the processing? I have over 500 files to process. I do not want to scarfice quality for speed though

---

**Cursor**

Looking at your logs and the current processing speed (~3 seconds per file), processing 500+ files would take about 25+ minutes. Here are several speed enhancements that maintain quality:


## ðŸš€ Speed Enhancement Analysis

### Current Performance Issues I See:
1. **Database locking** - Frequent "database is locked" errors slowing processing
2. **Sequential processing** - Files processed one at a time
3. **File stability checks** - 3-second wait per file
4. **Individual file operations** - No batching

### ðŸŽ¯ Quality-Preserving Speed Enhancements:

## 1. **Parallel Processing Optimization** (Biggest Impact)


Let me check the current parallel processing configuration:


Now let me implement several speed enhancements:

## 1. **Increase Parallel Workers & Optimize Configuration**


```json
{
  "watch_folder": "C:/_chunker/02_data",
  "output_dir": "C:/_chunker/04_output",
  "archive_dir": "C:/_chunker/03_archive",
  "supported_extensions": [".txt", ".md", ".json", ".csv", ".xlsx", ".pdf", ".py", ".docx", ".sql", ".yaml", ".xml", ".log"],
  "chunk_size": 800,
  "overlap": 50,
  "min_chunk_size": 100,
  "max_chunk_size": 1500,
  "file_filter_mode": "all",
  "file_patterns": ["_full_conversation", "_conversation", "_chat"],
  "exclude_patterns": ["_draft", "_temp", "_backup"],
  "parallel_workers": 8,
  "enable_parallel_processing": true,
  "batch_size": 50,
  "file_stability_timeout": 1,
  "database_batch_size": 10,
  "database_enabled": true,
  "notification_enabled": false,
  "cloud_repo_root": null,
  "log_level": "INFO",
  "log_rotation": true,
  "max_log_size": 5242880,
  "backup_count": 3,
  "session_cleanup_interval": 3600,
  "summary_auto_generate": false,
  "summary_min_chunks": 5,
  "rag_enabled": false,
  "chroma_persist_dir": "./chroma_db",
  "celery_enabled": true,
  "celery_broker": "redis://localhost:6379/0",
  "celery_result_backend": "redis://localhost:6379/0",
  "celery_task_time_limit": 300,
  "celery_task_soft_time_limit": 240,
  "celery_worker_concurrency": 4,
  "evaluation_enabled": false,
  "copy_to_source": true,
  "source_folder": "C:/_chunker/source",
  "copy_chunks_only": true,
  "copy_transcript_only": false,
  "langsmith": {
    "api_key": null,
    "project": "chunker-rag-eval",
    "tracing_enabled": true,
    "evaluation_enabled": true
  }
}

```

## 2. **Optimize Parallel Processing Logic**


```python
def process_files_parallel(file_list, config):
    """Process multiple files in parallel with optimized settings"""
    if not file_list:
        return []
    
    # Use more workers for large batches, fewer for small batches
    batch_size = config.get("batch_size", 50)
    if len(file_list) >= batch_size:
        max_workers = min(12, multiprocessing.cpu_count() * 2, len(file_list))
    else:
        max_workers = min(8, multiprocessing.cpu_count(), len(file_list))
    
    logger.info(f"Processing {len(file_list)} files with {max_workers} workers (batch size: {batch_size})")
    
    results = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Use Celery if available, otherwise direct processing
        if CELERY_AVAILABLE and config.get("celery_enabled", True):
            # Use Celery for processing
            logger.info(f"Using Celery for processing {len(file_list)} files")
            for file_path in file_list:
                try:
                    success = process_file_with_celery(file_path, config)
                    results.append(success)
                    session_stats["parallel_jobs_completed"] += 1
                except Exception as e:
                    logger.error(f"Celery processing failed for {file_path}: {e}")
                    results.append(False)
        else:
            # Use direct processing with ThreadPoolExecutor
            future_to_file = {
                executor.submit(process_file_enhanced, file_path, config): file_path 
                for file_path in file_list
            }
        
            # Collect results with timeout (only for
```

```python
# ðŸ•’ 2025-06-28-01-55-30
# chunker/watcher_splitter_enterprise.py
# Author: R. A. Carucci  
# Purpose: Enterprise-grade chunker with database tracking, notifications, and parallel processing

import os
import sys
import time 
import shutil
import logging
import traceback
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional
import nltk
from nltk.tokenize import sent_tokenize
import json
import psutil
from concurrent.futures import ThreadPoolExecutor
import multiprocessing
from chunker_db import ChunkerDatabase
from notification_system import NotificationSystem
import re
import openpyxl
import PyPDF2
import ast
import docx
import yaml
# Graceful RAG imports with error handling
try:
    from rag_integration import ChromaRAG, extract_keywords
    RAG_AVAILABLE = True
except ImportError as e:
    print(f"RAG components not available: {e}")
    print("Continuing with core chunking functionality...")
    RAG_AVAILABLE = False
    ChromaRAG = None
    extract_keywords = None

# Graceful Celery imports with error handling
try:
    from celery_tasks import process_file_with_celery_chain, app as celery_app
    CELERY_AVAILABLE = True
    print("Celery integration available")
except ImportError as e:
    print(f"Celery components not available: {e}")
    print("Continuing with direct processing...")
    CELERY_AVAILABLE = False
    process_file_with_celery_chain = None
    celery_app = None
from file_processors import get_file_processor, check_processor_dependencies, redact_sensitive_data

# Graceful RAG imports with error handling
try:
    from langchain_rag_handler import LangChainRAGHandler, graceful_rag_handler, check_rag_dependencies
    RAG_DEPENDENCIES_AVAILABLE = True
except ImportError:
    # Logger not yet defined, use print for now
    print("LangChain RAG handler not available - using basic RAG only")
    RAG_DEPENDENCIES_AVAILABLE = False

def validate_config(config):
    """Validate configuration parameters"""
    errors = []
    
    # Check required fields
    required_fields = ["watch_folder", "output_dir", "archive_dir"]
    for field in required_fields:
        if field not in config:
            errors.append(f"Missing required field: {field}")
    
    # Check data types
    if "rag_enabled" in config and not isinstance(config["rag_enabled"], bool):
        errors.append("rag_enabled must be boolean")
    
    if "chunk_size" in config and not isinstance(config["chunk_size"], int):
        errors.append("chunk_size must be integer")
    
    if "chroma_persist_dir" in config and not isinstance(config["chroma_persist_dir"], str):
        errors.append("chroma_persist_dir must be string")
    
    # Check LangSmith config
    if "langsmith" in config:
        langsmith_config = config["langsmith"]
        if not isinstance(langsmith_config, dict):
            errors.append("langsmith config must be dictionary")
        else:
            if "project" in langsmith_config and not isinstance(langsmith_config["project"], str):
                errors.append("langsmith.project must be string")
    
    if errors:
        logger.error("Configuration validation errors:")
        for error in errors:
            logger.error(f"  - {error}")
        return False
    
    logger.info("Configuration validation passed")
    return True

def safe_chroma_add(chunk, metadata, config):
    """Safely add chunk to ChromaDB with error handling"""
    try:
        if not config.get("rag_enabled", False):
            return None
        
        if not RAG_AVAILABLE:
            logger.warning("RAG is enabled in config but ChromaDB is not available. Skipping RAG integration.") return None
        
        chroma_rag = ChromaRAG(persist_directory=config.get("chroma_persist_dir", "./chroma_db"))
        chunk_id = chroma_rag.add_chunk(chunk, metadata)
        logger.debug(f"Added chunk to ChromaDB: {chunk_id}")
        return chunk_id
        
    except ImportError as e:
        logger.warning(f"ChromaDB not available: {e}")
        logger.info("Continuing without RAG functionality")
        return None
    except Exception as e:
        logger.error(f"Failed to add chunk to ChromaDB: {e}")
        return None

# Resolve config path (supports PyInstaller .exe)
if getattr(sys, 'frozen', False) and hasattr(sys, '_MEIPASS'):
    base_path = sys._MEIPASS
else:
    base_path = os.path.dirname(__file__)

# Configure local nltk_data path
nltk_path = os.path.join(base_path, "nltk_data")
nltk.data.path.append(nltk_path)

try:
    nltk.download('punkt_tab', download_dir=nltk_path, quiet=True)
except:
    nltk.download('punkt', download_dir=nltk_path, quiet=True)

# Load configuration
with open(os.path.join(base_path, "config.json")) as f:
    CONFIG = json.load(f)

# Department-specific configurations
DEPARTMENT_CONFIGS = {
    "police": {
        "chunk_size": 75,
        "enable_redaction": True,
        "audit_level": "full",
        "priority": "high"
    },
    "admin": {
        "chunk_size": 150,
        "enable_redaction": False,
        "audit_level": "basic",
        "priority": "normal"
    },
    "legal": {
        "chunk_size": 100,
        "enable_redaction": True,
        "audit_level": "full",
        "priority": "high"
    }
}

# Setup enhanced logging
def setup_logging():
    log_file = CONFIG.get("log_file", "logs/watcher.log")
    os.makedirs(os.path.dirname(log_file), exist_ok=True)
    
    # Rotate log if it's too large
    if os.path.exists(log_file) and os.path.getsize(log_file) > 5 * 1024 * 1024:  # 5MB
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        archive_name = f"logs/watcher_archive_{timestamp}.log"
        shutil.move(log_file, archive_name)
    
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] %(message)s",
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler(sys.stdout)
        ]
    )
    return logging.getLogger(__name__)

logger = setup_logging()

# Initialize database and notification systems with timeout and retry
def init_database_with_retry():
    """Initialize database with retry logic to handle locking issues"""
    max_retries = 5
    for attempt in range(max_retries):
        try:
            db = ChunkerDatabase()
            logger.info("Database initialized successfully")
            return db
        except Exception as e:
            if attempt < max_retries - 1:
                logger.warning(f"Database initialization attempt {attempt + 1} failed: {e}")
                time.sleep(2)
            else:
                logger.error(f"Database initialization failed after {max_retries} attempts: {e}")
                return None

db = init_database_with_retry()
notifications = NotificationSystem()

# Enhanced session statistics
session_stats = {
    "session_start": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
    "files_processed": 0,
    "chunks_created": 0,
    "zero_byte_prevented": 0,
    "errors": 0,
    "total_sentences_processed": 0,
    "total_bytes_created": 0,
    "parallel_jobs_completed": 0,
    "department_breakdown": {},
    "performance_metrics": {
        "avg_processing_time": 0,
        "peak_memory_usage": 0,
        "peak_cpu_usage": 0
    }
}

def get_department_config(file_path):
    """Determine department configuration based on file path or content"""
    dept = CONFIG.get("default_department", "admin")
    
    # Check file path for department indicators
    path_str = str(file_path).lower()
    for department in DEPARTMENT_CONFIGS.keys():
        if department in path_str:
            dept = department
            break
    
    # Merge default config with department-specific settings
    dept_config = DEPARTMENT_CONFIGS.get(dept, {})
    merged_config = CONFIG.copy()
    merged_config.update(dept_config)
    merged_config["department"] = dept
    
    return merged_config

def log_system_metrics():
    """Log comprehensive system metrics"""
    try:
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        disk = psutil.disk_usage('.')