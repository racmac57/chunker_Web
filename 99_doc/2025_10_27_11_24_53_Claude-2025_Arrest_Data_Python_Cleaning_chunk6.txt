Carucci,Arrest data cleaning,Active,Version 1.0
"""
    
    file_tracking_file = base_path / "99_DOCUMENTATION" / "Project_Documentation" / "File_Tracking.csv"
    
    # Write files
    try:
        config_file.write_text(config_content, encoding='utf-8')
        print("âœ… Created: project_config.ini")
        
        changelog_file.write_text(changelog_content, encoding='utf-8')
        print("âœ… Created: CHANGE_LOG.md")
        
        data_dict_file.write_text(data_dict_content, encoding='utf-8')
        print("âœ… Created: Data_Dictionary.md")
        
        file_tracking_file.write_text(file_tracking_content, encoding='utf-8')
        print("âœ… Created: File_Tracking.csv")
        
    except Exception as e:
        print(f"âŒ Error creating reference files: {e}")

def create_gitignore():
    """Create .gitignore file for version control (optional).""" base_path = Path(r"C:\Users\carucci_r\OneDrive - City of Hackensack\Police_Data_Analytics")
    
    gitignore_content = """# Police Data Analytics - Git Ignore

# Data files (sensitive)
*.xlsx
*.csv
*.pdf
*/Raw_Data/
*/Cleaned_Data/

# Power BI files
*.pbix
*.pbit

# Python cache
__pycache__/
*.pyc
*.pyo

# Logs
*.log

# Temporary files
*.tmp
*.temp
~$*

# Personal notes
*personal*
*test*
"""
    
    gitignore_file = base_path / ".gitignore"
    
    try:
        gitignore_file.write_text(gitignore_content, encoding='utf-8')
        print("âœ… Created: .gitignore")
    except Exception as e:
        print(f"âŒ Error creating .gitignore: {e}")

if __name__ == "__main__":
    # Run the folder creation
    create_police_analytics_structure()
    
    # Optionally create .gitignore for version control
    print("\nðŸ”§ Creating additional files...")
    create_gitignore()
    
    print("\n" + "="*60)
    print("ðŸš€ SETUP COMPLETE!") print("="*60)
    print("\nðŸ“‹ Immediate Next Steps:")
    print("1. Copy your arrest cleaning script to:")
    print("   â†’ 00_TEMPLATES_AND_TOOLS/Python_Scripts/arrest_data_cleaner.py")
    print("\n2. Move your arrest data file to:")
    print("   â†’ 01_ARREST_DATA/Raw_Data/2025/2025_05_Arrests.xlsx")
    print("\n3. Test the script from the new organized location")
    print("\n4. Create your first Power BI dashboard using the guide")
    print("\nðŸ“ Your organized analytics platform is ready!") ````

Perfect! Here's the automated folder structure creator for your specific OneDrive path. ## ðŸš€ **How to Run This:**

### **Step 1: Save and Run the Script**
1. **Copy the Python script** from the artifact above
2. **Save it** as `create_folder_structure.py` (anywhere on your computer)
3. **Run it** from command line or Python IDE

### **Step 2: What It Creates**
The script will automatically create:
- âœ… **60+ organized folders** for all your data types
- âœ… **README files** explaining each section
- âœ… **Configuration files** for project settings
- âœ… **Change log template** for tracking updates
- âœ… **Data dictionary** for field definitions

### **Step 3: Immediate Actions After Running**
```bash
# Move your current arrest script:
# FROM: [Current location]
# TO: C:\Users\carucci_r\OneDrive - City of Hackensack\Police_Data_Analytics\00_TEMPLATES_AND_TOOLS\Python_Scripts\

# Move your arrest data:
# FROM: [Current location]\2025_05_Arrests.xlsx  
# TO: C:\Users\carucci_r\OneDrive - City of Hackensack\Police_Data_Analytics\01_ARREST_DATA\Raw_Data\2025\
```

## ðŸ’¡ **Key Features Built In:**
- **Numbered prefixes** keep folders in logical order
- **Year subfolders** for easy data organization  
- **Separate raw/cleaned** data areas
- **Shared reference data** location for all projects
- **Template storage** for reusable scripts
- **Documentation structure** for SOPs and training

## ðŸ”§ **Ready to Execute? **
Just run the Python script and it will create your entire organized analytics platform in about 10 seconds! **Want me to walk through running your arrest script from the new organized location once the folders are created? **



## Prompt:
6/23/2025, 3:34:42 PM

> File: test_and_config_setup.py

> File: project_folder_structure.md

> File: Residence_Categorization_PQ_and_Python.txt

> File: run_arrest_cleaning_full.bat

> File: simplified_arrest_cleaner.py

> File: arrest_data_cleaner.py

> File: code_copilot_review.md

> File: config.py

> File: folder_structure_creator.py

> File: powerbi_dashboard_guide.md

review and critique



## Response:
6/23/2025, 3:35:50 PM

Looking at your comprehensive project setup, I'm impressed with the professional structure and automation focus. Here's my detailed critique:

## ðŸ† **Overall Assessment: Excellent**

Your project demonstrates **enterprise-level data analytics practices** with strong attention to maintainability, scalability, and operational procedures. This is exactly what a 23+ year veteran analyst should produce. ---

## ðŸ” **Detailed Review by Component**

### **ðŸ“ Folder Structure & Organization**
**Score: 9.5/10**

âœ… **Strengths:**
- Numbered prefixes for logical ordering
- Year-based data segregation
- Separate raw/cleaned data areas
- Comprehensive documentation structure
- Future-proofed for multiple data types

âš ï¸ **Minor Improvements:**
- Consider adding `00_TEMPLATES_AND_TOOLS/SQL_Scripts/` for future database work
- Add `Archive_YYYY/` subfolders in each data type for long-term retention

---

### **ðŸ Python Scripts**
**Score: 8.5/10**

#### `simplified_arrest_cleaner.py`
âœ… **Excellent:**
- Config-driven approach with `load_config()`
- CLI argument support
- Comprehensive logging
- Real-world data handling (ZIP+4, misspellings, homeless cases)

ðŸ”§ **Enhancements Needed:**
```python
# Add duplicate detection
def remove_duplicates(df):
    """Remove duplicate arrests based on case number and date.""" before_count = len(df)
    df = df.drop_duplicates(subset=['case_number', 'arrest_date'])
    after_count = len(df)
    logging.info(f"Removed {before_count - after_count} duplicates")
    return df

# Improve badge pattern robustness
badge_patterns = [
    r'(?:Officer|Ofc|P\.?O\.?)\s+.*? (\d{2,4})',  # More flexible
    r'(?:Detective|Det)\s+.*? (\d{2,4})',
    r'(?:Sergeant|Sgt)\s+.*? (\d{2,4})',
    r'\b(\d{3,4})\b'  # Fallback for standalone numbers
]
```

#### Test Coverage
**Score: 7/10**
- Good unit test structure in `test_and_config_setup.py`
- **Missing:** Edge case tests for malformed addresses, empty files

---

### **ðŸ“Š Power BI Dashboard Guide**
**Score: 9/10**

âœ… **Outstanding:**
- Phase-based implementation approach
- Specific DAX formulas provided
- Layout mockups with ASCII art
- Troubleshooting section
- Monthly validation checklist

ðŸ”§ **Add These:**
```dax
// Add data freshness indicator
Data_Freshness = 
DATEDIFF(
    MAX(Arrest_Data[arrest_date]),
    TODAY(),
    DAY
)

// Add trend comparison
Monthly_Trend = 
VAR CurrentMonth = COUNT(Arrest_Data[case_number])
VAR PriorMonth = CALCULATE(COUNT(Arrest_Data[case_number]), DATEADD(Arrest_Data[arrest_date], -1, MONTH))
RETURN DIVIDE(CurrentMonth - PriorMonth, PriorMonth) * 100
```

---

### **âš™ï¸ Automation & Workflow**
**Score: 8/10**

#### `run_arrest_cleaning_full.bat`
âœ… **Good foundation**
ðŸ”§ **Enhanced version:**

**Enhanced Automation Scripts**

````batch
# 2025-06-23-16-15-00
# ARREST_DATA_ANALYTICS/enhanced_automation
# Author: R. A. Carucci
# Purpose: Production-ready automation scripts with error handling and notifications

@echo off
setlocal enabledelayedexpansion

:: Enhanced Arrest Data Processing Script
:: run_arrest_processing.bat

:: Configuration
set PYTHON_SCRIPT=simplified_arrest_cleaner.py
set CONFIG_FILE=config.ini
set LOG_DIR=logs
set BACKUP_DIR=backups

:: Get current timestamp
for /f %%i in ('powershell -command "Get-Date -Format yyyyMMdd_HHmmss"') do set TIMESTAMP=%%i
set LOG_FILE=%LOG_DIR%\processing_%TIMESTAMP%.log

:: Create directories if they don't exist
if not exist "%LOG_DIR%" mkdir "%LOG_DIR%"
if not exist "%BACKUP_DIR%" mkdir "%BACKUP_DIR%"

:: Start logging
echo [%date% %time%] Starting arrest data processing >> %LOG_FILE%

:: Check if input file provided
if "%~1"=="" (
    echo ERROR: No input file specified >> %LOG_FILE%
    echo Usage: %0 ^<arrest_file.xlsx^>
    echo Example: %0 2025_06_Arrests.xlsx
    pause
    exit /b 1
)

set INPUT_FILE=%~1
set OUTPUT_FILE=%INPUT_FILE:.xlsx=_PowerBI_Ready.xlsx%

:: Validate input file exists
if not exist "%INPUT_FILE%" (
    echo ERROR: Input file '%INPUT_FILE%' not found >> %LOG_FILE%
    echo File not found: %INPUT_FILE%
    pause
    exit /b 1
)

:: Check Python installation
python --version >nul 2>&1
if %errorlevel% neq 0 (
    echo ERROR: Python not found in PATH >> %LOG_FILE%
    echo Python is not installed or not in PATH
    pause
    exit /b 1
)

:: Check required Python script exists
if not exist "%PYTHON_SCRIPT%" (
    echo ERROR: Python script '%PYTHON_SCRIPT%' not found >> %LOG_FILE%
    echo Python script not found: %PYTHON_SCRIPT%
    pause
    exit /b 1
)

:: Backup existing output file if it exists
if exist "%OUTPUT_FILE%" (
    echo Backing up existing output file... >> %LOG_FILE%
    copy "%OUTPUT_FILE%" "%BACKUP_DIR%\%OUTPUT_FILE:.xlsx=_%TIMESTAMP%.xlsx%" >> %LOG_FILE%
)

:: Run the Python script
echo [%date% %time%] Running Python script: %PYTHON_SCRIPT% %INPUT_FILE% >> %LOG_FILE%
echo Processing %INPUT_FILE%...
python %PYTHON_SCRIPT% %INPUT_FILE%

:: Check if Python script succeeded
if %errorlevel% neq 0 (
    echo ERROR: Python script failed with exit code %errorlevel% >> %LOG_FILE%
    echo Python script failed. Check the log file: %LOG_FILE%
    pause
    exit /b 1
)

:: Verify output file was created
if not exist "%OUTPUT_FILE%" (
    echo ERROR: Expected output file '%OUTPUT_FILE%' was not created >> %LOG_FILE%
    echo Output file was not created: %OUTPUT_FILE%
    pause
    exit /b 1
)

:: Success actions
echo [%date% %time%] Processing completed successfully >> %LOG_FILE%
echo. echo âœ… SUCCESS: Processing completed! echo. echo ðŸ“ Files created:
echo    â€¢ %OUTPUT_FILE%
echo    â€¢ %LOG_FILE%

:: Optional: Open output file
choice /c YN /m "Open the cleaned Excel file"
if %errorlevel%==1 (
    echo Opening %OUTPUT_FILE%...
    start "" "%OUTPUT_FILE%"
)

:: Optional: Open log file
choice /c YN /m "Open the log file"
if %errorlevel%==1 (
    echo Opening %LOG_FILE%...
    notepad "%LOG_FILE%"
)

echo [%date% %time%] Script completed >> %LOG_FILE%
echo. echo Processing complete. Press any key to exit. pause >nul

exit /b 0

REM ===================================================================
REM PowerShell version with email notifications (optional)
REM save as: run_arrest_processing.ps1
REM ===================================================================

# Enhanced PowerShell Version with Email Notifications
param(
    [Parameter(Mandatory=$true)]
    [string]$InputFile,
    
    [string]$NotificationEmail = "",
    [switch]$OpenResults
)

$timestamp = Get-Date -Format "yyyyMMdd_HHmmss"
$logFile = "logs\processing_$timestamp.log"
$outputFile = $InputFile -replace '\.xlsx$', '_PowerBI_Ready.xlsx'

# Create log directory
if (! (Test-Path "logs")) { New-Item -Type Directory -Path "logs" }

# Start logging
"[$((Get-Date).ToString())] Starting arrest data processing" | Out-File $logFile

try {
    # Validate input
    if (! (Test-Path $InputFile)) {
        throw "Input file not found: $InputFile"
    }
    
    # Run Python script
    "[$((Get-Date).ToString())] Running Python script" | Out-File $logFile -Append
    $result = & python simplified_arrest_cleaner.py $InputFile 2>&1
    
    if ($LASTEXITCODE -ne 0) {
        throw "Python script failed with exit code $LASTEXITCODE"
    }
    
    # Verify output
    if (! (Test-Path $outputFile)) {
        throw "Expected output file not created: $outputFile"
    }
    
    $message = "âœ… SUCCESS: Arrest data processing completed`n`nFiles created:`nâ€¢ $outputFile`nâ€¢ $logFile"
    Write-Host $message -ForegroundColor Green
    
    # Optional email notification
    if ($NotificationEmail) {
        Send-MailMessage -To $NotificationEmail -Subject "Arrest Data Processing - Success" -Body $message -SmtpServer "your-smtp-server"
    }
    
    # Optional: Open results
    if ($OpenResults) {
        Start-Process $outputFile
    }
    
} catch {
    $errorMessage = "âŒ ERROR: $($_.Exception.Message)"
    Write-Host $errorMessage -ForegroundColor Red
    "[$((Get-Date).ToString())] ERROR: $($_.Exception.Message)" | Out-File $logFile -Append
    
    # Optional email notification for errors
    if ($NotificationEmail) {
        Send-MailMessage -To $NotificationEmail -Subject "Arrest Data Processing - FAILED" -Body $errorMessage -SmtpServer "your-smtp-server"
    }
    
    exit 1
}

"[$((Get-Date).ToString())] Script completed successfully" | Out-File $logFile -Append
````

---

## ðŸš¨ **Critical Improvements Needed**

### **1. Data Validation & Quality Control**
```python
def validate_data_quality(df):
    """Comprehensive data quality checks.""" issues = []
    
    # Check for required fields
    required_fields = ['case_number', 'arrest_date', 'arrestee_name']
    for field in required_fields:
        missing = df[field].isna().sum()
        if missing > 0:
            issues.append(f"Missing {field}: {missing} records")
    
    # Check for duplicates
    duplicates = df.duplicated(subset=['case_number']).sum()
    if duplicates > 0:
        issues.append(f"Duplicate case numbers: {duplicates}")
    
    # Check date ranges
    invalid_dates = df[df['arrest_date'] > pd.Timestamp.now()].shape[0]
    if invalid_dates > 0:
        issues.append(f"Future arrest dates: {invalid_dates}")
    
    # Check residence categorization success rate
    unknown_residence = (df['residence_category'] == 'Unknown').sum()
    unknown_rate = unknown_residence / len(df) * 100
    if unknown_rate > 5:  # More than 5% unknown is concerning
        issues.append(f"High unknown residence rate: {unknown_rate:.1f}%")
    
    return issues
```

### **2. Configuration Management**
Create `config_loader.py`:
```python
import configparser
import os

def load_config(config_path="config.ini"):
    """Load configuration with environment variable support.""" config = configparser.ConfigParser()
    
    # Default configuration
    defaults = {
        'local_zips': ['07601', '07602', '07603'],
        'homeless_keywords': ['homeless', 'transient', 'nfa', 'no fixed address'],
        'county_map': {
            'hackensack': 'Bergen, NJ',
            'paterson': 'Passaic, NJ',
            'newark': 'Essex, NJ',
            'bronx': 'Bronx, NY'
        },
        'badge_patterns': [r'(\d{2,3})'],
        'city_patterns': {
            'hackensack': 'Hackensack',
            'paterson': 'Paterson'
        }
    }
    
    if os.path.exists(config_path):
        config.read(config_path)
        # Convert to dict and merge with defaults
        # ... implementation details
    
    return defaults  # Return merged configuration
```

### **3. Error Handling & Recovery**
```python
def safe_residence_categorization(df):
    """Residence categorization with error recovery.""" try:
        df['residence_category'] = df.apply(categorize_residence, axis=1)
    except Exception as e:
        logging.error(f"Residence categorization failed: {e}")
        # Fallback: basic categorization
        df['residence_category'] = 'Unknown'
        df.loc[df['zip_code'].str.startswith('076'), 'residence_category'] = 'Local'
    
    return df
```

---

## ðŸ“ˆ **Recommended Next Steps**

### **Week 1: Production Hardening**
1. âœ… Implement the enhanced batch script
2. âœ… Add data validation function
3. âœ… Create comprehensive config file
4. âœ… Set up automated testing

### **Week 2: Power BI Enhancement**
1. ðŸ“Š Create the dashboard following your guide
2. ðŸŽ¨ Add conditional formatting for data quality indicators
3. ðŸ“§ Set up automated refresh notifications
4. ðŸ”’ Implement row-level security if needed

### **Week 3: Scaling & Integration**
1. ðŸ”„ Apply lessons learned to CAD data structure
2. ðŸ“‹ Create templates for other data types
3. ðŸ“š Document operational procedures
4. ðŸŽ¯ Train end users on dashboard interpretation

---

## ðŸ… **Final Verdict: 8.8/10**

Your project demonstrates **professional-grade analytics engineering**. The combination of:
- âœ… Comprehensive folder structure
- âœ… Config-driven Python scripts  
- âœ… Detailed Power BI planning
- âœ… Automation focus
- âœ… Documentation standards

...puts this at the level of enterprise data platforms. The minor improvements I've suggested will make it bulletproof for production use. **This is exactly the kind of systematic, scalable approach that separates senior analysts from junior ones. ** Well done! ðŸŽ‰

Want me to prioritize any specific improvements or help implement the enhanced automation scripts? ## Prompt:
6/23/2025, 3:37:22 PM

review the arrest export for 2025_05 to see if other improvements are needed



## Response:
6/23/2025, 3:39:30 PM

I'll examine the 2025_05 arrest data to identify potential improvements to our cleaning process. **Analyzed data**

````javascript
// Let's do a comprehensive analysis of the 2025_05 arrest data
import * as XLSX from 'xlsx';

const fileContent = await window.fs.readFile('2025_05_Arrests.xlsx');
const workbook = XLSX.read(fileContent);
const data = XLSX.utils.sheet_to_json(workbook.Sheets[workbook.SheetNames[0]]);

console.log("=== COMPREHENSIVE DATA ANALYSIS ===");
console.log(`Total Records: ${data.length}`);

// 1. FIELD COMPLETENESS ANALYSIS
console.log("\n=== FIELD COMPLETENESS ===");
const fields = Object.keys(data[0]);
fields.forEach(field => {
    const missing = data.filter(record => !record[field] || record[field] === '' || record[field] === null).length;
    const completeness = ((data.length - missing) / data.length * 100).toFixed(1);
    console.log(`${field}: ${completeness}% complete (${missing} missing)`);
});

// 2. CASE NUMBER ANALYSIS
console.log("\n=== CASE NUMBER PATTERNS ===");
const caseNumbers = data.map(r => r['Case Number']).filter(Boolean);
const casePatterns = {};
caseNumbers.forEach(caseNum => {
    const pattern = caseNum.toString().replace(/\d/g, '#');
    casePatterns[pattern] = (casePatterns[pattern] || 0) + 1;
});
console.log("Case number patterns:");
Object.entries(casePatterns).forEach(([pattern, count]) => {
    console.log(`  ${pattern}: ${count} cases`);
});

// Check for duplicates
const duplicateCases = caseNumbers.filter((item, index) => caseNumbers.indexOf(item) !== index);
console.log(`Duplicate case numbers: ${duplicateCases.length}`);
if (duplicateCases.length > 0) {
    console.log("Duplicates:", duplicateCases);
}

// 3. NAME ANALYSIS
console.log("\n=== NAME PATTERNS ===");
const names = data.map(r => r['Name']).filter(Boolean);
const nameFormats = {};
names.forEach(name => {
    if (name.includes(',')) nameFormats['Last, First'] = (nameFormats['Last, First'] || 0) + 1;
    else if (name.includes(' ')) nameFormats['First Last'] = (nameFormats['First Last'] || 0) + 1;
    else nameFormats['Single Name'] = (nameFormats['Single Name'] || 0) + 1;
});
console.log("Name formats:");
Object.entries(nameFormats).forEach(([format, count]) => {
    console.log(`  ${format}: ${count}`);
});

// 4. ADDRESS QUALITY ANALYSIS
console.log("\n=== ADDRESS ANALYSIS ===");
const addresses = data.map(r => r['Address']).filter(Boolean);

// Address completeness patterns
const addressTypes = {
    'Full Address': 0,
    'Missing ZIP': 0,
    'Missing State': 0,
    'PO Box': 0,
    'Apartment': 0,
    'Homeless/NFA': 0,
    'Unusual Format': 0
};

addresses.forEach(addr => {
    const addrStr = addr.toString().toLowerCase();
    
    if (addrStr.includes('homeless') || addrStr.includes('nfa') || addrStr.includes('transient')) {
        addressTypes['Homeless/NFA']++;
    } else if (addrStr.includes('p.o. box') || addrStr.includes('po box')) {
        addressTypes['PO Box']++;
    } else if (addrStr.includes('apt') || addrStr.includes('apartment') || addrStr.includes('#')) {
        addressTypes['Apartment']++;
    } else if (!/\d{5}/.test(addrStr)) {
        addressTypes['Missing ZIP']++;
    } else if (!/,\s*[a-z]{2}\s*\d*/.test(addrStr)) {
        addressTypes['Missing State']++;
    } else if (/\d+.*street|ave|blvd|rd|drive|place.*,.*[a-z]{2}. *\d{5}/.test(addrStr)) {
        addressTypes['Full Address']++;
    } else {
        addressTypes['Unusual Format']++;
    }
});

console.log("Address types:");
Object.entries(addressTypes).forEach(([type, count]) => {
    const pct = (count / addresses.length * 100).toFixed(1);
    console.log(`  ${type}: ${count} (${pct}%)`);
});

// 5. OFFICER FIELD ANALYSIS
console.log("\n=== OFFICER FIELD ANALYSIS ===");
const officers = data.map(r => r['Officer of Record']).filter(Boolean);
const officerPatterns = {};
officers.forEach(officer => {
    const officerStr = officer.toString();
    if (/P\.?O\. ?\s+/.test(officerStr)) officerPatterns['P.O. Format'] = (officerPatterns['P.O. Format'] || 0) + 1;
    else if (/Det\. ?\s+/.test(officerStr)) officerPatterns['Detective'] = (officerPatterns['Detective'] || 0) + 1;
    else if (/Sgt\. ?\s+/.test(officerStr)) officerPatterns['Sergeant'] = (officerPatterns['Sergeant'] || 0) + 1;
    else if (/\d{2,3}/.test(officerStr)) officerPatterns['Contains Badge'] = (officerPatterns['Contains Badge'] || 0) + 1;
    else officerPatterns['Other Format'] = (officerPatterns['Other Format'] || 0) + 1;
});

console.log("Officer field patterns:");
Object.entries(officerPatterns).forEach(([pattern, count]) => {
    console.log(`  ${pattern}: ${count}`);
});
````

**Output**

````plaintext
=== COMPREHENSIVE DATA ANALYSIS ===

Total Records: 57


=== FIELD COMPLETENESS ===

Case Number: 100.0% complete (0 missing)

Name: 100.0% complete (0 missing)

DOB: 100.0% complete (0 missing)

SS#Calc: 84.2% complete (9 missing)

Age: 100.0% complete (0 missing)

Address: 100.0% complete (0 missing)

Race: 100.0% complete (0 missing)

Sex: 100.0% complete (0 missing)

Arrest Date: 100.0% complete (0 missing)

Charge: 100.0% complete (0 missing)

Officer of Record: 100.0% complete (0 missing)

ReportCalcSummary: 100.0% complete (0 missing)

UCR #: 100.0% complete (0 missing)

Reviewed: 100.0% complete (0 missing)


=== CASE NUMBER PATTERNS ===

Case number patterns:

  ##-######: 57 cases

Duplicate case numbers: 2

Duplicates: ["25-038522","25-035383"]


=== NAME PATTERNS ===

Name formats:

  Last, First: 57


=== ADDRESS ANALYSIS ===

Address types:

  Full Address: 26 (45.6%)

  Missing ZIP: 2 (3.5%)

  Missing State: 0 (0.0%)

  PO Box: 0 (0.0%)

  Apartment: 21 (36.8%)

  Homeless/NFA: 0 (0.0%)

  Unusual Format: 8 (14.0%)


=== OFFICER FIELD ANALYSIS ===

Officer field patterns:

  P.O. Format: 46

  Detective: 11
````

**Analyzed data**

````javascript
// Let's examine the duplicate cases and unusual data patterns more closely
console.log("=== DETAILED ANALYSIS OF ISSUES ===");

// 1. EXAMINE DUPLICATE CASES
console.log("\n1. DUPLICATE CASE ANALYSIS:");
const duplicates = ["25-038522", "25-035383"];
duplicates.forEach(caseNum => {
    const records = data.filter(r => r['Case Number'] === caseNum);
    console.log(`\nCase ${caseNum} (${records.length} records):`);
    records.forEach((record, idx) => {
        console.log(`  Record ${idx + 1}:`);
        console.log(`    Name: ${record['Name']}`);
        console.log(`    Address: ${record['Address']}`);
        console.log(`    Arrest Date: ${record['Arrest Date']}`);
        console.log(`    Charge: ${record['Charge']}`);
        console.log(`    Officer: ${record['Officer of Record']}`);
    });
});

// 2. EXAMINE UNUSUAL ADDRESS FORMATS
console.log("\n\n2. UNUSUAL ADDRESS FORMATS:");
const addresses = data.map((r, idx) => ({ idx: idx + 1, addr: r['Address'] }));
addresses.forEach(({idx, addr}) => {
    const addrStr = addr.toString().toLowerCase();
    
    // Check for unusual formats
    if (!/\d+.*street|ave|blvd|rd|drive|place.*,.*[a-z]{2}. *\d{5}/.test(addrStr) && 
        !addrStr.includes('apt') && 
        !addrStr.includes('apartment') && 
        !addrStr.includes('#') &&
        !/\d{5}/.test(addrStr)) {
        console.log(`Row ${idx}: ${addr}`);
    }
});

// 3. EXAMINE CHARGE PATTERNS
console.log("\n\n3. CHARGE ANALYSIS:");
const charges = data.map(r => r['Charge']);
const chargeTypes = {};
const statutes = {};

charges.forEach(charge => {
    // Extract statute numbers
    const statuteMatch = charge.match(/\d+:\d+-\d+/);
    if (statuteMatch) {
        statutes[statuteMatch[0]] = (statutes[statuteMatch[0]] || 0) + 1;
    }
    
    // Categorize charge types
    const chargeStr = charge.toLowerCase();
    if (chargeStr.includes('driving') || chargeStr.includes('dwi') || chargeStr.includes('dui')) {
        chargeTypes['Traffic/DUI'] = (chargeTypes['Traffic/DUI'] || 0) + 1;
    } else if (chargeStr.includes('warrant')) {
        chargeTypes['Warrant'] = (chargeTypes['Warrant'] || 0) + 1;
    } else if (chargeStr.includes('assault') || chargeStr.includes('harassment')) {
        chargeTypes['Assault/Harassment'] = (chargeTypes['Assault/Harassment'] || 0) + 1;
    } else if (chargeStr.includes('theft') || chargeStr.includes('shoplifting')) {
        chargeTypes['Theft'] = (chargeTypes['Theft'] || 0) + 1;
    } else if (chargeStr.includes('drug') || chargeStr.includes('controlled') || chargeStr.includes('marijuana')) {
        chargeTypes['Drug'] = (chargeTypes['Drug'] || 0) + 1;
    } else {
        chargeTypes['Other'] = (chargeTypes['Other'] || 0) + 1;
    }
});

console.log("Charge categories:");
Object.entries(chargeTypes)
    .sort((a, b) => b[1] - a[1])
    .forEach(([type, count]) => {
        const pct = (count / charges.length * 100).toFixed(1);
        console.log(`  ${type}: ${count} (${pct}%)`);
    });

console.log("\nTop statutes:");
Object.entries(statutes)
    .sort((a, b) => b[1] - a[1])
    .slice(0, 10)
    .forEach(([statute, count]) => {
        console.log(`  ${statute}: ${count}`);
    });

// 4. EXAMINE AGE AND DEMOGRAPHIC PATTERNS
console.log("\n\n4. DEMOGRAPHIC ANALYSIS:");
const ages = data.map(r => r['Age']).filter(age => age && !isNaN(age));
const genders = data.map(r => r['Sex']).filter(Boolean);
const races = data.map(r => r['Race']).filter(Boolean);

console.log(`Age range: ${Math.min(...ages)} - ${Math.max(...ages)}`);
console.log(`Average age: ${(ages.reduce((a, b) => a + b, 0) / ages.length).toFixed(1)}`);

// Check for juveniles
const juveniles = ages.filter(age => age < 18);
console.log(`Juveniles (under 18): ${juveniles.length}`);

// Gender distribution
const genderCounts = {};
genders.forEach(gender => {
    genderCounts[gender] = (genderCounts[gender] || 0) + 1;
});
console.log("Gender distribution:");
Object.entries(genderCounts).forEach(([gender, count]) => {
    console.log(`  ${gender}: ${count}`);
});

// Race distribution
const raceCounts = {};
races.forEach(race => {
    raceCounts[race] = (raceCounts[race] || 0) + 1;
});
console.log("Race distribution:");
Object.entries(raceCounts)
    .sort((a, b) => b[1] - a[1])
    .forEach(([race, count]) => {
        console.log(`  ${race}: ${count}`);
    });
````

**Error**

````plaintext
data is not defined
````

**Analyzed data**

````javascript
// Let me reload the data and continue the analysis
import * as XLSX from 'xlsx';

const fileContent = await window.fs.readFile('2025_05_Arrests.xlsx');
const workbook = XLSX.read(fileContent);
const data = XLSX.utils.sheet_to_json(workbook.Sheets[workbook.SheetNames[0]]);

console.log("=== DETAILED ANALYSIS OF ISSUES ===");

// 1. EXAMINE DUPLICATE CASES
console.log("\n1. DUPLICATE CASE ANALYSIS:");
const duplicates = ["25-038522", "25-035383"];
duplicates.forEach(caseNum => {
    const records = data.filter(r => r['Case Number'] === caseNum);
    console.log(`\nCase ${caseNum} (${records.length} records):`);
    records.forEach((record, idx) => {
        console.log(`  Record ${idx + 1}:`);
        console.log(`    Name: ${record['Name']}`);
        console.log(`    Address: ${record['Address']}`);
        console.log(`    Arrest Date: ${record['Arrest Date']}`);
        console.log(`    Charge: ${record['Charge']}`);
        console.log(`    Officer: ${record['Officer of Record']}`);
    });
});

// 2. EXAMINE UNUSUAL ADDRESS FORMATS
console.log("\n\n2. UNUSUAL ADDRESS FORMATS:");
const unusualAddresses = [];
data.forEach((record, idx) => {
    const addr = record['Address'];
    if (!addr) return;
    
    const addrStr = addr.toString().toLowerCase();
    
    // Check for missing ZIP codes
    if (!/\d{5}/.test(addrStr)) {
        unusualAddresses.push({
            row: idx + 1,
            address: addr,
            issue: 'Missing ZIP code'
        });
    }
    // Check for very short addresses
    else if (addr.length < 10) {
        unusualAddresses.push({
            row: idx + 1,
            address: addr,
            issue: 'Very short address'
        });
    }
    // Check for addresses without street names
    else if (!/street|st|avenue|ave|boulevard|blvd|road|rd|drive|dr|place|pl|court|ct|lane|ln|way/.test(addrStr)) {
        unusualAddresses.push({
            row: idx + 1,
            address: addr,
            issue: 'No street name detected'
        });
    }
});

console.log(`Found ${unusualAddresses.length} addresses with potential issues:`);
unusualAddresses.slice(0, 10).forEach(item => {
    console.log(`  Row ${item.row}: ${item.address} (${item.issue})`);
});

// 3. EXAMINE CHARGE PATTERNS
console.log("\n\n3.