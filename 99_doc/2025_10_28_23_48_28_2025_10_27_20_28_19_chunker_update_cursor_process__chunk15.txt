",
            "expected_answer": "Check data types and table references",
            "expected_sources": ["excel_guide.md", "troubleshooting.xlsx"]
        },
        {
            "query": "What is Power BI query syntax? ",
            "expected_answer": "Use M language for Power Query",
            "expected_sources": ["power_bi_guide.md"]
        }
    ]
    
    chroma_rag = ChromaRAG()
    faithfulness_scorer = FaithfulnessScorer()
    
    results = []
    
    for test_case in test_queries:
        # Retrieve
        retrieved = chroma_rag.search_similar(test_case["query"], n_results=5)
        
        # Generate answer
        context = "\n".join([chunk["document"] for chunk in retrieved])
        answer = generate_answer(test_case["query"], context)
        
        # Evaluate
        faithfulness = faithfulness_scorer.calculate_faithfulness(answer, context)
        
        # Check source relevance
        retrieved_sources = [chunk["metadata"]["file_name"] for chunk in retrieved]
        source_relevance = len(set(retrieved_sources) & set(test_case["expected_sources"])) / len(test_case["expected_sources"])
        
        results.append({
            "query": test_case["query"],
            "faithfulness_score": faithfulness,
            "source_relevance": source_relevance,
            "retrieved_sources": retrieved_sources
        })
    
    return results
```

These concrete examples provide Grok with:
1. **Complete faithfulness scoring** with detailed analysis
2. **Full ChromaDB integration** with CRUD operations
3. **Real-world usage examples** for RAG pipeline
4. **Evaluation framework** for performance monitoring

Should I create the complete implementation files for Grok to use? ```python
import re
import nltk
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

class FaithfulnessScorer:
    def __init__(self):
        self.model = SentenceTransformer('all-MiniLM-L6-v2')
        self.stop_words = set(nltk.corpus.stopwords.words('english'))
    
    def extract_claims(self, answer):
        """Extract factual claims from the answer"""
        # Split into sentences and filter out questions/opinions
        sentences = nltk.sent_tokenize(answer)
        claims = []
        
        for sentence in sentences:
            # Remove questions and opinion indicators
            if not re.search(r'\?|I think|I believe|in my opinion', sentence, re.IGNORECASE):
                # Extract factual statements
                if re.search(r'\b(is|are|was|were|has|have|will|can|should)\b', sentence, re.IGNORECASE):
                    claims.append(sentence.strip())
        
        return claims
    
    def calculate_faithfulness(self, answer, context, threshold=0.7):
        """
        Calculate faithfulness score (0-1) based on how well answer claims
        are supported by the context
        """
        claims = self.extract_claims(answer)
        if not claims:
            return 1.0  # No claims to verify
        
        # Get embeddings for claims and context
        claim_embeddings = self.model.encode(claims)
        context_embeddings = self.model.encode([context])
        
        faithfulness_scores = []
        
        for claim_emb in claim_embeddings:
            # Find best matching context sentence
            similarities = cosine_similarity([claim_emb], context_embeddings)[0]
            max_similarity = np.max(similarities)
            
            # Check if claim is supported by context
            if max_similarity >= threshold:
                faithfulness_scores.append(1.0)
            else:
                # Check for partial support
                faithfulness_scores.append(max_similarity)
        
        return np.mean(faithfulness_scores)
    
    def detailed_faithfulness_analysis(self, answer, context):
        """Provide detailed analysis of faithfulness"""
        claims = self.extract_claims(answer)
        analysis = {
            "total_claims": len(claims),
            "supported_claims": 0,
            "unsupported_claims": 0,
            "claim_details": []
        }
        
        for claim in claims:
            claim_emb = self.model.encode([claim])
            context_emb = self.model.encode([context])
            similarity = cosine_similarity(claim_emb, context_emb)[0][0]
            
            is_supported = similarity >= 0.7
            if is_supported:
                analysis["supported_claims"] += 1
            else:
                analysis["unsupported_claims"] += 1
            
            analysis["claim_details"].append({
                "claim": claim,
                "similarity": float(similarity),
                "supported": is_supported
            })
        
        analysis["faithfulness_score"] = analysis["supported_claims"] / analysis["total_claims"] if analysis["total_claims"] > 0 else 1.0
        
        return analysis

# Usage example
scorer = FaithfulnessScorer()
answer = "The vlookup function requires exact matches. You should use FALSE for exact matching." context = "VLOOKUP syntax: VLOOKUP(lookup_value, table_array, col_index_num, [range_lookup]). Use FALSE for exact matches, TRUE for approximate matches." faithfulness_score = scorer.calculate_faithfulness(answer, context)
print(f"Faithfulness Score: {faithfulness_score:.2f}")

detailed_analysis = scorer.detailed_faithfulness_analysis(answer, context)
print(f"Supported claims: {detailed_analysis['supported_claims']}/{detailed_analysis['total_claims']}")
```

```python
import chromadb
from chromadb.config import Settings
import uuid
from datetime import datetime
import json

class ChromaRAG:
    def __init__(self, persist_directory="./chroma_db"):
        self.client = chromadb.PersistentClient(
            path=persist_directory,
            settings=Settings(anonymized_telemetry=False)
        )
        
        # Create or get collection
        self.collection = self.client.get_or_create_collection(
            name="chunker_knowledge_base",
            metadata={"description": "Enterprise chunker knowledge base"}
        )
    
    def add_chunk(self, chunk_text, metadata):
        """Add a chunk to the vector database"""
        chunk_id = f"{metadata['timestamp']}_{metadata['file_name']}_chunk{metadata['chunk_index']}"
        
        # Prepare metadata for ChromaDB
        chroma_metadata = {
            "file_name": metadata["file_name"],
            "file_type": metadata["file_type"],
            "chunk_index": metadata["chunk_index"],
            "timestamp": metadata["timestamp"],
            "department": metadata.get("department", "admin"),
            "keywords": json.dumps(metadata.get("keywords", [])),
            "file_size": metadata.get("file_size", 0),
            "processing_time": metadata.get("processing_time", 0)
        }
        
        # Add to collection
        self.collection.add(
            documents=[chunk_text],
            metadatas=[chroma_metadata],
            ids=[chunk_id]
        )
        
        return chunk_id
    
    def search_similar(self, query, n_results=5, file_type=None, department=None):
        """Search for similar chunks"""
        where_clause = {}
        
        if file_type:
            where_clause["file_type"] = file_type
        if department:
            where_clause["department"] = department
        
        results = self.collection.query(
            query_texts=[query],
            n_results=n_results,
            where=where_clause if where_clause else None
        )
        
        return self._format_results(results)
    
    def search_by_keywords(self, keywords, n_results=5):
        """Search by specific keywords"""
        # Convert keywords to query string
        query = " ".join(keywords)
        return self.search_similar(query, n_results)
    
    def get_chunk_by_id(self, chunk_id):
        """Retrieve specific chunk by ID"""
        results = self.collection.get(ids=[chunk_id])
        return self._format_results(results)
    
    def update_chunk(self, chunk_id, new_text, new_metadata):
        """Update existing chunk"""
        self.collection.update(
            ids=[chunk_id],
            documents=[new_text],
            metadatas=[new_metadata]
        )
    
    def delete_chunk(self, chunk_id):
        """Delete chunk from database"""
        self.collection.delete(ids=[chunk_id])
    
    def get_collection_stats(self):
        """Get statistics about the collection"""
        count = self.collection.count()
        return {
            "total_chunks": count,
            "collection_name": self.collection.name,
            "last_updated": datetime.now().isoformat()
        }
    
    def _format_results(self, results):
        """Format ChromaDB results for easier use"""
        formatted_results = []
        
        if results.get("documents"):
            for i, doc in enumerate(results["documents"][0]):
                formatted_results.append({
                    "id": results["ids"][0][i],
                    "document": doc,
                    "metadata": results["metadatas"][0][i],
                    "distance": results["distances"][0][i] if results.get("distances") else None
                })
        
        return formatted_results

# Integration with watcher_splitter.py
def integrate_chroma_with_watcher():
    """Example of how to integrate ChromaDB with the existing watcher"""
    
    # Initialize ChromaDB
    chroma_rag = ChromaRAG()
    
    def process_file_with_chroma(file_path, config):
        """Enhanced file processing with ChromaDB integration"""
        # ... existing processing code ...
        
        # After creating chunks, add them to ChromaDB
        for i, chunk in enumerate(chunks):
            metadata = {
                "file_name": file_path.name,
                "file_type": file_path.suffix,
                "chunk_index": i + 1,
                "timestamp": datetime.now().isoformat(),
                "department": department,
                "keywords": extract_keywords(chunk),
                "file_size": file_path.stat().st_size,
                "processing_time": time.time() - start_time
            }
            
            chunk_id = chroma_rag.add_chunk(chunk, metadata)
            logger.info(f"Added chunk to ChromaDB: {chunk_id}")
        
        return True

# Usage examples
def example_usage():
    # Initialize
    rag = ChromaRAG()
    
    # Add chunks
    metadata = {
        "file_name": "excel_guide.md",
        "file_type": ".md",
        "chunk_index": 1,
        "timestamp": "2025-10-27T12:00:00Z",
        "department": "admin",
        "keywords": ["excel", "vlookup", "formulas"]
    }
    
    chunk_id = rag.add_chunk(
        "VLOOKUP is used to find values in a table. Syntax: VLOOKUP(lookup_value, table_array, col_index_num, [range_lookup])",
        metadata
    )
    
    # Search
    results = rag.search_similar("How do I use vlookup in Excel? ", n_results=3)
    print(f"Found {len(results)} similar chunks")
    
    # Search by keywords
    keyword_results = rag.search_by_keywords(["vlookup", "excel"])
    print(f"Found {len(keyword_results)} chunks with keywords")
    
    # Get stats
    stats = rag.get_collection_stats()
    print(f"Total chunks in database: {stats['total_chunks']}")
```

```python
def complete_rag_example():
    """Complete RAG pipeline with evaluation"""
    
    # Initialize components
    chroma_rag = ChromaRAG()
    faithfulness_scorer = FaithfulnessScorer()
    
    # Query
    query = "How do I fix Power BI query errors?" # Retrieve relevant chunks
    retrieved_chunks = chroma_rag.search_similar(query, n_results=5)
    
    # Generate answer (simplified)
    context = "\n".join([chunk["document"] for chunk in retrieved_chunks])
    answer = f"Based on the context: {context[:500]}..."
    
    # Evaluate faithfulness
    faithfulness_score = faithfulness_scorer.calculate_faithfulness(answer, context)
    
    # Return results
    return {
        "query": query,
        "answer": answer,
        "sources": [chunk["metadata"]["file_name"] for chunk in retrieved_chunks],
        "faithfulness_score": faithfulness_score,
        "retrieved_chunks": len(retrieved_chunks)
    }
```

```python
def batch_process_files(file_paths):
    """Process multiple files and add to ChromaDB"""
    chroma_rag = ChromaRAG()
    
    for file_path in file_paths:
        # Process file (existing logic)
        chunks = process_file_enhanced(file_path, config)
        
        # Add each chunk to ChromaDB
        for i, chunk in enumerate(chunks):
            metadata = extract_metadata(file_path, chunk, i)
            chunk_id = chroma_rag.add_chunk(chunk, metadata)
            print(f"Added {chunk_id} to knowledge base")
    
    # Get final stats
    stats = chroma_rag.get_collection_stats()
    print(f"Knowledge base now contains {stats['total_chunks']} chunks")
```

```python
def evaluate_rag_performance():
    """Evaluate RAG performance with test queries"""
    
    test_queries = [
        {
            "query": "How do I fix vlookup errors? ",
            "expected_answer": "Check data types and table references",
            "expected_sources": ["excel_guide.md", "troubleshooting.xlsx"]
        },
        {
            "query": "What is Power BI query syntax? ",
            "expected_answer": "Use M language for Power Query",
            "expected_sources": ["power_bi_guide.md"]
        }
    ]
    
    chroma_rag = ChromaRAG()
    faithfulness_scorer = FaithfulnessScorer()
    
    results = []
    
    for test_case in test_queries:
        # Retrieve
        retrieved = chroma_rag.search_similar(test_case["query"], n_results=5)
        
        # Generate answer
        context = "\n".join([chunk["document"] for chunk in retrieved])
        answer = generate_answer(test_case["query"], context)
        
        # Evaluate
        faithfulness = faithfulness_scorer.calculate_faithfulness(answer, context)
        
        # Check source relevance
        retrieved_sources = [chunk["metadata"]["file_name"] for chunk in retrieved]
        source_relevance = len(set(retrieved_sources) & set(test_case["expected_sources"])) / len(test_case["expected_sources"])
        
        results.append({
            "query": test_case["query"],
            "faithfulness_score": faithfulness,
            "source_relevance": source_relevance,
            "retrieved_sources": retrieved_sources
        })
    
    return results
```

---

**User**

do we want Expand on faithfulness scoring
Integrate Llamalndex

---

**Cursor**

Yesâ€”expanding faithfulness scoring and integrating LlamaIndex will significantly enhance your RAG system. Here's why and how:

## Expanded Faithfulness Scoring

### 1. **Multi-dimensional faithfulness**
```python
class AdvancedFaithfulnessScorer:
    def __init__(self):
        self.model = SentenceTransformer('all-MiniLM-L6-v2')
        self.llm = Ollama(model="llama3.2")  # Local LLM for verification
    
    def comprehensive_faithfulness(self, answer, context, query):
        """Multi-dimensional faithfulness evaluation"""
        
        # 1. Claim extraction and verification
        claims = self.extract_claims(answer)
        claim_scores = []
        
        for claim in claims:
            # Semantic similarity
            semantic_score = self.calculate_semantic_similarity(claim, context)
            
            # Factual verification using LLM
            factual_score = self.verify_fact_with_llm(claim, context)
            
            # Contradiction detection
            contradiction_score = self.detect_contradictions(claim, context)
            
            claim_scores.append({
                "claim": claim,
                "semantic_similarity": semantic_score,
                "factual_verification": factual_score,
                "contradiction_detection": contradiction_score,
                "overall_score": (semantic_score + factual_score + contradiction_score) / 3
            })
        
        # 2. Context utilization analysis
        context_utilization = self.analyze_context_utilization(answer, context)
        
        # 3. Hallucination detection
        hallucination_score = self.detect_hallucinations(answer, context)
        
        # 4. Source attribution
        source_attribution = self.verify_source_attribution(answer, context)
        
        return {
            "overall_faithfulness": np.mean([score["overall_score"] for score in claim_scores]),
            "claim_analysis": claim_scores,
            "context_utilization": context_utilization,
            "hallucination_score": hallucination_score,
            "source_attribution": source_attribution,
            "detailed_breakdown": {
                "semantic_similarity": np.mean([score["semantic_similarity"] for score in claim_scores]),
                "factual_verification": np.mean([score["factual_verification"] for score in claim_scores]),
                "contradiction_detection": np.mean([score["contradiction_detection"] for score in claim_scores])
            }
        }
    
    def verify_fact_with_llm(self, claim, context):
        """Use local LLM to verify factual accuracy"""
        prompt = f"""
        Context: {context}
        Claim: {claim}
        
        Is this claim factually accurate based on the context? Respond with: ACCURATE, INACCURATE, or UNCLEAR
        """
        
        response = self.llm.generate(prompt)
        
        if "ACCURATE" in response.upper():
            return 1.0
        elif "INACCURATE" in response.upper():
            return 0.0
        else:
            return 0.5  # UNCLEAR
    
    def detect_hallucinations(self, answer, context):
        """Detect if answer contains information not in context"""
        answer_sentences = nltk.sent_tokenize(answer)
        context_sentences = nltk.sent_tokenize(context)
        
        hallucination_scores = []
        
        for answer_sent in answer_sentences:
            # Check if sentence is supported by context
            max_similarity = 0
            for context_sent in context_sentences:
                similarity = self.calculate_semantic_similarity(answer_sent, context_sent)
                max_similarity = max(max_similarity, similarity)
            
            # If similarity is too low, it might be hallucination
            if max_similarity < 0.3:
                hallucination_scores.append(0.0)  # Likely hallucination
            else:
                hallucination_scores.append(1.0)  # Supported by context
        
        return np.mean(hallucination_scores)
```

## LlamaIndex Integration

### 1. **Why LlamaIndex? **
- **Advanced retrieval**: Hybrid search, re-ranking, query routing
- **Built-in evaluation**: Comprehensive RAG evaluation metrics
- **Multiple data sources**: Handles various file types natively
- **Production-ready**: Scalable and enterprise-grade

### 2. **LlamaIndex Integration**
```python
from llama_index.core import VectorStoreIndex, Document, Settings
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.embeddings.ollama import OllamaEmbedding
from llama_index.llms.ollama import Ollama
from llama_index.core.evaluation import FaithfulnessEvaluator, RelevancyEvaluator
import chromadb

class LlamaIndexRAG:
    def __init__(self, persist_dir="./llama_chroma_db"):
        # Initialize ChromaDB
        self.chroma_client = chromadb.PersistentClient(path=persist_dir)
        self.chroma_collection = self.chroma_client.get_or_create_collection("llama_knowledge")
        
        # Initialize LlamaIndex components
        Settings.embed_model = OllamaEmbedding(model_name="nomic-embed-text")
        Settings.llm = Ollama(model="llama3.2")
        
        # Create vector store
        self.vector_store = ChromaVectorStore(chroma_collection=self.chroma_collection)
        
        # Initialize evaluators
        self.faithfulness_evaluator = FaithfulnessEvaluator()
        self.relevancy_evaluator = RelevancyEvaluator()
    
    def add_documents(self, file_paths):
        """Add documents to the knowledge base"""
        documents = []
        
        for file_path in file_paths:
            # Process different file types
            if file_path.suffix == '.xlsx':
                content = self.process_excel_file(file_path)
            elif file_path.suffix == '.pdf':
                content = self.process_pdf_file(file_path)
            elif file_path.suffix == '.py':
                content = self.process_python_file(file_path)
            else:
                content = self.process_text_file(file_path)
            
            # Create document with metadata
            doc = Document(
                text=content,
                metadata={
                    "file_name": file_path.name,
                    "file_type": file_path.suffix,
                    "file_size": file_path.stat().st_size,
                    "timestamp": datetime.now().isoformat()
                }
            )
            documents.append(doc)
        
        # Create index
        self.index = VectorStoreIndex.from_documents(
            documents, 
            vector_store=self.vector_store
        )
        
        return len(documents)
    
    def query_with_evaluation(self, query, top_k=5):
        """Query with comprehensive evaluation"""
        
        # Create query engine
        query_engine = self.index.as_query_engine(
            similarity_top_k=top_k,
            response_mode="compact"
        )
        
        # Execute query
        response = query_engine.query(query)
        
        # Evaluate response
        evaluation_results = self.evaluate_response(query, response)
        
        return {
            "query": query,
            "answer": str(response),
            "sources": [node.metadata for node in response.source_nodes],
            "evaluation": evaluation_results
        }
    
    def evaluate_response(self, query, response):
        """Comprehensive evaluation using LlamaIndex evaluators"""
        
        # Faithfulness evaluation
        faithfulness_result = self.faithfulness_evaluator.evaluate_response(
            response=response
        )
        
        # Relevancy evaluation
        relevancy_result = self.relevancy_evaluator.evaluate_response(
            query=query,
            response=response
        )
        
        # Custom evaluation
        custom_eval = self.custom_evaluation(query, response)
        
        return {
            "faithfulness": faithfulness_result.score,
            "relevancy": relevancy_result.score,
            "custom_metrics": custom_eval
        }
    
    def custom_evaluation(self, query, response):
        """Custom evaluation metrics"""
        
        # Source diversity
        sources = [node.metadata["file_name"] for node in response.source_nodes]
        source_diversity = len(set(sources)) / len(sources) if sources else 0
        
        # Answer length appropriateness
        answer_length = len(str(response))
        length_score = 1.0 if 50 <= answer_length <= 500 else 0.5
        
        # Technical accuracy (for code-related queries)
        technical_score = self.evaluate_technical_accuracy(query, str(response))
        
        return {
            "source_diversity": source_diversity,
            "length_appropriateness": length_score,
            "technical_accuracy": technical_score
        }
    
    def process_excel_file(self, file_path):
        """Process Excel files with metadata extraction"""
        import pandas as pd
        
        # Read Excel file
        excel_data = pd.read_excel(file_path, sheet_name=None)
        
        content_parts = []
        
        for sheet_name, df in excel_data.items():
            # Extract sheet content
            content_parts.append(f"Sheet: {sheet_name}")
            content_parts.append(f"Columns: {', '.join(df.columns)}")
            content_parts.append(f"Data types: {df.dtypes.to_dict()}")
            
            # Extract formulas if any
            formulas = self.extract_formulas(df)
            if formulas:
                content_parts.append(f"Formulas: {formulas}")
            
            # Sample data
            content_parts.append(f"Sample data:\n{df.head().to_string()}")
        
        return "\n".join(content_parts)
    
    def process_python_file(self, file_path):
        """Process Python files with code analysis"""
        import ast
        
        with open(file_path, 'r', encoding='utf-8') as f:
            code = f.read()
        
        try:
            tree = ast.parse(code)
            
            # Extract functions, classes, imports
            functions = [node.name for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)]
            classes = [node.name for node in ast.walk(tree) if isinstance(node, ast.ClassDef)]
            imports = [node.names[0].name for node in ast.walk(tree) if isinstance(node, ast.Import)]
            
            content_parts = [
                f"Python file: {file_path.name}",
                f"Functions: {', '.join(functions)}",
                f"Classes: {', '.join(classes)}",
                f"Imports: {', '.join(imports)}",
                f"Code:\n{code}"
            ]
            
            return "\n".join(content_parts)
            
        except SyntaxError:
            return f"Python file: {file_path.name}\nCode:\n{code}"
```

## Integration with Existing System

### 1. **Enhanced watcher_splitter.py**
```python
def integrate_llamaindex_with_watcher():
    """Integrate LlamaIndex with existing watcher system"""
    
    # Initialize LlamaIndex RAG
    llama_rag = LlamaIndexRAG()
    
    def process_file_with_llamaindex(file_path, config):
        """Enhanced file processing with LlamaIndex"""
        
        # Process file (existing logic)
        chunks = process_file_enhanced(file_path, config)
        
        # Add to LlamaIndex knowledge base
        documents_added = llama_rag.add_documents([file_path])
        
        # Log processing
        logger.info(f"Added {documents_added} documents to LlamaIndex knowledge base")
        
        return True
    
    def query_knowledge_base(query):
        """Query the knowledge base with evaluation"""
        return llama_rag.query_with_evaluation(query)
```

## Benefits of This Approach

### 1. **Advanced Faithfulness Scoring**
- Multi-dimensional evaluation
- LLM-based verification
- Hallucination detection
- Source attribution

### 2. **LlamaIndex Advantages**
- Production-ready RAG
- Built-in evaluation metrics
- Multiple data source support
- Advanced retrieval strategies

### 3. **Comprehensive Evaluation**
- Faithfulness and relevancy
- Custom metrics
- Source diversity
- Technical accuracy

## Recommendation

Tell Grok to implement:
1. **Advanced faithfulness scoring** with multi-dimensional analysis
2. **LlamaIndex integration** for production-ready RAG
3. **Comprehensive evaluation pipeline** with custom metrics
4. **File type-specific processing** for Excel, PDF, Python files

This will give you enterprise-grade RAG with robust evaluation and faithfulness scoring. Should I create the complete implementation files for Grok to use? ```python
class AdvancedFaithfulnessScorer:
    def __init__(self):
        self.model = SentenceTransformer('all-MiniLM-L6-v2')
        self.llm = Ollama(model="llama3.2")  # Local LLM for verification
    
    def comprehensive_faithfulness(self, answer, context, query):
        """Multi-dimensional faithfulness evaluation"""
        
        # 1. Claim extraction and verification
        claims = self.extract_claims(answer)
        claim_scores = []
        
        for claim in claims:
            # Semantic similarity
            semantic_score = self.calculate_semantic_similarity(claim, context)
            
            # Factual verification using LLM
            factual_score = self.verify_fact_with_llm(claim, context)
            
            # Contradiction detection
            contradiction_score = self.detect_contradictions(claim, context)
            
            claim_scores.append({
                "claim": claim,
                "semantic_similarity": semantic_score,
                "factual_verification": factual_score,
                "contradiction_detection": contradiction_score,
                "overall_score": (semantic_score + factual_score + contradiction_score) / 3
            })
        
        # 2. Context utilization analysis
        context_utilization = self.analyze_context_utilization(answer, context)
        
        # 3. Hallucination detection
        hallucination_score = self.detect_hallucinations(answer, context)
        
        # 4. Source attribution
        source_attribution = self.verify_source_attribution(answer, context)
        
        return {
            "overall_faithfulness": np.mean([score["overall_score"] for score in claim_scores]),
            "claim_analysis": claim_scores,
            "context_utilization": context_utilization,
            "hallucination_score": hallucination_score,
            "source_attribution": source_attribution,
            "detailed_breakdown": {
                "semantic_similarity": np.mean([score["semantic_similarity"] for score in claim_scores]),
                "factual_verification": np.mean([score["factual_verification"] for score in claim_scores]),
                "contradiction_detection": np.mean([score["contradiction_detection"] for score in claim_scores])
            }
        }
    
    def verify_fact_with_llm(self, claim, context):
        """Use local LLM to verify factual accuracy"""
        prompt = f"""
        Context: {context}
        Claim: {claim}
        
        Is this claim factually accurate based on the context?