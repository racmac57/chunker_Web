**Add the helper functions** at the top (after imports)
3. **Find and replace** the specific sections I highlighted
4. **Save and test**

### **Option 2: Complete Fixed Script**
**Want me to create a complete replacement script? ** I can take the Claude Code version from your project knowledge and apply all the fixes to give you a ready-to-use file. ## üö® **Why I Provided Patches Instead:**

- **Preserves Claude Code's work** - keeps all the date parsing fixes
- **Surgical approach** - only fixes the formatting issue
- **Learning opportunity** - you see exactly what was wrong
- **Safer** - less chance of introducing new bugs

## üí° **Quick Decision:**

**If you want the complete fixed script ready to drop in and replace your current file, just say "yes, give me the complete script" and I'll create a full replacement version with all fixes applied. **

**Otherwise, the patch approach in the artifact will fix your specific formatting errors while keeping everything else Claude Code perfected! **



## Prompt:
7/29/2025, 3:04:23 AM

> File: RMS_Statistical_Export.py

the script returned this error
===================================================
  SCRPA RMS STATISTICAL EXPORT SYSTEM
  Hackensack Police Department
  Author: R. A. Carucci
===================================================
‚â°∆í√â√¨ Using Python: C:\Program Files\ArcGIS\Pro\bin\Python\envs\arcgispro-py3\python.exe
‚â°∆í√¥√§ Running Script: C:\Users\carucci_r\OneDrive - City of Hackensack\02_ETL_Scripts\SCRPA_ArcPy\05_orchestrator\RMS_Statistical_Export.py
‚â°∆í√¥√º RMS Data Source: C:\Users\carucci_r\OneDrive - City of Hackensack\05_EXPORTS\_RMS\SCRPA
‚â°∆í√¥√® Output Location: C:\Users\carucci_r\OneDrive - City of Hackensack\02_ETL_Scripts\SCRPA_ArcPy\06_Output
‚â°∆í√ú√á Starting RMS Statistical Export...
Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂
  File "C:\Users\carucci_r\OneDrive - City of Hackensack\02_ETL_Scripts\SCRPA_ArcPy\05_orchestrator\RMS_Statistical_Export.py", line 431
    report += f"| Total Incidents | {safe_int_format(stats['total_incidents'])} |
IndentationError: unexpected indent
Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂Œì√á√∂
Œì¬•√Æ Script execution failed with error code 1
Common issues:
  Œì√á√≥ No RMS files found in source folder
  Œì√á√≥ Missing pandas/numpy libraries
  Œì√á√≥ Permission issues with output folder
  Œì√á√≥ Corrupted RMS Excel file
Check the error messages above for specific details. Press any key to continue...



## Response:
7/29/2025, 3:06:17 AM

**Found the issue! ** There's a **syntax error** on line 431 - the f-string is missing its closing quote and has incorrect indentation. Looking at your script, I can see several lines got corrupted during the manual editing. Let me create a **completely fixed version** for you:

**Complete Fixed RMS Statistical Export Script**

````python
#!/usr/bin/env python3
# üïí 2025-07-29-16-00-00
# SCRPA_Statistics/RMS_Statistical_Export_COMPLETE_FIXED
# Author: R. A. Carucci
# Purpose: Generate Markdown reports with automatic column detection - SYNTAX FIXED

import os
import sys
import pandas as pd
import numpy as np
import glob
from datetime import date, datetime, timedelta
from collections import Counter
import re

# CRITICAL FIX: Helper functions for safe formatting
def safe_int_format(value):
    """Safely convert value to int for string formatting"""
    if pd.isna(value):
        return 0
    try:
        return int(float(value))
    except (ValueError, TypeError):
        return 0

def safe_str_format(value):
    """Safely convert value to string"""
    if pd.isna(value):
        return 'Unknown'
    return str(value)

def safe_zone_format(value):
    """Safely format zone numbers"""
    if pd.isna(value):
        return 'Unknown'
    try:
        return str(int(float(value)))
    except (ValueError, TypeError):
        return str(value)

print("=== SCRPA RMS STATISTICAL EXPORT SYSTEM ===")
print("Generating executive summaries from RMS data with auto-detection")

# Configuration
RMS_FOLDER = r"C:\Users\carucci_r\OneDrive - City of Hackensack\05_EXPORTS\_RMS\SCRPA"
OUTPUT_FOLDER = r"C:\Users\carucci_r\OneDrive - City of Hackensack\02_ETL_Scripts\SCRPA_ArcPy\06_Output"

# Column mapping patterns - multiple possible names for each field
COLUMN_PATTERNS = {
    'call_time': ['Time of Call', 'Call Time', 'TimeOfCall', 'CallTime', 'Reported Time', 'Report Time'],
    'incident_date': ['Incident Date', 'IncidentDate', 'Date of Incident', 'Offense Date'],
    'incident_time': ['Incident Time', 'IncidentTime', 'Time of Incident', 'Offense Time'],
    'incident': ['Incident', 'Crime Type', 'CrimeType', 'Offense', 'Charge', 'Incident Type', 'Incident Type_1'],
    'zone': ['PDZone', 'Zone', 'District', 'Beat', 'Sector', 'Area'],
    'address': ['FullAddress2', 'Address', 'Location', 'Full Address', 'Street Address', 'FullAddress'],
    'report_number': ['ReportNumberNew', 'Report Number', 'ReportNumber', 'Case Number', 'CaseNumber', 'RMS Number'],
    'disposition': ['Disposition', 'Status', 'Case Status', 'Resolution', 'Case_Status'],
    'response_time': ['Time Response', 'Response Time', 'ResponseTime'],
    'officer': ['Officer', 'Responding Officer', 'Primary Officer', 'Assigned Officer', 'Officer of Record']
}

def get_latest_rms_file():
    """Find the most recent RMS Excel file"""
    try:
        patterns = [
            os.path.join(RMS_FOLDER, "*RMS*.xlsx"),
            os.path.join(RMS_FOLDER, "*SCRPA*.xlsx"),
            os.path.join(RMS_FOLDER, "*.xlsx")
        ]
        
        files = []
        for pattern in patterns:
            files.extend(glob.glob(pattern))
        
        if not files:
            print(f"‚ùå No Excel files found in {RMS_FOLDER}")
            return None
        
        # Remove duplicates and sort by modification time
        files = list(set(files))
        latest_file = max(files, key=os.path.getmtime)
        print(f"‚úÖ Using latest RMS file: {os.path.basename(latest_file)}")
        return latest_file
        
    except Exception as e:
        print(f"‚ùå Error finding RMS file: {e}")
        return None

def detect_columns(df):
    """Automatically detect column mappings"""
    print("üîç Detecting column structure...")
    
    available_columns = df.columns.tolist()
    print(f"üìã Available columns ({len(available_columns)}):")
    for i, col in enumerate(available_columns, 1):
        print(f"   {i:2d}. '{col}'")
    
    column_map = {}
    
    for field, possible_names in COLUMN_PATTERNS.items():
        found_column = None
        
        # Exact match first
        for col_name in available_columns:
            if col_name in possible_names:
                found_column = col_name
                break
        
        # If no exact match, try case-insensitive partial matching
        if not found_column:
            for col_name in available_columns:
                for pattern in possible_names:
                    if pattern.lower() in col_name.lower() or col_name.lower() in pattern.lower():
                        found_column = col_name
                        break
                if found_column:
                    break
        
        column_map[field] = found_column
        status = "‚úÖ" if found_column else "‚ùå"
        print(f"   {status} {field}: {found_column or 'NOT FOUND'}")
    
    return column_map

def load_and_analyze_rms_data(file_path):
    """Load RMS data with robust column detection"""
    try:
        print(f"üìä Loading RMS data from: {os.path.basename(file_path)}")
        
        # Read Excel file
        df = pd.read_excel(file_path)
        print(f"‚úÖ Loaded {len(df)} records")
        
        # Detect column structure
        column_map = detect_columns(df)
        
        # Check if we have minimum required columns
        has_datetime = (column_map.get('call_time') or 
                       (column_map.get('incident_date') and column_map.get('incident_time')) or
                       column_map.get('incident_date'))
        
        required_fields = ['incident']
        missing_required = [field for field in required_fields if not column_map.get(field)]
        
        if not has_datetime:
            missing_required.append('date/time (need call_time, incident_date+incident_time, or incident_date)')
        
        if missing_required:
            print(f"‚ùå Missing required columns for: {missing_required}")
            print("Available columns suggest this might not be a standard RMS export.") # Try to work with whatever we have
            print("üîÑ Attempting alternative column detection...")
            
            # Look for date columns specifically
            date_columns = [col for col in df.columns if any(word in col.lower() for word in ['date']) and 'incident' in col.lower()]
            time_columns = [col for col in df.columns if any(word in col.lower() for word in ['time']) and 'incident' in col.lower()]
            
            if date_columns:
                column_map['incident_date'] = date_columns[0]
                print(f"   ‚úÖ Using '{date_columns[0]}' as incident date")
            
            if time_columns:
                column_map['incident_time'] = time_columns[0]
                print(f"   ‚úÖ Using '{time_columns[0]}' as incident time")
            
            # If no incident-specific date/time, look for any date/time column
            if not date_columns and not time_columns:
                general_datetime_columns = [col for col in df.columns if any(word in col.lower() for word in ['time', 'date', 'when', 'occurred'])]
                if general_datetime_columns:
                    column_map['call_time'] = general_datetime_columns[0]
                    print(f"   ‚úÖ Using '{general_datetime_columns[0]}' as call time")
            
            # Look for any incident/crime column
            crime_columns = [col for col in df.columns if any(word in col.lower() for word in ['incident', 'crime', 'offense', 'charge', 'type'])]
            if crime_columns:
                column_map['incident'] = crime_columns[0]
                print(f"   ‚úÖ Using '{crime_columns[0]}' as incident type")
        
        # Create standardized dataframe
        cleaned_df = pd.DataFrame()
        
        # Handle datetime columns - intelligent parsing
        if column_map.get('incident_date') and column_map.get('incident_time'):
            # RMS style: separate date and time columns
            print(f"üìÖ Combining separate date/time columns: '{column_map['incident_date']}' + '{column_map['incident_time']}'")
            
            # Parse date and time separately
            date_col = pd.to_datetime(df[column_map['incident_date']], errors='coerce')
            time_col = pd.to_datetime(df[column_map['incident_time']], errors='coerce')
            
            # Combine date from first column with time from second column
            combined_datetimes = []
            for i in range(len(df)):
                try:
                    if pd.notna(date_col.iloc[i]) and pd.notna(time_col.iloc[i]):
                        # Extract date part from date_col and time part from time_col
                        date_part = date_col.iloc[i].date()
                        time_part = time_col.iloc[i].time()
                        combined_dt = pd.Timestamp.combine(date_part, time_part)
                        combined_datetimes.append(combined_dt)
                    elif pd.notna(date_col.iloc[i]):
                        # Use just the date if time is missing
                        combined_datetimes.append(pd.Timestamp.combine(date_col.iloc[i].date(), pd.Timestamp.min.time()))
                    else:
                        combined_datetimes.append(pd.NaT)
                except Exception as e:
                    print(f"‚ö†Ô∏è Date combination error at row {i}: {e}")
                    combined_datetimes.append(pd.NaT)
            
            cleaned_df['ReportDate'] = combined_datetimes
            
        elif column_map.get('call_time'):
            # CAD style: single datetime column
            print(f"üìÖ Using single datetime column: '{column_map['call_time']}'")
            cleaned_df['ReportDate'] = pd.to_datetime(df[column_map['call_time']], errors='coerce')
            
        elif column_map.get('incident_date'):
            # Fallback: use just incident date
            print(f"üìÖ Using incident date only: '{column_map['incident_date']}'")
            cleaned_df['ReportDate'] = pd.to_datetime(df[column_map['incident_date']], errors='coerce')
            
        else:
            print("‚ö†Ô∏è No date/time columns found - using current date")
            cleaned_df['ReportDate'] = pd.Timestamp.now()
        
        # Handle incident type - check for multiple incident type columns
        if column_map.get('incident'):
            incident_col = column_map['incident']
            if 'Incident Type_1' in df.columns:
                # Use Incident Type_1 as primary
                cleaned_df['CrimeType'] = df['Incident Type_1'].fillna('Unknown').astype(str).str.strip()
            else:
                cleaned_df['CrimeType'] = df[incident_col].fillna('Unknown').astype(str).str.strip()
        else:
            cleaned_df['CrimeType'] = 'Unknown Incident'
        
        # Handle zone with safe formatting
        if column_map.get('zone'):
            zone_data = df[column_map['zone']].copy()
            cleaned_df['Zone'] = zone_data.apply(safe_zone_format)
        else:
            cleaned_df['Zone'] = 'Unknown'
        
        # Handle address
        if column_map.get('address'):
            cleaned_df['Address'] = df[column_map['address']].fillna('Address Not Available').astype(str)
        else:
            cleaned_df['Address'] = 'Address Not Available'
        
        # Handle report number
        if column_map.get('report_number'):
            cleaned_df['ReportNumber'] = df[column_map['report_number']].fillna('N/A').astype(str)
        else:
            cleaned_df['ReportNumber'] = df.index.astype(str)
        
        # Handle disposition
        if column_map.get('disposition'):
            cleaned_df['Disposition'] = df[column_map['disposition']].fillna('Unknown').astype(str)
        else:
            cleaned_df['Disposition'] = 'Unknown'
        
        # Add derived fields
        cleaned_df['Hour'] = cleaned_df['ReportDate'].dt.hour
        cleaned_df['DayName'] = cleaned_df['ReportDate'].dt.day_name()
        cleaned_df['IsCleared'] = cleaned_df['Disposition'].str.contains('report|closed|cleared', case=False, na=False)
        
        # Filter out invalid dates
        valid_dates = cleaned_df['ReportDate'].notna()
        cleaned_df = cleaned_df[valid_dates].copy()
        
        print(f"üìÖ After processing: {len(cleaned_df)} records with valid dates")
        print(f"üìä Date range: {cleaned_df['ReportDate'].min()} to {cleaned_df['ReportDate'].max()}")
        
        return cleaned_df, column_map
        
    except Exception as e:
        print(f"‚ùå Error loading RMS data: {e}")
        print(f"‚ùå Exception details: {type(e).__name__}: {str(e)}")
        return None, None

def filter_data_by_period(df, period_days):
    """Filter data by number of days"""
    if period_days == "YTD":
        start_date = datetime(datetime.now().year, 1, 1)
        period_name = "Year-to-Date"
    else:
        start_date = datetime.now() - timedelta(days=period_days)
        period_name = f"{period_days}-Day"
    
    filtered = df[df['ReportDate'] >= start_date].copy()
    print(f"üìÖ {period_name} filter: {len(filtered)} records from {start_date.strftime('%Y-%m-%d')}")
    return filtered

def calculate_enhanced_stats(df):
    """Calculate comprehensive statistics with safe formatting"""
    if len(df) == 0:
        return {
            'total_incidents': 0,
            'zones_affected': 0,
            'top_crime': 'None',
            'top_crime_count': 0,
            'clearance_rate': 0,
            'crime_distribution': Counter(),
            'zone_distribution': Counter(),
            'hour_distribution': Counter(),
            'day_distribution': Counter()
        }
    
    # Basic statistics with safe conversion
    total_incidents = safe_int_format(len(df))
    zones_affected = safe_int_format(df['Zone'].nunique() if 'Zone' in df.columns else 1)
    
    # Crime analysis
    crime_counts = df['CrimeType'].value_counts()
    top_crime = crime_counts.index[0] if len(crime_counts) > 0 else "None"
    top_crime_count = safe_int_format(crime_counts.iloc[0] if len(crime_counts) > 0 else 0)
    
    # Clearance rate
    clearance_rate = (df['IsCleared'].sum() / total_incidents * 100) if total_incidents > 0 else 0
    
    # Temporal distributions
    hour_dist = df['Hour'].value_counts().sort_index()
    day_dist = df['DayName'].value_counts()
    zone_dist = df['Zone'].value_counts()
    
    return {
        'total_incidents': total_incidents,
        'zones_affected': zones_affected,
        'top_crime': top_crime,
        'top_crime_count': top_crime_count,
        'clearance_rate': round(clearance_rate, 1),
        'crime_distribution': crime_counts,
        'zone_distribution': zone_dist,
        'hour_distribution': hour_dist,
        'day_distribution': day_dist
    }

def create_markdown_table(data_dict, title, limit=10):
    """Create a markdown table from data with safe formatting"""
    if not data_dict or len(data_dict) == 0:
        return f"\n### {title}\n\nNo data available for this period.\n"
    
    table = f"\n### {title}\n\n"
    table += "| Item | Count | Percentage |\n"
    table += "|------|-------|------------|\n"
    
    total = sum(data_dict.values())
    
    for item, count in list(data_dict.items())[:limit]:
        # Safe formatting for all numeric values
        safe_count = safe_int_format(count)
        percentage = (count / total * 100) if total > 0 else 0
        
        # Clean up item names for display
        display_item = str(item).replace(' - 2C:', ' (2C:').replace('2C:', '').strip()
        if display_item.endswith(')'):
            display_item = display_item
        elif ' (2C:' in display_item:
            display_item += ')'
        
        table += f"| {display_item} | {safe_count} | {percentage:.1f}% |\n"
    
    return table

def generate_executive_narrative(stats, period_name):
    """Generate intelligent executive summary"""
    if stats['total_incidents'] == 0:
        return f"\n## Executive Summary\n\nNo incidents were recorded during the {period_name} reporting period.\n\n"
    
    narrative = f"\n## Executive Summary\n\n"
    narrative += f"During the {period_name} reporting period, the department responded to **{stats['total_incidents']} incidents** "
    narrative += f"across **{stats['zones_affected']} zones**. " if stats['top_crime'] != 'None':
        narrative += f"**{stats['top_crime']}** was the most frequent incident type with {stats['top_crime_count']} occurrences "
        narrative += f"({stats['top_crime_count']/stats['total_incidents']*100:.1f}% of total activity). " if stats['clearance_rate'] > 0:
        narrative += f"The department achieved a **{stats['clearance_rate']}% clearance rate** during this period. " if stats['clearance_rate'] >= 75:
            narrative += "This clearance rate demonstrates effective case resolution and investigative follow-through.\n\n"
        elif stats['clearance_rate'] >= 50:
            narrative += "This clearance rate indicates satisfactory case management with room for improvement.\n\n"
        else:
            narrative += "Current clearance rates suggest opportunities for enhanced investigative protocols.\n\n"
    
    # Geographic insights
    if len(stats['zone_distribution']) > 0:
        top_zones = list(stats['zone_distribution'].head(3).keys())
        if len(top_zones) > 1:
            zone_list = ', '.join([f"Zone {z}" for z in top_zones[:-1]]) + f" and Zone {top_zones[-1]}"
        else:
            zone_list = f"Zone {top_zones[0]}"
        narrative += f"**Geographic Focus:** Primary activity was concentrated in {zone_list}.\n\n"
    
    # Temporal insights
    if len(stats['hour_distribution']) > 0:
        peak_hours = stats['hour_distribution'].nlargest(3)
        if len(peak_hours) > 0:
            hour_list = ', '.join([f"{safe_int_format(h):02d}:00" for h in peak_hours.index[:2]])
            if len(peak_hours) > 2:
                hour_list += f" and {safe_int_format(peak_hours.index[2]):02d}:00"
            narrative += f"**Peak Activity:** Highest incident volumes occurred during {hour_list} hours.\n\n"
    
    return narrative

def generate_markdown_report(df, period_name, period_days, output_filename, column_map):
    """Generate comprehensive markdown report"""
    
    # Filter data for period
    filtered_df = filter_data_by_period(df, period_days)
    
    # Calculate statistics
    stats = calculate_enhanced_stats(filtered_df)
    
    # Build the report
    report = f"# SCRPA Crime Analysis - {period_name} Executive Summary\n\n"
    report += f"**Period:** {period_name} Ending {datetime.now().strftime('%B %d, %Y')}\n"
    report += f"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
    report += f"**Total Records Analyzed:** {len(filtered_df)}\n"
    report += f"**Data Source:** RMS Export with Auto-Detection\n\n"
    
    # Column mapping info
    report += "**Column Mappings Detected:**\n"
    for field, column in column_map.items():
        if column:
            report += f"- {field.replace('_', ' ').title()}: `{column}`\n"
    report += "\n---\n"
    
    # Executive Summary
    report += generate_executive_narrative(stats, period_name)
    
    # Key Metrics
    report += "## Key Metrics\n\n"
    report += "| Metric | Value |\n"
    report += "|--------|-------|\n"
    report += f"| Total Incidents | {safe_int_format(stats['total_incidents'])} |\n"
    report += f"| Zones Affected | {safe_int_format(stats['zones_affected'])} |\n"
    report += f"| Primary Incident Type | {stats['top_crime']} ({safe_int_format(stats['top_crime_count'])}) |\n"
    if stats['clearance_rate'] > 0:
        report += f"| Clearance Rate | {stats['clearance_rate']}% |\n"
    report += "\n"
    
    # Distribution tables
    if len(stats['crime_distribution']) > 0:
        report += create_markdown_table(stats['crime_distribution'], "Incident Type Distribution")
    
    if len(stats['zone_distribution']) > 0:
        report += create_markdown_table(stats['zone_distribution'], "Zone Activity Distribution")
    
    # Temporal analysis
    if len(stats['hour_distribution']) > 0:
        report += "\n### Hourly Distribution\n\n"
        report += "| Hour | Incidents | Percentage |\n"
        report += "|------|-----------|------------|\n"
        
        total_incidents = sum(stats['hour_distribution'].values())
        for hour in sorted(stats['hour_distribution'].keys()):
            count = stats['hour_distribution'][hour]
            percentage = (count / total_incidents * 100) if total_incidents > 0 else 0
            safe_hour = safe_int_format(hour)
            safe_count = safe_int_format(count)
            report += f"| {safe_hour:02d}:00 | {safe_count} | {percentage:.1f}% |\n"
        report += "\n"
    
    if len(stats['day_distribution']) > 0:
        report += "\n### Day of Week Distribution\n\n"
        report += "| Day | Incidents | Percentage |\n"
        report += "|-----|-----------|------------|\n"
        
        day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
        total_incidents = sum(stats['day_distribution'].values())
        
        for day in day_order:
            count = stats['day_distribution'].get(day, 0)
            percentage = (count / total_incidents * 100) if total_incidents > 0 else 0
            safe_count = safe_int_format(count)
            report += f"| {day} | {safe_count} | {percentage:.1f}% |\n"
        report += "\n"
    
    # Recent incidents
    if len(filtered_df) > 0:
        report += "## Recent Significant Incidents\n\n"
        recent = filtered_df.nlargest(5, 'ReportDate')
        
        for _, incident in recent.iterrows():
            report_num = incident.get('ReportNumber', 'N/A')
            crime_type = incident.get('CrimeType', 'Unknown')
            date_str = incident['ReportDate'].strftime('%m/%d/%Y %H:%M') if pd.notna(incident['ReportDate']) else 'Date Unknown'
            
            # Sanitize address
            address = incident.get('Address', 'Address Not Available')
            if isinstance(address, str) and len(address) > 10:
                # Replace specific numbers with XXX
                sanitized = re.sub(r'\d+', 'XXX', str(address)[:50])
                if ',' in sanitized:
                    parts = sanitized.split(',')
                    sanitized = parts[0] + (f", {parts[1].strip()}" if len(parts) > 1 else "")
            else:
                sanitized = "Location Withheld"
            
            report += f"- **{report_num}** - {crime_type} - {sanitized} - {date_str}\n"
        report += "\n"
    
    # Footer
    report += "---\n\n"
    report += "*Report generated by SCRPA Statistical Export System with Auto-Detection*\n"
    report += f"*Data processed on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n"
    report += "*Column mappings automatically detected from RMS export structure*\n"
    
    # Save the report
    os.makedirs(OUTPUT_FOLDER, exist_ok=True)
    report_path = os.path.join(OUTPUT_FOLDER, output_filename)
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"‚úÖ Report saved: {output_filename}")
    
    return {
        'file_path': report_path,
        'stats': stats,
        'period': period_name
    }

def main():
    """Main execution function"""
    print("üöÄ Starting Robust RMS Statistical Export...")
    
    # Get latest RMS file
    rms_file = get_latest_rms_file()
    if not rms_file:
        return False
    
    # Load data with auto-detection
    df, column_map = load_and_analyze_rms_data(rms_file)
    if df is None or len(df) == 0:
        print("‚ùå No usable data found - exiting")
        return False
    
    # Generate timestamp for filenames
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # Generate reports for all periods
    periods = [
        ("7-Day", 7, f"SCRPA_7Day_Executive_Summary_{timestamp}.md"),
        ("28-Day", 28, f"SCRPA_28Day_Executive_Summary_{timestamp}.md"),
        ("YTD", "YTD", f"SCRPA_YTD_Executive_Summary_{timestamp}.md")
    ]
    
    results = {}
    
    for period_name, period_days, filename in periods:
        print(f"\nüìä Generating {period_name} report...")
        try:
            result = generate_markdown_report(df, period_name, period_days, filename, column_map)
            results[period_name] = result
        except Exception as e:
            print(f"‚ùå Error generating {period_name} report: {e}")
            results[period_name] = None
    
    # Generate system summary
    summary_path = os.path.join(OUTPUT_FOLDER, f"System_Summary_{timestamp}.md")
    with open(summary_path, 'w', encoding='utf-8') as f:
        f.write("# SCRPA RMS Statistical Export - System Summary\n\n")
        f.write(f"**Execution Time:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"**RMS Source:** {os.path.basename(rms_file)}\n")
        f.write(f"**Total Records Processed:** {len(df)}\n")
        f.write(f"**Output Location:** {OUTPUT_FOLDER}\n\n")
        
        f.write("## Column Mappings Detected\n\n")
        for field, column in column_map.items():
            status = "‚úÖ" if column else "‚ùå"
            f.write(f"- {status} **{field.replace('_', ' ').title()}**: {column or 'Not Found'}\n")
        
        f.write("\n## Generated Reports\n\n")
        for period, result in results.items():
            if result:
                stats = result['stats']
                f.write(f"### {period}\n")
                f.write(f"- **File:** {os.path.basename(result['file_path'])}\n")
                f.write(f"- **Incidents:** {stats['total_incidents']}\n")
                f.write(f"- **Zones:** {stats['zones_affected']}\n")
                f.write(f"- **Top Crime:** {stats['top_crime']}\n")
                f.write(f"- **Clearance Rate:** {stats['clearance_rate']}%\n\n")
        
        f.write("## System Status\n")
        f.write("‚úÖ Auto-detection successfully mapped RMS columns\n")
        f.write("‚úÖ All period reports generated\n")
        f.write("‚úÖ Professional Markdown formatting applied\n")
        f.write("‚úÖ Statistical analysis completed\n")
        f.write("‚úÖ Ready for distribution and Power BI integration\n")
    
    print(f"\n‚úÖ System summary: {os.path.basename(summary_path)}")
    
    # Final summary
    print(f"\n{'='*60}")
    print("üéâ ROBUST RMS STATISTICAL EXPORT COMPLETE!")