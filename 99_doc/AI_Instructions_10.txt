# **Project Documentation for AI Agent**

## **Project Title:**

**AI Chat Log Chunker System — Automated Transcript Segmentation & Archival**

---

### **Project Purpose & Objectives**

* **Purpose:**
  Build an automated, enterprise-grade system that monitors a designated folder for raw AI chat transcripts, then processes, segments, and archives them in a structured, searchable, and privacy-compliant format.

* **Key Goals:**

  1. **Automatic Chunking**: Split large transcripts into manageable conversational segments, each ≤300 lines, always ending at a speaker turn.
  2. **Data Cleaning & Transformation**:

     * Tabular data is processed using Power Query M code (camelCase).
     * NLP/text data uses Python scripts (snake\_case).
     * All code must be clearly commented.
  3. **Metadata Generation**:

     * For every chunk, produce a metadata JSON including:

       * `timestampRange: [startISO, endISO]`
       * `primaryTopics`: 2–3 themes
       * `participants`: list of speaker names
  4. **Output Generation**:

     * For each chunk:

       * Cleaned transcript (`.txt`)
       * 2–3 sentence summary (`.md`)
       * Transformation scripts as needed (`.m`, `.py`)
       * Metadata (`.json`)
  5. **Consolidation & Quality Control**:

     * Combine all summaries into `CombinedSummary.md` (split sequentially if >5,000 words).
     * Merge all cleaned transcripts into `CompleteTranscript.md` (each chunk starts with filename header).
     * Flag duplicates, nulls, or format issues, and recommend fixes.
  6. **Security & Compliance**:

     * Encrypt/redact sensitive data per department policy.
     * Maintain an audit trail and comply with organizational privacy/version-control standards.
  7. **System Monitoring**:

     * Provide processing statistics, error analytics, and system health (e.g., CPU/memory/disk/process count).
     * Alert via notification system for failures or anomalies.

---

### **System Workflow**

* Watch a folder (`C:\_chunker` or as set in config) for new files matching `"*chunk*"` in the filename.
* When a file is stable and meets size/minimum requirements:

  * Process through chunking/cleaning pipeline.
  * Generate all outputs (chunked transcripts, summaries, metadata, scripts).
  * Log all processing, errors, and system metrics to SQLite database.
  * Sync results to cloud location if configured.
  * Notify on completion/errors.
* Consolidate outputs, QC, and maintain a manifest of all generated files.
* Optionally, send daily/weekly analytics reports to admins.

---

### **System Conventions**

* Use **explicit, unambiguous language** in outputs.
* **No zero-byte outputs.** Validation at every stage.
* Encrypted/redacted outputs for confidential data.
* Audit and log every processing event.
* Modular, testable, and well-commented code.
* All configuration is managed via `config.json` and is environment-adaptable.

---

### **References for Agents**

* The core processing engine is in `watcher_splitter.py` or `watcher_splitter_enterprise.py`.
* Security, encryption, and access are managed in `security_module.py`.
* System stats and error logging are tracked via `chunker_database.py`.
* Notification and reporting handled by `notification_system.py`.
* Configuration is defined in `config.json` and may have department-specific overrides.
* PowerShell scripts like `test_chunker.ps1` and batch files automate deployment and monitoring.

---

## **Sample Prompts for Efficient AI Operation**

**Prompt 1: General Processing Task**

```plaintext
You are tasked with processing a newly added AI chat transcript located in C:\_chunker\2025-08-14_example_chunkfile.txt. 
- Chunk the transcript into ≤300-line segments, ending only at speaker turn boundaries.
- For each chunk, produce:
  • A cleaned transcript (.txt)
  • 2–3 sentence summary (.md)
  • Metadata JSON (with timestamp range, topics, participants)
  • Any transformation scripts required (.m, .py), commented
- When done, consolidate all summaries into CombinedSummary.md and all cleaned transcripts into CompleteTranscript.md.
- Generate a manifest JSON listing all output files.
- Flag and report any duplicates, nulls, or format issues found.
- Encrypt or redact sensitive data per department policy.
- Log all processing events.
```

---

**Prompt 2: Focused Chunk Validation & QC**

```plaintext
Review the generated chunk outputs in the /output folder:
- Validate that all chunks have sufficient content (≥10 words, ≥50 characters).
- Ensure no chunk is a zero-byte file.
- Check each chunk for duplicates or excessive whitespace.
- For any chunk that fails validation, flag it and recommend a fix.
- Summarize the QC results in a markdown report.
```

---

**Prompt 3: Onboarding/Configuration Audit**

```plaintext
Review the current chunker configuration (config.json):
- List the key operational parameters (watch_folder, output_dir, cloud_repo_root, chunk_size, etc.).
- Check for misconfigurations or missing recommended fields.
- Suggest configuration improvements for security, efficiency, and compliance.
```

---

**Prompt 4: Analytics & Reporting**

```plaintext
Using the chunker database, produce a weekly summary:
- Files processed, chunks created, errors encountered, zero-byte files prevented, and average processing time per file.
- Highlight any trends or anomalies.
- Prepare a markdown report for administrators.
```

---

**Prompt 5: Security & Compliance Check**

```plaintext
Review recent chunker activity for compliance:
- Ensure all outputs containing sensitive data are properly redacted or encrypted.
- Verify that all processing and access events are logged in the audit trail.
- Summarize any compliance gaps found and recommend corrective actions.
```

---

**Prompt 6: Error/Alert Notification**

```plaintext
A critical processing error was detected in the latest run.
- Review the watcher log and system error reports.
- Identify the cause and affected file(s).
- Draft a clear summary and recommended actions for the system administrator.
```

---

## **How to Use These Prompts**

* **For day-to-day operations:** Use Prompt 1 or Prompt 2 as needed.
* **For onboarding or tuning the system:** Use Prompt 3.
* **For regular reporting:** Use Prompt 4 or Prompt 5.
* **When troubleshooting or after failures:** Use Prompt 6.

