",
        "retrieved_docs": ["excel_guide.md", "troubleshooting.xlsx"],
        "relevant_docs": ["excel_guide.md", "troubleshooting.xlsx"]
    }
    
    # Run evaluation
    metrics = evaluator.evaluate_end_to_end(
        test_case["query"],
        test_case["answer"],
        test_case["context"],
        test_case["retrieved_docs"],
        test_case["relevant_docs"]
    )
    
    print("Evaluation Results:")
    for metric, value in metrics.items():
        print(f"{metric}: {value:.3f}")
    
    print("RAG evaluation test completed successfully!") ```

Now let me create the LangSmith integration:


```python
"""
LangSmith Integration for Chunker_v2
Provides tracing, evaluation, and feedback collection for RAG pipeline
"""

import os
import logging
import json
from typing import List, Dict, Any, Optional, Callable
from datetime import datetime
from langsmith import Client, traceable
from langsmith.evaluation import evaluate
from langsmith.schemas import Example, Run
import time

logger = logging.getLogger(__name__)

class LangSmithIntegration:
    """
    LangSmith integration for tracing and evaluation.
    """ def __init__(self, api_key: str = None, project: str = "chunker-rag-eval"):
        """
        Initialize LangSmith integration. Args:
            api_key: LangSmith API key
            project: Project name for tracing
        """
        self.api_key = api_key or os.getenv("LANGCHAIN_API_KEY")
        self.project = project
        
        if not self.api_key:
            logger.warning("LangSmith API key not provided. Tracing will be disabled.") self.client = None
        else:
            try:
                os.environ["LANGCHAIN_API_KEY"] = self.api_key
                os.environ["LANGCHAIN_PROJECT"] = self.project
                self.client = Client()
                logger.info(f"Initialized LangSmith client for project: {self.project}")
            except Exception as e:
                logger.error(f"Failed to initialize LangSmith client: {e}")
                self.client = None
    
    @traceable(run_type="chain")
    def traced_rag_query(self, query: str, rag_system, llm=None) -> Dict[str, Any]:
        """
        Traced RAG query with LangSmith tracing. Args:
            query: Input query
            rag_system: RAG system for retrieval
            llm: Optional LLM for answer generation
            
        Returns:
            Query results with tracing
        """
        try:
            # Retrieve relevant documents
            retrieved_results = rag_system.hybrid_search(query, top_k=5)
            
            # Generate context
            context = "\n".join([r["content"] for r in retrieved_results])
            
            # Generate answer
            if llm:
                answer = llm.invoke(f"Context: {context}\nQuery: {query}")
            else:
                answer = f"Based on the context: {context[:200]}..."
            
            return {
                "query": query,
                "answer": answer,
                "context": context,
                "retrieved_docs": [r["metadata"] for r in retrieved_results],
                "success": True
            }
            
        except Exception as e:
            logger.error(f"Traced RAG query failed: {e}")
            return {
                "query": query,
                "answer": "",
                "context": "",
                "retrieved_docs": [],
                "success": False,
                "error": str(e)
            }
    
    def create_feedback(self, run_id: str, score: float, comment: str = "", key: str = "user_feedback") -> bool:
        """
        Create feedback for a run. Args:
            run_id: LangSmith run ID
            score: Feedback score (0-1)
            comment: Optional comment
            key: Feedback key
            
        Returns:
            True if successful, False otherwise
        """
        if not self.client:
            logger.warning("LangSmith client not available")
            return False
        
        try:
            self.client.create_feedback(
                run_id=run_id,
                key=key,
                score=score,
                comment=comment
            )
            logger.info(f"Created feedback for run {run_id}: {score}")
            return True
        except Exception as e:
            logger.error(f"Failed to create feedback: {e}")
            return False
    
    def evaluate_retrieval(self, run: Run, example: Example) -> Dict[str, float]:
        """
        Evaluate retrieval quality. Args:
            run: LangSmith run
            example: Example with expected outputs
            
        Returns:
            Retrieval evaluation metrics
        """
        try:
            retrieved_docs = run.outputs.get("retrieved_docs", [])
            expected_sources = example.outputs.get("expected_sources", [])
            
            if not retrieved_docs or not expected_sources:
                return {"precision": 0.0, "recall": 0.0}
            
            # Calculate precision and recall
            retrieved_sources = [doc.get("source_file", "") for doc in retrieved_docs]
            relevant_count = sum(1 for source in retrieved_sources if source in expected_sources)
            
            precision = relevant_count / len(retrieved_sources) if retrieved_sources else 0.0
            recall = relevant_count / len(expected_sources) if expected_sources else 0.0
            
            return {
                "precision": precision,
                "recall": recall,
                "f1": 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
            }
            
        except Exception as e:
            logger.error(f"Retrieval evaluation failed: {e}")
            return {"precision": 0.0, "recall": 0.0, "f1": 0.0}
    
    def evaluate_answer_relevance(self, run: Run, example: Example) -> Dict[str, float]:
        """
        Evaluate answer relevance. Args:
            run: LangSmith run
            example: Example with expected outputs
            
        Returns:
            Answer relevance metrics
        """
        try:
            generated_answer = run.outputs.get("answer", "")
            expected_answer = example.outputs.get("expected_answer", "")
            
            if not generated_answer or not expected_answer:
                return {"relevance": 0.0}
            
            # Simple keyword-based relevance (can be enhanced with embeddings)
            generated_words = set(generated_answer.lower().split())
            expected_words = set(expected_answer.lower().split())
            
            overlap = len(generated_words & expected_words)
            relevance = overlap / len(expected_words) if expected_words else 0.0
            
            return {"relevance": relevance}
            
        except Exception as e:
            logger.error(f"Answer relevance evaluation failed: {e}")
            return {"relevance": 0.0}
    
    def evaluate_context_relevance(self, run: Run, example: Example) -> Dict[str, float]:
        """
        Evaluate context relevance. Args:
            run: LangSmith run
            example: Example with expected outputs
            
        Returns:
            Context relevance metrics
        """
        try:
            context = run.outputs.get("context", "")
            query = run.inputs.get("query", "")
            
            if not context or not query:
                return {"context_relevance": 0.0}
            
            # Simple keyword-based relevance (can be enhanced with embeddings)
            context_words = set(context.lower().split())
            query_words = set(query.lower().split())
            
            overlap = len(context_words & query_words)
            relevance = overlap / len(query_words) if query_words else 0.0
            
            return {"context_relevance": relevance}
            
        except Exception as e:
            logger.error(f"Context relevance evaluation failed: {e}")
            return {"context_relevance": 0.0}
    
    def run_evaluation(self, examples: List[Example], rag_func: Callable, 
                      experiment_prefix: str = "rag-eval") -> Dict[str, Any]:
        """
        Run evaluation on examples. Args:
            examples: List of examples to evaluate
            rag_func: RAG function to evaluate
            experiment_prefix: Prefix for experiment name
            
        Returns:
            Evaluation results
        """
        if not self.client:
            logger.warning("LangSmith client not available. Skipping evaluation.") return {"error": "LangSmith client not available"}
        
        try:
            # Define evaluators
            evaluators = [
                self.evaluate_retrieval,
                self.evaluate_answer_relevance,
                self.evaluate_context_relevance
            ]
            
            # Run evaluation
            results = evaluate(
                rag_func,
                data=examples,
                evaluators=evaluators,
                experiment_prefix=experiment_prefix
            )
            
            logger.info(f"Evaluation completed: {experiment_prefix}")
            return results
            
        except Exception as e:
            logger.error(f"Evaluation failed: {e}")
            return {"error": str(e)}
    
    def load_evaluation_dataset(self, file_path: str) -> List[Example]:
        """
        Load evaluation dataset from JSON file. Args:
            file_path: Path to JSON file
            
        Returns:
            List of Example objects
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            examples = []
            for item in data:
                example = Example(
                    inputs={"query": item["query"]},
                    outputs={
                        "expected_answer": item.get("expected_answer", ""),
                        "expected_sources": item.get("expected_sources", [])
                    }
                )
                examples.append(example)
            
            logger.info(f"Loaded {len(examples)} examples from {file_path}")
            return examples
            
        except Exception as e:
            logger.error(f"Failed to load evaluation dataset: {e}")
            return []
    
    def create_evaluation_dataset(self, test_queries: List[Dict[str, Any]], output_path: str) -> None:
        """
        Create evaluation dataset file. Args:
            test_queries: List of test queries
            output_path: Output file path
        """
        try:
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(test_queries, f, indent=2, ensure_ascii=False)
            
            logger.info(f"Created evaluation dataset: {output_path}")
            
        except Exception as e:
            logger.error(f"Failed to create evaluation dataset: {e}")
    
    def get_run_traces(self, run_id: str) -> Optional[Dict[str, Any]]:
        """
        Get traces for a specific run. Args:
            run_id: LangSmith run ID
            
        Returns:
            Run traces or None if failed
        """
        if not self.client:
            logger.warning("LangSmith client not available")
            return None
        
        try:
            run = self.client.read_run(run_id)
            return {
                "run_id": run.id,
                "name": run.name,
                "start_time": run.start_time,
                "end_time": run.end_time,
                "status": run.status,
                "inputs": run.inputs,
                "outputs": run.outputs,
                "error": run.error
            }
        except Exception as e:
            logger.error(f"Failed to get run traces: {e}")
            return None
    
    def get_project_stats(self) -> Dict[str, Any]:
        """
        Get project statistics. Returns:
            Project statistics
        """
        if not self.client:
            logger.warning("LangSmith client not available")
            return {"error": "LangSmith client not available"}
        
        try:
            # Get project runs
            runs = list(self.client.list_runs(project_name=self.project, limit=100))
            
            stats = {
                "total_runs": len(runs),
                "successful_runs": sum(1 for run in runs if run.status == "success"),
                "failed_runs": sum(1 for run in runs if run.status == "error"),
                "project_name": self.project,
                "last_updated": datetime.now().isoformat()
            }
            
            return stats
            
        except Exception as e:
            logger.error(f"Failed to get project stats: {e}")
            return {"error": str(e)}

def initialize_langsmith(api_key: str = None, project: str = "chunker-rag-eval") -> LangSmithIntegration:
    """
    Initialize LangSmith integration. Args:
        api_key: LangSmith API key
        project: Project name
        
    Returns:
        LangSmithIntegration instance
    """
    return LangSmithIntegration(api_key=api_key, project=project)

def create_sample_evaluation_dataset() -> List[Dict[str, Any]]:
    """
    Create sample evaluation dataset. Returns:
        List of sample test queries
    """
    return [
        {
            "query": "How do I fix vlookup errors in Excel? ",
            "expected_answer": "Check data types and table references for vlookup errors. ",
            "expected_sources": ["excel_guide.md", "troubleshooting.xlsx"],
            "category": "excel"
        },
        {
            "query": "What is Power BI query syntax? ",
            "expected_answer": "Power BI uses M language for Power Query transformations. ",
            "expected_sources": ["powerbi_guide.md", "m_language_reference.md"],
            "category": "powerbi"
        },
        {
            "query": "How do I use pandas for data analysis? ",
            "expected_answer": "Pandas provides data structures and tools for data analysis. ",
            "expected_sources": ["python_guide.md", "pandas_tutorial.md"],
            "category": "python"
        }
    ]

# Example usage
if __name__ == "__main__":
    # Initialize LangSmith
    langsmith = initialize_langsmith()
    
    # Create sample dataset
    sample_dataset = create_sample_evaluation_dataset()
    
    # Save dataset
    langsmith.create_evaluation_dataset(sample_dataset, "test_queries.json")
    
    # Load dataset
    examples = langsmith.load_evaluation_dataset("test_queries.json")
    print(f"Loaded {len(examples)} examples")
    
    # Get project stats
    stats = langsmith.get_project_stats()
    print(f"Project stats: {stats}")
    
    print("LangSmith integration test completed successfully!") ```

Now let me create the automated evaluation pipeline:


```python
"""
Automated RAG Evaluation Pipeline for Chunker_v2
Runs evaluations on schedule and generates reports
"""

import os
import json
import logging
import schedule
import time
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
import pandas as pd
from pathlib import Path

logger = logging.getLogger(__name__)

class AutomatedEvaluator:
    """
    Automated evaluation system for RAG pipeline.
    """ def __init__(self, config: Dict[str, Any]):
        """
        Initialize automated evaluator. Args:
            config: Configuration dictionary
        """
        self.config = config
        self.evaluation_dir = config.get("evaluation_dir", "./evaluations")
        self.reports_dir = config.get("reports_dir", "./reports")
        self.baseline_file = config.get("baseline_file", "./baseline_metrics.json")
        self.alert_thresholds = config.get("alert_thresholds", {})
        
        # Create directories
        os.makedirs(self.evaluation_dir, exist_ok=True)
        os.makedirs(self.reports_dir, exist_ok=True)
        
        # Load baseline metrics
        self.baseline_metrics = self._load_baseline_metrics()
        
        logger.info("Initialized automated evaluator")
    
    def _load_baseline_metrics(self) -> Dict[str, float]:
        """
        Load baseline metrics for comparison. Returns:
            Baseline metrics dictionary
        """
        try:
            if os.path.exists(self.baseline_file):
                with open(self.baseline_file, 'r') as f:
                    baseline = json.load(f)
                logger.info("Loaded baseline metrics")
                return baseline
            else:
                logger.info("No baseline metrics found, will create after first evaluation")
                return {}
        except Exception as e:
            logger.error(f"Failed to load baseline metrics: {e}")
            return {}
    
    def _save_baseline_metrics(self, metrics: Dict[str, float]) -> None:
        """
        Save baseline metrics. Args:
            metrics: Metrics to save as baseline
        """
        try:
            with open(self.baseline_file, 'w') as f:
                json.dump(metrics, f, indent=2)
            logger.info("Saved baseline metrics")
        except Exception as e:
            logger.error(f"Failed to save baseline metrics: {e}")
    
    def run_evaluation(self, rag_system, test_queries: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Run evaluation on test queries. Args:
            rag_system: RAG system to evaluate
            test_queries: List of test queries
            
        Returns:
            Evaluation results
        """
        try:
            from rag_evaluation import RAGEvaluator, run_evaluation_pipeline
            
            # Initialize evaluator
            evaluator = RAGEvaluator()
            
            # Run evaluation pipeline
            results = run_evaluation_pipeline(test_queries, rag_system, evaluator)
            
            # Calculate aggregate metrics
            aggregate_metrics = self._calculate_aggregate_metrics(results)
            
            # Check for regressions
            regression_alerts = self._check_regressions(aggregate_metrics)
            
            # Generate report
            report = self._generate_report(results, aggregate_metrics, regression_alerts)
            
            # Save results
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            results_file = os.path.join(self.evaluation_dir, f"evaluation_{timestamp}.json")
            self._save_evaluation_results(results, results_file)
            
            # Save report
            report_file = os.path.join(self.reports_dir, f"report_{timestamp}.json")
            self._save_report(report, report_file)
            
            # Update baseline if this is the first evaluation
            if not self.baseline_metrics:
                self.baseline_metrics = aggregate_metrics
                self._save_baseline_metrics(aggregate_metrics)
                logger.info("Set initial baseline metrics")
            
            # Send alerts if needed
            if regression_alerts:
                self._send_alerts(regression_alerts)
            
            logger.info(f"Evaluation completed: {timestamp}")
            return report
            
        except Exception as e:
            logger.error(f"Evaluation failed: {e}")
            return {"error": str(e)}
    
    def _calculate_aggregate_metrics(self, results: List[Dict[str, Any]]) -> Dict[str, float]:
        """
        Calculate aggregate metrics from individual results. Args:
            results: List of individual evaluation results
            
        Returns:
            Aggregate metrics dictionary
        """
        if not results:
            return {}
        
        # Extract numeric metrics
        numeric_metrics = {}
        for result in results:
            if result.get("success", True):  # Skip failed results
                for key, value in result.items():
                    if isinstance(value, (int, float)) and key not in ["test_case_id"]:
                        if key not in numeric_metrics:
                            numeric_metrics[key] = []
                        numeric_metrics[key].append(value)
        
        # Calculate aggregate statistics
        aggregate_metrics = {}
        for metric, values in numeric_metrics.items():
            if values:
                aggregate_metrics[f"{metric}_mean"] = sum(values) / len(values)
                aggregate_metrics[f"{metric}_min"] = min(values)
                aggregate_metrics[f"{metric}_max"] = max(values)
                aggregate_metrics[f"{metric}_count"] = len(values)
        
        return aggregate_metrics
    
    def _check_regressions(self, current_metrics: Dict[str, float]) -> List[Dict[str, Any]]:
        """
        Check for regressions compared to baseline. Args:
            current_metrics: Current evaluation metrics
            
        Returns:
            List of regression alerts
        """
        alerts = []
        
        if not self.baseline_metrics:
            return alerts
        
        for metric, current_value in current_metrics.items():
            if metric.endswith("_mean") and metric in self.baseline_metrics:
                baseline_value = self.baseline_metrics[metric]
                
                # Calculate percentage change
                if baseline_value > 0:
                    change_percent = ((current_value - baseline_value) / baseline_value) * 100
                    
                    # Check if regression exceeds threshold
                    threshold = self.alert_thresholds.get(metric, 10.0)  # Default 10% threshold
                    
                    if change_percent < -threshold:
                        alerts.append({
                            "metric": metric,
                            "baseline": baseline_value,
                            "current": current_value,
                            "change_percent": change_percent,
                            "threshold": threshold,
                            "severity": "high" if change_percent < -20 else "medium"
                        })
        
        return alerts
    
    def _generate_report(self, results: List[Dict[str, Any]], 
                        aggregate_metrics: Dict[str, float], 
                        regression_alerts: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Generate evaluation report. Args:
            results: Individual evaluation results
            aggregate_metrics: Aggregate metrics
            regression_alerts: Regression alerts
            
        Returns:
            Evaluation report
        """
        report = {
            "timestamp": datetime.now().isoformat(),
            "total_queries": len(results),
            "successful_queries": sum(1 for r in results if r.get("success", True)),
            "failed_queries": sum(1 for r in results if not r.get("success", True)),
            "aggregate_metrics": aggregate_metrics,
            "regression_alerts": regression_alerts,
            "baseline_metrics": self.baseline_metrics,
            "summary": self._generate_summary(aggregate_metrics, regression_alerts)
        }
        
        return report
    
    def _generate_summary(self, aggregate_metrics: Dict[str, float], 
                         regression_alerts: List[Dict[str, Any]]) -> str:
        """
        Generate human-readable summary. Args:
            aggregate_metrics: Aggregate metrics
            regression_alerts: Regression alerts
            
        Returns:
            Summary string
        """
        summary_parts = []
        
        # Overall performance
        if "precision_at_5_mean" in aggregate_metrics:
            precision = aggregate_metrics["precision_at_5_mean"]
            summary_parts.append(f"Average Precision@5: {precision:.3f}")
        
        if "recall_at_5_mean" in aggregate_metrics:
            recall = aggregate_metrics["recall_at_5_mean"]
            summary_parts.append(f"Average Recall@5: {recall:.3f}")
        
        if "faithfulness_mean" in aggregate_metrics:
            faithfulness = aggregate_metrics["faithfulness_mean"]
            summary_parts.append(f"Average Faithfulness: {faithfulness:.3f}")
        
        # Regression alerts
        if regression_alerts:
            summary_parts.append(f"⚠️ {len(regression_alerts)} regression(s) detected")
        else:
            summary_parts.append("✅ No regressions detected")
        
        return " | ".join(summary_parts)
    
    def _save_evaluation_results(self, results: List[Dict[str, Any]], file_path: str) -> None:
        """
        Save evaluation results to file. Args:
            results: Evaluation results
            file_path: Output file path
        """
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            logger.info(f"Saved evaluation results to {file_path}")
        except Exception as e:
            logger.error(f"Failed to save evaluation results: {e}")
    
    def _save_report(self, report: Dict[str, Any], file_path: str) -> None:
        """
        Save evaluation report to file. Args:
            report: Evaluation report
            file_path: Output file path
        """
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            logger.info(f"Saved evaluation report to {file_path}")
        except Exception as e:
            logger.error(f"Failed to save evaluation report: {e}")
    
    def _send_alerts(self, alerts: List[Dict[str, Any]]) -> None:
        """
        Send regression alerts. Args:
            alerts: List of regression alerts
        """
        try:
            # Log alerts
            for alert in alerts:
                logger.warning(f"Regression detected: {alert['metric']} "
                             f"({alert['change_percent']:.1f}% change)")
            
            # TODO: Implement email/Slack notifications
            # This would integrate with the existing notification system
            
        except Exception as e:
            logger.error(f"Failed to send alerts: {e}")
    
    def generate_csv_report(self, days: int = 7) -> str:
        """
        Generate CSV report for the last N days. Args:
            days: Number of days to include
            
        Returns:
            Path to generated CSV file
        """
        try:
            # Find evaluation files from the last N days
            cutoff_date = datetime.now() - timedelta(days=days)
            evaluation_files = []
            
            for file in os.listdir(self.evaluation_dir):
                if file.startswith("evaluation_") and file.endswith(".json"):
                    file_path = os.path.join(self.evaluation_dir, file)
                    file_time = datetime.fromtimestamp(os.path.getmtime(file_path))
                    if file_time >= cutoff_date:
                        evaluation_files.append(file_path)
            
            # Load and combine data
            all_results = []
            for file_path in evaluation_files:
                with open(file_path, 'r') as f:
                    results = json.load(f)
                    all_results.extend(results)
            
            # Convert to DataFrame
            df = pd.DataFrame(all_results)
            
            # Generate CSV
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            csv_path = os.path.join(self.reports_dir, f"evaluation_summary_{timestamp}.csv")
            df.to_csv(csv_path, index=False)
            
            logger.info(f"Generated CSV report: {csv_path}")
            return csv_path
            
        except Exception as e:
            logger.error(f"Failed to generate CSV report: {e}")
            return ""
    
    def cleanup_old_files(self, days: int = 30) -> None:
        """
        Clean up old evaluation files. Args:
            days: Number of days to keep files
        """
        try:
            cutoff_date = datetime.now() - timedelta(days=days)
            
            # Clean evaluation files
            for file in os.listdir(self.evaluation_dir):
                file_path = os.path.join(self.evaluation_dir, file)
                if os.path.isfile(file_path):
                    file_time = datetime.fromtimestamp(os.path.getmtime(file_path))
                    if file_time < cutoff_date:
                        os.remove(file_path)
                        logger.info(f"Removed old evaluation file: {file}")
            
            # Clean report files
            for file in os.listdir(self.reports_dir):
                file_path = os.path.join(self.reports_dir, file)
                if os.path.isfile(file_path):
                    file_time = datetime.fromtimestamp(os.path.getmtime(file_path))
                    if file_time < cutoff_date:
                        os.remove(file_path)
                        logger.info(f"Removed old report file: {file}")
            
        except Exception as e:
            logger.error(f"Failed to cleanup old files: {e}")

def run_scheduled_evaluation(config: Dict[str, Any], rag_system, test_queries: List[Dict[str, Any]]) -> None:
    """
    Run scheduled evaluation.