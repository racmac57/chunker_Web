## �� **Summary**

- **`output/`** = **Processed content** (chunks + complete transcript)
- **`processed/`** = **Original source files** (moved after processing)
- **`processed/admin/`** = **Admin-categorized original files**

The script creates both individual chunks AND a complete combined transcript, giving you flexibility to work with either the smaller pieces or the full document!

---

**User**

Lets:
1. Update the script that all chunks and combined chunk files are added to a folder which has the name of the file being processed.
2. Make the final admin output .md
3. relocate "C:\_chunker\01_scripts\new_chunker_project" to C:\_chunker.  I assume the files currently in the C:\_chunker should be moved to the new folder I made "C:\_chunker\03_Archive"

---

**Cursor**

Perfect! Let's implement these improvements step by step. I'll help you:

1. **Organize chunks by source file name**
2. **Convert admin output to .md format**
3. **Relocate the project to the new structure**

Let me start by updating the script to organize chunks by source file name:


Now let me update the script to organize chunks by source file name and convert admin output to .md:


```python
        # Prepare output
        timestamp = datetime.now().strftime("%Y_%m_%d_%H_%M_%S")
        clean_base = Path(file_path.name).stem.replace(" ", "_")
        output_folder = config.get("output_dir", "output")
        chunk_files = []
        valid_chunks = 0
        total_chunk_size = 0


        # Write chunks with validation
        for i, chunk in enumerate(chunks):
            if validate_chunk_content_enhanced(chunk, department_config=department_config):
                chunk_file = Path(output_folder) / f"{timestamp}_{clean_base}_chunk{i+1}.txt"
                try:
                    os.makedirs(output_folder, exist_ok=True)
                    with open(chunk_file, "w", encoding="utf-8") as cf:
                        cf.write(chunk)
                    # Verify file was written correctly
                    written_size = os.path.getsize(chunk_file)
                    if written_size > 0:
                        chunk_files.append(chunk_file)
                        valid_chunks += 1
                        total_chunk_size += written_size
                        logger.info(f"Created chunk: {chunk_file.name} ({len(chunk)} chars, {written_size} bytes)")
                    else:
                        logger.warning(f"Zero-byte chunk prevented: {chunk_file.name}")
                        session_stats["zero_byte_prevented"] += 1
                        os.remove(chunk_file)
                except Exception as e:
                    logger.error(f"Failed to write chunk {i+1} for {file_path.name}: {e}")
                    if db:
                        try:
                            db.log_error("ChunkWriteError", str(e), traceback.format_exc(), str(file_path))
                        except Exception as db_error:
                            logger.warning(f"Failed to log chunk write error to database: {db_error}")
            else:
                logger.warning(f"Invalid chunk {i+1} skipped for {file_path.name}")

        # Concatenate all chunk files into a final transcript
        if chunk_files:
            transcript_file = Path(output_folder) / f"{timestamp}_{clean_base}_transcript.txt"
            try:
                with open(transcript_file, "w", encoding="utf-8") as tf:
                    for chunk_file in chunk_files:
                        with open(chunk_file, "r", encoding="utf-8") as cf:
                            tf.write(cf.read())
                            tf.write("\n\n")
                logger.info(f"Final transcript created: {transcript_file.name}")
            except Exception as e:
                logger.error(f"Failed to create final transcript for {file_path.name}: {e}")

->

        # Prepare output with organized folder structure
        timestamp = datetime.now().strftime("%Y_%m_%d_%H_%M_%S")
        clean_base = Path(file_path.name).stem.replace(" ", "_")
        output_folder = config.get("output_dir", "output")
        
        # Create folder named after the source file
        file_output_folder = Path(output_folder) / clean_base
        os.makedirs(file_output_folder, exist_ok=True)
        
        chunk_files = []
        valid_chunks = 0
        total_chunk_size = 0

        # Write chunks with validation
        for i, chunk in enumerate(chunks):
            if validate_chunk_content_enhanced(chunk, department_config=department_config):
                chunk_file = file_output_folder / f"{timestamp}_{clean_base}_chunk{i+1}.txt"
                try:
                    with open(chunk_file, "w", encoding="utf-8") as cf:
                        cf.write(chunk)
                    # Verify file was written correctly
                    written_size = os.path.getsize(chunk_file)
                    if written_size > 0:
                        chunk_files.append(chunk_file)
                        valid_chunks += 1
                        total_chunk_size += written_size
                        logger.info(f"Created chunk: {chunk_file.name} ({len(chunk)} chars, {written_size} bytes)")
                    else:
                        logger.warning(f"Zero-byte chunk prevented: {chunk_file.name}")
                        session_stats["zero_byte_prevented"] += 1
                        os.remove(chunk_file)
                except Exception as e:
                    logger.error(f"Failed to write chunk {i+1} for {file_path.name}: {e}")
                    if db:
                        try:
                            db.log_error("ChunkWriteError", str(e), traceback.format_exc(), str(file_path))
                        except Exception as db_error:
                            logger.warning(f"Failed to log chunk write error to database: {db_error}")
            else:
                logger.warning(f"Invalid chunk {i+1} skipped for {file_path.name}")

        # Concatenate all chunk files into a final transcript
        if chunk_files:
            # Use .md extension for admin files, .txt for others
            if department == "admin":
                transcript_file = file_output_folder / f"{timestamp}_{clean_base}_transcript.md"
            else:
                transcript_file = file_output_folder / f"{timestamp}_{clean_base}_transcript.txt"
            
            try:
                with open(transcript_file, "w", encoding="utf-8") as tf:
                    # Add markdown header for admin files
                    if department == "admin":
                        tf.write(f"# {clean_base.replace('_', ' ').title()}\n\n")
                        tf.write(f"**Processing Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                        tf.write(f"**Source File:** {file_path.name}\n")
                        tf.write(f"**Total Chunks:** {len(chunk_files)}\n\n")
                        tf.write("---\n\n")
                    
                    for chunk_file in chunk_files:
                        with open(chunk_file, "r", encoding="utf-8") as cf:
                            tf.write(cf.read())
                            tf.write("\n\n")
                logger.info(f"Final transcript created: {transcript_file.name}")
            except Exception as e:
                logger.error(f"Failed to create final transcript for {file_path.name}: {e}")
```

Now let me also update the cloud copy function to work with the new folder structure:


```python
        # Cloud copy with retry
        cloud_success = False
        if config.get("cloud_repo_root"):
            cloud_dir = Path(config["cloud_repo_root"]) / f"{timestamp}_{clean_base}"
            for attempt in range(3):
                if copy_to_cloud_enhanced(chunk_files, cloud_dir, department_config):
                    logger.info(f"Cloud sync successful: {cloud_dir}")
                    cloud_success = True
                    break
                logger.warning(f"Cloud sync attempt {attempt+1} failed for {file_path.name}")
                time.sleep(2)

->

        # Cloud copy with retry
        cloud_success = False
        if config.get("cloud_repo_root"):
            cloud_dir = Path(config["cloud_repo_root"]) / clean_base
            for attempt in range(3):
                if copy_to_cloud_enhanced(chunk_files, cloud_dir, department_config):
                    logger.info(f"Cloud sync successful: {cloud_dir}")
                    cloud_success = True
                    break
                logger.warning(f"Cloud sync attempt {attempt+1} failed for {file_path.name}")
                time.sleep(2)
```

Now let's create the migration script to move the project to the new location:


```python