# Chunk 14 Summary

'),
        ('nltk_data', 'nltk_data')  # Include NLTK punkt tokenizer data
    ],
    hiddenimports=[
        'nltk.tokenize.punkt',
        'nltk.tokenize',
        'nltk.data'
    ],
    hookspath=[],
    runtime_hooks=[],
    excludes=[],
    win_no_prefer_redirects=False,
    win_private_assemblies=False,
    cipher=block_cipher,
    noarchive=False,
)

pyz = PYZ(a.pure, a.zipped_data, cipher=block_cipher)

exe = EXE(
    pyz,
    a.scripts,
    a.binaries,
    a.zipfiles,
    a.datas,
    [],
    name='chunker_hidden',
    debug=False,
    bootloader_ignore_signals=False,
    strip=False,
    upx=True,
    upx_exclude=[],
    runtime_tmpdir=None,
    console=False,  # Hidden console for background operation
    disable_windowed_traceback=False,
    target_arch=None,
    codesign_identity=None,
    entitlements_file=None,
)

# üïí 2025-06-28-01-15-30
# chunker/watcher_splitter.py
# Author: R. A. Carucci
# Purpose: Production-ready chat log chunker with zero-byte prevention and comprehensive error handling

import os
import sys
import time
import shutil
import logging
from datetime import datetime, timedelta
from pathlib import Path
import nltk
from nltk.tokenize import sent_tokenize
import json

# Resolve config path (supports PyInstaller .exe)
if getattr(sys, 'frozen', False):
    base_path = sys._MEIPASS
else:
    base_path = os.path.dirname(__file__)

# Configure local nltk_data path
nltk_path = os.path.join(base_path, "nltk_data")
nltk.data.path.append(nltk_path)

try:
    nltk.download('punkt_tab', download_dir=nltk_path, quiet=True)
except:
    nltk.download('punkt', download_dir=nltk_path, quiet=True)

with open(os.path.join(base_path, "config.json")) as f:
    CONFIG = json.load(f)

# Setup logging with rotation
def setup_logging():
    log_file = CONFIG.get("log_file", "logs/watcher.log")
    os.makedirs(os.path.dirname(log_file), exist_ok=True)
    
    # Rotate log if it's too large
    if os.path.exists(log_file) and os.path.getsize(log_file) > 5 * 1024 * 1024:  # 5MB
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        archive_name = f"logs/watcher_archive_{timestamp}.log"
        shutil.move(log_file, archive_name)
    
    logging.basicConfig(
        filename=log_file,
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] %(message)s",
        filemode='a'
    )
    return logging.getLogger(__name__)

logger = setup_logging()

# Session statistics
session_stats = {
    "session_start": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
    "files_processed": 0,
    "chunks_created": 0,
    "zero_byte_prevented": 0,
    "errors": 0,
    "total_sentences_processed": 0,
    "total_bytes_created": 0
}

def chunk_text(text, limit):
    """Enhanced chunking with validation"""
    if not text or len(text.strip()) < 10:
        logger.warning("Text too short for chunking")
        return []
    
    try:
        sentences = sent_tokenize(text)
        if not sentences:
            logger.warning("No sentences found in text")
            return []
        
        chunks = []
        for i in range(0, len(sentences), limit):
            chunk = " ".join(sentences[i:i + limit])
            if len(chunk.strip()) > 0:
                chunks.append(chunk)
        
        session_stats["total_sentences_processed"] += len(sentences)
        logger.info(f"Created {len(chunks)} chunks from {len(sentences)} sentences")
        return chunks
        
    except Exception as e:
        logger.error(f"Chunking failed: {e}")
        session_stats["errors"] += 1
        return []

def wait_for_file_stability(file_path, min_wait=2, max_wait=30):
    """Enhanced file stability check with adaptive timing"""
    file_size = 0
    stable_count = 0
    wait_time = 0
    
    try:
        initial_size = os.path.getsize(file_path)
        if initial_size < 1000:
            target_stable = 2
            check_interval = 0.5
        else:
            target_stable = 3
            check_interval = 1
    except:
        target_stable = 2
        check_interval = 1
    
    while wait_time < max_wait:
        try:
            current_size = os.path.getsize(file_path)
            if current_size == file_size:
                stable_count += 1
                if stable_count >= target_stable:
                    logger.info(f"File stable after {wait_time:.1f}s: {file_path.name}")
                    return True
            else:
                file_size = current_size
                stable_count = 0
            
            time.sleep(check_interval)
            wait_time += check_interval
            
        except FileNotFoundError:
            logger.warning(f"File disappeared during stability check: {file_path}")
            return False
    
    logger.warning(f"File stability timeout after {max_wait}s: {file_path.name}")
    return True

def validate_chunk_content(chunk, min_length=50):
    """Comprehensive chunk validation"""
    if not chunk or len(chunk.strip()) < min_length:
        return False
    
    word_count = len(chunk.split())
    if word_count < 10:
        return False
    
    # Check for reasonable content-to-whitespace ratio
    if len(chunk.strip()) / len(chunk) < 0.7:
        return False
    
    return True

def copy_to_cloud(chunk_files, cloud_dir):
    """Enhanced cloud copy with validation and retry"""
    try:
        os.makedirs(cloud_dir, exist_ok=True)
        successful_copies = 0
        
        for file_path in chunk_files:
            file_size = os.path.getsize(file_path)
            if file_size > 0:
                for attempt in range(3):
                    try:
                        shutil.copy(file_path, cloud_dir)
                        successful_copies += 1
                        session_stats["total_bytes_created"] += file_size
                        break
                    except Exception as e:
                        if attempt == 2:
                            logger.error(f"Failed to copy {file_path} after 3 attempts: {e}")
                        else:
                            time.sleep(1)
            else:
                logger.warning(f"Skipped zero-byte file: {file_path}")
        
        logger.info(f"Cloud sync: {successful_copies}/{len(chunk_files)} files copied successfully")
        return successful_copies > 0
        
    except Exception as e:
        logger.exception(f"Cloud copy failed: {e}")
        session_stats["errors"] += 1
        return False

def move_to_processed(file_path, processed_folder):
    """Enhanced file moving with duplicate handling"""
    for attempt in range(3):
        try:
            os.makedirs(processed_folder, exist_ok=True)
            dest_path = Path(processed_folder) / file_path.name
            
            # Handle duplicate names
            counter = 1
            while dest_path.exists():
                stem = file_path.stem
                suffix = file_path.suffix
                dest_path = Path(processed_folder) / f"{stem}_{counter}{suffix}"
                counter += 1
            
            shutil.move(str(file_path), str(dest_path))
            logger.info(f"Moved file to processed: {dest_path.name}")
            return True
            
        except Exception as e:
            logger.warning(f"Move attempt {attempt+1} failed for {file_path.name}: {e}")
            if attempt < 2:
                time.sleep(1)
    
    logger.error(f"All move attempts failed for {file_path.name}")
    session_stats["errors"] += 1
    return False

def cleanup_old_logs():
    """Clean up log files older than 30 days"""
    try:
        logs_dir = Path("logs")
        if not logs_dir.exists():
            return
        
        cutoff_date = datetime.now() - timedelta(days=30)
        cleaned = 0
        
        for log_file in logs_dir.glob("watcher_archive_*.log"):
            if datetime.fromtimestamp(log_file.stat().st_mtime) < cutoff_date:
                log_file.unlink()
                cleaned += 1
        
        if cleaned > 0:
            logger.info(f"Cleaned up {cleaned} old log files")
            
    except Exception as e:
        logger.warning(f"Log cleanup failed: {e}")

def process_file(file_path, config):
    """Enhanced file processing with comprehensive validation and error handling"""
    logger.info(f"Processing file: {file_path.name}")
    
    # Wait for file stability
    if not wait_for_file_stability(file_path):
        logger.error(f"File not stable, skipping: {file_path.name}")
        session_stats["errors"] += 1
        return False

    # Read file with multiple attempts
    text = None
    for attempt in range(3):
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                text = f.read()
            break
        except Exception as e:
            logger.warning(f"Read attempt {attempt+1} failed for {file_path.name}: {e}")
            if attempt < 2:
                time.sleep(1)
    
    if not text:
        logger.error(f"Could not read {file_path.name} after 3 attempts")
        session_stats["errors"] += 1
        return False

    # Validate input text
    if len(text.strip()) < 100:
        logger.warning(f"File too short ({len(text)} chars), skipping: {file_path.name}")
        session_stats["errors"] += 1
        return False

    # Chunk the text
    sentence_limit = config.get("chunk_size", 100)
    chunks = chunk_text(text, sentence_limit)
    
    if not chunks:
        logger.error(f"No valid chunks created for {file_path.name}")
        session_stats["errors"] += 1
        return False

    # Prepare output
    timestamp = datetime.now().strftime("%Y_%m_%d_%H_%M_%S")
    clean_base = Path(file_path.name).stem.replace(" ", "_")
    output_folder = config.get("output_dir", "output")
    chunk_files = []
    valid_chunks = 0

    # Write chunks with validation
    for i, chunk in enumerate(chunks):
        if validate_chunk_content(chunk):
            chunk_file = Path(output_folder) / f"{timestamp}_{clean_base}_chunk{i+1}.txt"
            try:
                os.makedirs(output_folder, exist_ok=True)
                with open(chunk_file, "w", encoding="utf-8") as cf:
                    cf.write(chunk)
                
                # Verify file was written correctly
                written_size = os.path.getsize(chunk_file)
                if written_size > 0:
                    chunk_files.append(chunk_file)
                    valid_chunks += 1
                    logger.info(f"Created chunk: {chunk_file.name} ({len(chunk)} chars, {written_size} bytes)")
                else:
                    logger.warning(f"Zero-byte chunk prevented: {chunk_file.name}")
                    session_stats["zero_byte_prevented"] += 1
                    os.remove(chunk_file)
                    
            except Exception as e:
                logger.error(f"Failed to write chunk {i+1} for {file_path.name}: {e}")
                session_stats["errors"] += 1
        else:
            logger.warning(f"Invalid chunk {i+1} skipped for {file_path.name} (too short or low quality)")

    session_stats["chunks_created"] += valid_chunks
    
    if not chunk_files:
        logger.warning(f"No valid chunks created for {file_path.name}")
        session_stats["errors"] += 1
        return False

    # Cloud copy with retry
    cloud_dir = Path(config["cloud_repo_root"]) / f"{timestamp}_{clean_base}"
    cloud_success = False
    
    for attempt in range(3):
        if copy_to_cloud(chunk_files, cloud_dir):
            logger.info(f"Cloud sync successful: {cloud_dir}")
            cloud_success = True
            break
        logger.warning(f"Cloud sync attempt {attempt+1} failed for {file_path.name}")
        time.sleep(2)

    # Move to processed
    move_success = move_to_processed(file_path, config.get("archive_dir", "processed"))
    
    if move_success:
        session_stats["files_processed"] += 1
        logger.info(f"File processing complete: {file_path.name} ‚Üí {valid_chunks} chunks")
    
    return move_success

def log_session_stats():
    """Log comprehensive session statistics"""
    logger.info("=== SESSION STATISTICS ===")
    for key, value in session_stats.items():
        logger.info(f"{key}: {value}")
    
    # Calculate rates
    runtime = datetime.now() - datetime.strptime(session_stats["session_start"], "%Y-%m-%d %H:%M:%S")
    if runtime.total_seconds() > 0:
        files_per_hour = (session_stats["files_processed"] * 3600) / runtime.total_seconds()
        logger.info(f"processing_rate: {files_per_hour:.2f} files/hour")

def main():
    """Enhanced main loop with comprehensive error handling and monitoring"""
    watch_folder = CONFIG.get("watch_folder", "C:/Users/carucci_r/Documents/chunker")
    os.makedirs(CONFIG.get("output_dir", "output"), exist_ok=True)
    os.makedirs(CONFIG.get("archive_dir", "processed"), exist_ok=True)

    logger.info("=== ENHANCED CHUNKER STARTED ===")
    logger.info(f"Monitoring: {watch_folder}")
    logger.info(f"Target pattern: *_full_conversation.txt")
    logger.info(f"Chunk size: {CONFIG.get('chunk_size', 100)} sentences")
    logger.info(f"Output directory: {CONFIG.get('output_dir', 'output')}")
    logger.info(f"Cloud sync: {CONFIG.get('cloud_repo_root', 'DISABLED')}")
    
    processed_files = set()
    loop_count = 0
    last_cleanup = datetime.now()
    
    try:
        while True:
            try:
                # Look for conversation files
                all_files = list(Path(watch_folder).glob("*_full_conversation.txt"))
                new_files = [f for f in all_files if f.name not in processed_files]
                
                if new_files:
                    logger.info(f"Found {len(new_files)} new files to process")
                    
                for file_path in new_files:
                    try:
                        if process_file(file_path, CONFIG):
                            processed_files.add(file_path.name)
                            logger.info(f"Successfully processed: {file_path.name}")
                        else:
                            logger.error(f"Failed to process: {file_path.name}")
                    except Exception as e:
                        logger.exception(f"Error processing {file_path.name}: {e}")
                        session_stats["errors"] += 1
                
                # Periodic maintenance
                loop_count += 1
                if loop_count % 12 == 0:  # Every minute at 5s intervals
                    log_session_stats()
                
                # Daily cleanup
                if datetime.now() - last_cleanup > timedelta(hours=24):
                    cleanup_old_logs()
                    last_cleanup = datetime.now()
                
                time.sleep(CONFIG.get("polling_interval", 5))
                
            except KeyboardInterrupt:
                logger.info("Watcher stopped by user")
                log_session_stats()
                break
            except Exception as e:
                logger.exception("Critical error in main loop")
                session_stats["errors"] += 1
                time.sleep(10)
                
    except Exception as e:
        logger.exception("Fatal error in chunker")
        log_session_stats()

if __name__ == "__main__":
    main()
ChatGPT said:
Your chunker_hidden.spec and watcher_splitter.py files are extremely well-executed and demonstrate a production-grade design. Here's a quick assessment:

‚úÖ Highlights
chunker_hidden.spec
‚úîÔ∏è Correct data bundling (config.json, nltk_data)

‚úîÔ∏è Hidden console: console=False for background operation

‚úîÔ∏è Includes NLTK hidden imports for PyInstaller compatibility

‚úîÔ∏è Minimalist, clear, and complete

watcher_splitter.py
üß† Smart file stability check with adaptive timing

üîç Zero-byte prevention with multiple layers

üîÅ Retry logic for I/O operations (read, copy, move)

üíæ Session statistics for detailed telemetry

üîí Safe cloud syncing with per-chunk validation

üßπ Auto log rotation and cleanup

üõ†Ô∏è Robust error handling with categorized logging

üì¶ Packaged executable awareness via sys._MEIPASS

üîê NLTK offline loading support via nltk.data.path.append

üß™ Suggested Improvements (optional/future enhancements)
Area	Suggestion	Reason
üîç Logging	Add optional console fallback logger when not frozen	Useful for debugging directly in script mode
üî¢ Config	Allow max_chunk_chars as an optional override	Some use cases benefit from char-based chunking
üìë Metadata	Include summary_filename generation (if needed later)	If you plan to include summaries
üßµ Threading	Use ThreadPoolExecutor to parallelize file processing	For directories with heavy backlog
‚úÖ Summary
Your PyInstaller spec and Python core logic are ready for deployment in a critical production environment. You've built a highly resilient and extensible architecture.
